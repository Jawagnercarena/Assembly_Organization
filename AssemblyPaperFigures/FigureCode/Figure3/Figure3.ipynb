{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Figure 4 Code to Produce Figures\n",
    "\n",
    "\n",
    "\n",
    " This figure will focus on the presentation of Further Analysis of Extracted Assemblies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster, cophenet\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from difflib import diff_bytes\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "import upsetplot\n",
    "import ptitprince\n",
    "import matplotlib.cm as cm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "import v1dd_physiology.data_fetching as daf\n",
    "from statannotations.Annotator import Annotator\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "paired_blue0 = cm.get_cmap('Paired')(0)\n",
    "paired_blue1 = cm.get_cmap('Paired')(1)\n",
    "\n",
    "colors1 = ['grey', (.4, .6, .8, .5)]\n",
    "colors2 = ['grey', (.2, .3, .5, .5), (.4, .6, .8, .5), 'white']\n",
    "colors3 = [(.2, .3, .5, .5), 'white']\n",
    "colors4 = ['grey', (.2, .3, .5, .5), (.4, .6, .8, .5),]\n",
    "colors5 = [(.2, .3, .5, .5), (.4, .6, .8, .5), 'white']\n",
    "colors6 = ['grey', 'steelblue', (.4, .6, .8, .5),]\n",
    "colors7 = ['grey', 'steelblue']\n",
    "colors8 = ['grey', 'cornflowerblue']\n",
    "colors9 = ['grey', paired_blue1]\n",
    "colors10 = ['dimgrey', paired_blue0]\n",
    "greymap = LinearSegmentedColormap.from_list(\n",
    "        \"Custom\", colors10, N=80) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dF_trace = np.load(\"/media/berteau/Elements/sessionM409828_12_nm_dff.npy\")[0,:,:]\n",
    "dF_trace = np.load(\"../Data/Session13/sessionM409828_13_dff.npy\") # stefan extracted\n",
    "print(dF_trace.shape)\n",
    "dF_trace_clipped = dF_trace[16000:16600, 1300:1600].T\n",
    "plt.figure(figsize=(4,16))\n",
    "sns.heatmap(dF_trace_clipped, cmap=greymap, cbar=False, yticklabels=False, xticklabels=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Suppress ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Oracle Scores Assemblies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions relevant for Oracle Score Analysis\n",
    "\n",
    "def distance(x, y):\n",
    "    return math.sqrt((x[0]-y[0])**2 + (x[1]-y[1])**2)\n",
    "\n",
    "# def process(img):\n",
    "#     img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     img_canny = cv2.Canny(img_gray, 0, 50)\n",
    "#     img_dilate = cv2.dilate(img_canny, None, iterations=1)\n",
    "#     img_erode = cv2.erode(img_dilate, None, iterations=1)\n",
    "#     return img_erode\n",
    "\n",
    "def get_assembly_time_trace(coactivity_trace, scores_in_order, name='scored_time_trace'):\n",
    "    # Set up three subplots\n",
    "    num_assemblies = coactivity_trace.shape[1]\n",
    "    fig, ax = plt.subplots(num_assemblies, 1, figsize=(12, 12))\n",
    "\n",
    "    # plot\n",
    "    for i in range(num_assemblies):\n",
    "        ax[i].plot(coactivity_trace[:, i], color='green')\n",
    "        ax[i].set_ylabel(f\"{scores_in_order[i]:.2}\")\n",
    "        ax[i].set_xlabel(\"Time Steps\")\n",
    "        ax[i].grid()\n",
    "\n",
    "    fig.suptitle(\"Assembly Time Trace\")\n",
    "    #plt.savefig(f\"oracle_dists2/assemblies_esteps_150000_affinity_04_{name}.png\", dpi = 1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITY_RASTER = scipy.io.loadmat(\n",
    "    \"../Data/Session13/Assembly_Files/esteps_150000_affinity_04_sessionM409828_13_ACTIVITY-RASTER.mat\", struct_as_record=True, squeeze_me=True)\n",
    "SGC_ASSEMBLIES = scipy.io.loadmat(\n",
    "    \"../Data/Session13/Assembly_Files/esteps_150000_affinity_04_sessionM409828_13_SGC-ASSEMBLIES.mat\", struct_as_record=True, squeeze_me=True)\n",
    "\n",
    "#print(ACTIVITY_RASTER.keys())\n",
    "\n",
    "activity_raster = ACTIVITY_RASTER['activity_raster']\n",
    "activity_raster_peaks = ACTIVITY_RASTER['activity_raster_peaks']\n",
    "\n",
    "coactivity_trace = activity_raster.mean(axis=1)\n",
    "\n",
    "assemblies = SGC_ASSEMBLIES['assemblies']\n",
    "\n",
    "# Ensure correct indexing (convert 1-based to 0-based)\n",
    "assemblies = [np.array(A) - 1 for A in assemblies]\n",
    "assemblies.sort(key = len)\n",
    "assemblies.reverse()\n",
    "\n",
    "# for assembly in assemblies:\n",
    "#     print(np.max(assembly))\n",
    "#     print(np.min)\n",
    "\n",
    "assembly_coactivity_trace = np.vstack(\n",
    "    [activity_raster[:, A].mean(axis=1) for A in assemblies]).T\n",
    "\n",
    "scores_in_order = np.load('oracle_dists/assemblies_esteps_150000_affinity_04_session13_natural_movie_oracle_scores.npy')\n",
    "get_assembly_time_trace(assembly_coactivity_trace, scores_in_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "print(activity_raster.shape)\n",
    "sns.heatmap(activity_raster[0:60,0:100].T, cmap='PuBu')\n",
    "plt.title('Cellular Activity Raster')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cell')\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb_f = h5py.File('M409828_13_20181213.nwb', 'r')\n",
    "import v1dd_physiology.data_fetching as daf\n",
    "\n",
    "sess_id = daf.get_session_id(nwb_f=nwb_f)\n",
    "print(sess_id)\n",
    "\n",
    "# plane_ns = daf.get_plane_names(nwb_f=nwb_f)\n",
    "# print(\"Planes: \", plane_ns)\n",
    "\n",
    "# for plane_n in plane_ns:\n",
    "#     depth = daf.get_plane_depth(nwb_f=nwb_f, plane_n=plane_n)\n",
    "#     print(f'depth of {plane_n}: {depth} um')\n",
    "\n",
    "fs = []\n",
    "dffs = []\n",
    "events = []\n",
    "locomotions = []\n",
    "rois = []\n",
    "pika_rois = []\n",
    "coords = []\n",
    "\n",
    "# Uncomment this if you want the actual fluorescence activity trace\n",
    "# f = np.array(nwb_f['processing']['rois_and_traces_plane0']\n",
    "#              ['Fluorescence']['f_raw_subtracted'].get('data'))\n",
    "\n",
    "# If you want DFF, you need to compute it using the allensdk, or load saved traces from /home/berteau/v1DD\n",
    "\n",
    "f = assembly_coactivity_trace\n",
    "# print(coactivity_trace)\n",
    "f_ts = np.array(nwb_f['processing']['rois_and_traces_plane0']\n",
    "                ['Fluorescence']['f_raw_subtracted'].get('timestamps'))\n",
    "# events\n",
    "clip_ts = f.shape[1]\n",
    "\n",
    "total_movie_oracle_r_values = np.zeros((0, 9))\n",
    "\n",
    "passing_roi_count = f.shape[1]\n",
    "\n",
    "oracle_r_values = np.zeros((passing_roi_count, 25, 8))\n",
    "movie_oracle_r_values = np.zeros((passing_roi_count, 9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ns = daf.get_plane_names(nwb_f=nwb_f)\n",
    "plane_ns\n",
    "rois_dict = {}\n",
    "for plane_n in plane_ns:\n",
    "    roi_ns = daf.get_roi_ns(nwb_f=nwb_f, plane_n=plane_n)\n",
    "    print(f'there are {len(roi_ns)} in {plane_n} of session: {sess_id}:')\n",
    "    print('\\nnames of first 100 rois:\\n')\n",
    "    print(roi_ns[0:100])\n",
    "    rois_dict[plane_n] = roi_ns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_neurons = set()\n",
    "for assembly in assemblies:\n",
    "    assembly_neurons.update(set(assembly))\n",
    "assembly_neurons = np.array(list(assembly_neurons))\n",
    "len(assembly_neurons)\n",
    "\n",
    "# # %%\n",
    "pika_rois_all_dict = {}\n",
    "pika_rois_in_assembly_dict = {}\n",
    "pika_rois_no_assembly_dict = {}\n",
    "for plane_n, roi_ns in rois_dict.items():\n",
    "    pika_rois = []\n",
    "    pika_rois_in_assembly = []\n",
    "    pika_rois_no_assembly = []\n",
    "    # print(plane_n)\n",
    "    for roi_n in roi_ns:\n",
    "        score = daf.get_pika_classifier_score(nwb_f=nwb_f, plane_n=plane_n, roi_n=roi_n)\n",
    "        if score > 0.5:  # Using the threshold from team PIKA, per https://github.com/zhuangjun1981/v1dd_physiology/blob/main/v1dd_physiology/example_notebooks/2022-06-27-data-fetching-basic.ipynb\n",
    "            pika_rois.append(roi_n)\n",
    "            if int(roi_n[4:]) in assembly_neurons:\n",
    "                pika_rois_in_assembly.append(roi_n)\n",
    "            else:\n",
    "                pika_rois_no_assembly.append(roi_n)\n",
    "    pika_rois_all_dict[plane_n] = pika_rois\n",
    "    pika_rois_in_assembly_dict[plane_n] = pika_rois_in_assembly\n",
    "    pika_rois_no_assembly_dict[plane_n] = pika_rois_no_assembly\n",
    "\n",
    "total_rois = np.sum([len(val) for val in pika_rois_all_dict.values()])\n",
    "neuron_all_movie_oracle_r_values = np.zeros((total_rois, 9))\n",
    "\n",
    "total_rois = np.sum([len(val) for val in pika_rois_in_assembly_dict.values()])\n",
    "neuron_in_assembly_movie_oracle_r_values = np.zeros((total_rois, 9))\n",
    "\n",
    "total_rois = np.sum([len(val) for val in pika_rois_no_assembly_dict.values()])\n",
    "neuron_no_assembly_movie_oracle_r_values = np.zeros((total_rois, 9))\n",
    "trial_dff = []\n",
    "count_n_all = -1\n",
    "count_n_in_assembly = -1\n",
    "count_n_no_assembly = -1\n",
    "for curr_dict, oracle_array, c in zip([pika_rois_all_dict, pika_rois_in_assembly_dict, pika_rois_no_assembly_dict], \n",
    "        [neuron_all_movie_oracle_r_values, neuron_in_assembly_movie_oracle_r_values, neuron_no_assembly_movie_oracle_r_values],\n",
    "        [1,2,3]):\n",
    "    for plane_n, pika_roi_ns in curr_dict.items():\n",
    "        for roi_n in pika_roi_ns:\n",
    "            if c == 1:\n",
    "                count_n_all += 1\n",
    "                current_count = count_n_all\n",
    "            elif c == 2:\n",
    "                count_n_in_assembly += 1\n",
    "                current_count = count_n_in_assembly\n",
    "            elif c == 3:\n",
    "                count_n_no_assembly += 1\n",
    "                current_count = count_n_no_assembly\n",
    "            ### Get Time Trace\n",
    "            f, f_ts = daf.get_single_trace(nwb_f=nwb_f, plane_n=plane_n, roi_n=roi_n, trace_type='dff')\n",
    "            f_binary_raster = activity_raster[:, int(roi_n[4:])-1]\n",
    "\n",
    "            # Get Repeated Natural Movies\n",
    "            trial_fluorescence = []\n",
    "            # trial_dff = []\n",
    "            presentation = nwb_f['stimulus']['presentation']\n",
    "            nm_timestamps = np.array(\n",
    "                presentation['natural_movie'].get('timestamps'))\n",
    "            nm_data = np.array(presentation['natural_movie'].get('data'))\n",
    "            new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "            clip_duration = 300  # new_clips[1]-1\n",
    "            for repeat_id in range(new_clips.shape[0]):\n",
    "                frames_to_capture = np.where(f_ts >= nm_timestamps[new_clips[repeat_id]])[\n",
    "                    0][0:clip_duration]\n",
    "                trial_fluorescence.append(f_binary_raster[frames_to_capture])\n",
    "                trial_dff.append(f[frames_to_capture])\n",
    "            trial_fluorescence_np = np.array(trial_fluorescence)\n",
    "            for trial_idx in range(trial_fluorescence_np.shape[0]):\n",
    "                removed_trial = trial_fluorescence_np[trial_idx]\n",
    "                remaining_trials = np.delete(\n",
    "                    trial_fluorescence_np, trial_idx, 0)\n",
    "                r, p = scipy.stats.pearsonr(\n",
    "                    removed_trial, np.mean(remaining_trials, 0))\n",
    "                oracle_array[current_count, trial_idx] = r\n",
    "\n",
    "groups_p_values = ['T-test: {:.3g}'.format(stats.ttest_ind(np.array(neuron_all_movie_oracle_r_values).flatten(), np.array(neuron_in_assembly_movie_oracle_r_values).flatten(), equal_var = False).pvalue, 5),\n",
    "                    'T-test: {:.3g}'.format(stats.ttest_ind(np.array(neuron_all_movie_oracle_r_values).flatten(), np.array(neuron_no_assembly_movie_oracle_r_values).flatten(), equal_var = False).pvalue, 5)]\n",
    "\n",
    "all_arr = [np.array(neuron_all_movie_oracle_r_values).flatten(),\n",
    "            np.array(neuron_in_assembly_movie_oracle_r_values).flatten(),\n",
    "            np.array(neuron_no_assembly_movie_oracle_r_values).flatten()]\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.boxplot(data=all_arr)\n",
    "ax.set_xticklabels([\"All Neurons\", \"Neurons In Assembly\", \"Neurons Out Assembly\"], size = 16)\n",
    "ax.set_title('Natural Movie Oracle Score', size = 22)\n",
    "ax.set_ylabel('Oracle Score', size = 16)\n",
    "\n",
    "# print(groups, p_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = np.mean(np.vstack(trial_dff), axis=0) > 0.5\n",
    "# plt.figure(figsize=(4*8,12*8))\n",
    "# # sns.heatmap(np.vstack(trial_dff)[120*1:120*4, 40*5:300], cmap='viridis', cbar=False)\n",
    "# sns.heatmap(np.vstack(trial_dff)[0:300, mask], cmap=greymap, cbar=False)\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_oracle_r_values = np.zeros((passing_roi_count, 9))\n",
    "f = assembly_coactivity_trace\n",
    "# print(coactivity_trace)\n",
    "f_ts = np.array(nwb_f['processing']['rois_and_traces_plane0']\n",
    "                ['Fluorescence']['f_raw_subtracted'].get('timestamps'))\n",
    "for roi_n in range(passing_roi_count):\n",
    "\n",
    "    # Get Repeated Natural Movies\n",
    "    trial_fluorescence = []\n",
    "    presentation = nwb_f['stimulus']['presentation']\n",
    "    nm_timestamps = np.array(\n",
    "        presentation['natural_movie'].get('timestamps'))\n",
    "    nm_data = np.array(presentation['natural_movie'].get('data'))\n",
    "    new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "    clip_duration = 300  # new_clips[1]-1\n",
    "    for repeat_id in range(new_clips.shape[0]):\n",
    "        frames_to_capture = np.where(f_ts >= nm_timestamps[new_clips[repeat_id]])[\n",
    "            0][0:clip_duration]\n",
    "        trial_fluorescence.append(f[frames_to_capture, roi_n])\n",
    "    trial_fluorescence_np = np.array(trial_fluorescence)\n",
    "    for trial_idx in range(trial_fluorescence_np.shape[0]):\n",
    "        removed_trial = trial_fluorescence_np[trial_idx]\n",
    "        remaining_trials = np.delete(\n",
    "            trial_fluorescence_np, trial_idx, 0)\n",
    "        r, p = scipy.stats.pearsonr(\n",
    "            removed_trial, np.mean(remaining_trials, 0))\n",
    "        movie_oracle_r_values[roi_n, trial_idx] = r\n",
    "\n",
    "    # get_assembly_time_trace(trial_fluorescence_np.T, movie_oracle_r_values[roi_n, :], name=f'r_value_over_each_natural_movie_assembly{roi_n}')\n",
    "\n",
    "    # total_movie_oracle_r_values = np.append(\n",
    "    #     total_movie_oracle_r_values, movie_oracle_r_values, 0)\n",
    "# Plot Movie Oracles\n",
    "mean_over_holdouts = np.mean(movie_oracle_r_values, 1)\n",
    "fig = plt.figure()\n",
    "plt.title('Assembly natural movie oracle score')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.hist(mean_over_holdouts[:], bins=50)\n",
    "#plt.savefig('./figure4_plots/oracle_dists2/assemblies_esteps_150000__affinity_04_session'+str(13)+'_movies.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "all_arr = [np.array(movie_oracle_r_values).flatten(),\n",
    "            np.array(neuron_all_movie_oracle_r_values).flatten(),\n",
    "            np.array(neuron_in_assembly_movie_oracle_r_values).flatten(),\n",
    "            np.array(neuron_no_assembly_movie_oracle_r_values).flatten()]\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.boxplot(data=all_arr,\n",
    "                notch=True, showcaps=True,\n",
    "                flierprops={\"marker\": \"x\"},\n",
    "                boxprops={\"facecolor\": (.4, .6, .8, .5)},\n",
    "                medianprops={\"color\": \"coral\"},\n",
    "            )\n",
    "ax.set_xticklabels([\"Assemblies\", \"All Neurons\", \"Neurons In Assembly\", \"Neurons Out Assembly\"], size = 12)\n",
    "ax.set_title('Natural Movie Oracle Score', size = 20)\n",
    "ax.set_ylabel('Oracle Score', size = 14)\n",
    "\n",
    "medians = np.array(\n",
    "    [np.median(np.array(movie_oracle_r_values).flatten()),\n",
    "     np.median(np.array(neuron_all_movie_oracle_r_values).flatten()),\n",
    "     np.median(np.array(neuron_in_assembly_movie_oracle_r_values).flatten()),\n",
    "     np.median(np.array(neuron_no_assembly_movie_oracle_r_values).flatten())]\n",
    ")\n",
    "\n",
    "vertical_offset = medians * 0.2 # offset from median for display\n",
    "p_values = [np.nan,\n",
    "            'T-test: {:.3g}'.format(stats.ttest_ind(np.array(movie_oracle_r_values).flatten(), np.array(neuron_all_movie_oracle_r_values).flatten(), equal_var = False).pvalue, 5),\n",
    "            'T-test: {:.3g}'.format(stats.ttest_ind(np.array(movie_oracle_r_values).flatten(), np.array(neuron_in_assembly_movie_oracle_r_values).flatten(), equal_var = False).pvalue, 5),\n",
    "            'T-test: {:.3g}'.format(stats.ttest_ind(np.array(movie_oracle_r_values).flatten(), np.array(neuron_no_assembly_movie_oracle_r_values).flatten(), equal_var = False).pvalue, 5)]\n",
    "\n",
    "for xtick in ax.get_xticks():\n",
    "    if xtick != 0:\n",
    "        ax.text(xtick, medians[xtick] + vertical_offset[xtick], p_values[xtick], \n",
    "                horizontalalignment='center', size='small', color='black', weight='semibold')\n",
    "\n",
    "#plt.savefig('./figure4_plots/oracle_scores_histogram_f_subtracted.png', dpi = 1200)\n",
    "plt.show()\n",
    "\n",
    "plt.close()\n",
    "np.save('oracle_dists2/assemblies_esteps_150000_affinity_04_session'+str(13)+\n",
    "        '_natural_movie_oracle_r_values.npy', movie_oracle_r_values)\n",
    "np.save('oracle_dists2/assemblies_esteps_150000_affinity_04_session'+str(13)+\n",
    "        '_natural_movie_oracle_scores.npy', mean_over_holdouts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # %%\n",
    "pika_rois_all_dict = {}\n",
    "pika_rois_in_assembly_dict = {}\n",
    "pika_rois_no_assembly_dict = {}\n",
    "no_assembly_neurons = []\n",
    "# for plane_n, roi_ns in rois_dict.items():\n",
    "    # pika_rois = []\n",
    "    # pika_rois_in_assembly = []\n",
    "    # pika_rois_no_assembly = []\n",
    "    # # print(plane_n)\n",
    "    # for roi_n in roi_ns:\n",
    "    #     score = daf.get_pika_classifier_score(nwb_f=nwb_f, plane_n=plane_n, roi_n=roi_n)\n",
    "    #     if score > 0.5:  # Using the threshold from team PIKA, per https://github.com/zhuangjun1981/v1dd_physiology/blob/main/v1dd_physiology/example_notebooks/2022-06-27-data-fetching-basic.ipynb\n",
    "    #         pika_rois.append(roi_n)\n",
    "    #         if int(roi_n[4:]) in assembly_neurons:\n",
    "    #             pika_rois_in_assembly.append(roi_n)\n",
    "    #         else:\n",
    "    #             no_assembly_neurons.append(int(roi_n[4:]))\n",
    "    #             pika_rois_no_assembly.append(roi_n)\n",
    "    # pika_rois_all_dict[plane_n] = pika_rois\n",
    "    # pika_rois_in_assembly_dict[plane_n] = pika_rois_in_assembly\n",
    "    # pika_rois_no_assembly_dict[plane_n] = pika_rois_no_assembly\n",
    "\n",
    "# total_rois = np.sum([len(val) for val in pika_rois_all_dict.values()])\n",
    "# neuron_all_movie_oracle_r_values = np.zeros((total_rois, 9))\n",
    "\n",
    "# total_rois = np.sum([len(val) for val in pika_rois_in_assembly_dict.values()])\n",
    "# neuron_in_assembly_movie_oracle_r_values = np.zeros((total_rois, 9))\n",
    "\n",
    "# total_rois = np.sum([len(val) for val in pika_rois_no_assembly_dict.values()])\n",
    "# neuron_no_assembly_movie_oracle_r_values = np.zeros((total_rois, 9))\n",
    "\n",
    "# count_n_all = -1\n",
    "# count_n_in_assembly = -1\n",
    "# count_n_no_assembly = -1\n",
    "# for curr_dict, oracle_array, c in zip([pika_rois_all_dict, pika_rois_in_assembly_dict, pika_rois_no_assembly_dict], \n",
    "#         [neuron_all_movie_oracle_r_values, neuron_in_assembly_movie_oracle_r_values, neuron_no_assembly_movie_oracle_r_values],\n",
    "#         [1,2,3]):\n",
    "#     for plane_n, pika_roi_ns in curr_dict.items():\n",
    "#         for roi_n in pika_roi_ns:\n",
    "#             if c == 1:\n",
    "#                 count_n_all += 1\n",
    "#                 current_count = count_n_all\n",
    "#             elif c == 2:\n",
    "#                 count_n_in_assembly += 1\n",
    "#                 current_count = count_n_in_assembly\n",
    "#             elif c == 3:\n",
    "#                 count_n_no_assembly += 1\n",
    "#                 current_count = count_n_no_assembly\n",
    "#             ### Get Time Trace\n",
    "#             dff, dff_ts = daf.get_single_trace(nwb_f=nwb_f, plane_n=plane_n, roi_n=roi_n, trace_type='dff')\n",
    "#             f_binary_raster = activity_raster[:, int(roi_n[4:])-1]\n",
    "            \n",
    "#             # Get Repeated Natural Movies\n",
    "#             trial_fluorescence = []\n",
    "#             presentation = nwb_f['stimulus']['presentation']\n",
    "#             nm_timestamps = np.array(\n",
    "#                 presentation['natural_movie'].get('timestamps'))\n",
    "#             nm_data = np.array(presentation['natural_movie'].get('data'))\n",
    "#             new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "#             clip_duration = 300  # new_clips[1]-1\n",
    "#             for repeat_id in range(new_clips.shape[0]):\n",
    "#                 frames_to_capture = np.where(dff_ts >= nm_timestamps[new_clips[repeat_id]])[\n",
    "#                     0][0:clip_duration]\n",
    "#                 trial_fluorescence.append(f_binary_raster[frames_to_capture])\n",
    "#             trial_fluorescence_np = np.array(trial_fluorescence)\n",
    "#             for trial_idx in range(trial_fluorescence_np.shape[0]):\n",
    "#                 removed_trial = trial_fluorescence_np[trial_idx]\n",
    "#                 remaining_trials = np.delete(\n",
    "#                     trial_fluorescence_np, trial_idx, 0)\n",
    "#                 r, p = scipy.stats.pearsonr(\n",
    "#                     removed_trial, np.mean(remaining_trials, 0))\n",
    "#                 oracle_array[current_count, trial_idx] = r\n",
    "\n",
    "# movie_oracle_r_values = np.zeros((passing_roi_count, 9))\n",
    "# f = assembly_coactivity_trace\n",
    "# # print(coactivity_trace)\n",
    "# f_ts = np.array(nwb_f['processing']['rois_and_traces_plane0']\n",
    "#                 ['Fluorescence']['f_raw_subtracted'].get('timestamps'))\n",
    "# for roi_n in range(passing_roi_count):\n",
    "\n",
    "#     # Get Repeated Natural Movies\n",
    "#     trial_fluorescence = []\n",
    "#     presentation = nwb_f['stimulus']['presentation']\n",
    "#     nm_timestamps = np.array(\n",
    "#         presentation['natural_movie'].get('timestamps'))\n",
    "#     nm_data = np.array(presentation['natural_movie'].get('data'))\n",
    "#     new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "#     clip_duration = 300  # new_clips[1]-1\n",
    "#     for repeat_id in range(new_clips.shape[0]):\n",
    "#         frames_to_capture = np.where(f_ts >= nm_timestamps[new_clips[repeat_id]])[\n",
    "#             0][0:clip_duration]\n",
    "#         trial_fluorescence.append(f[frames_to_capture, roi_n])\n",
    "#     trial_fluorescence_np = np.array(trial_fluorescence)\n",
    "#     for trial_idx in range(trial_fluorescence_np.shape[0]):\n",
    "#         removed_trial = trial_fluorescence_np[trial_idx]\n",
    "#         remaining_trials = np.delete(\n",
    "#             trial_fluorescence_np, trial_idx, 0)\n",
    "#         r, p = scipy.stats.pearsonr(\n",
    "#             removed_trial, np.mean(remaining_trials, 0))\n",
    "#         movie_oracle_r_values[roi_n, trial_idx] = r\n",
    "\n",
    "#     # get_assembly_time_trace(trial_fluorescence_np.T, movie_oracle_r_values[roi_n, :], name=f'r_value_over_each_natural_movie_assembly{roi_n}')\n",
    "\n",
    "#     # total_movie_oracle_r_values = np.append(\n",
    "#     #     total_movie_oracle_r_values, movie_oracle_r_values, 0)\n",
    "# # Plot Movie Oracles\n",
    "# mean_over_holdouts = np.mean(movie_oracle_r_values, 1)\n",
    "# fig = plt.figure()\n",
    "# plt.title('Assembly natural movie oracle score')\n",
    "# plt.xlabel('Score')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.hist(mean_over_holdouts[:], bins=50)\n",
    "# #plt.savefig('./figure4_plots/oracle_dists2/assemblies_esteps_150000__affinity_04_session'+str(13)+'_movies.png')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(14, 10))\n",
    "# all_arr = [np.array(movie_oracle_r_values).flatten(),\n",
    "#             np.array(neuron_all_movie_oracle_r_values).flatten(),\n",
    "#             np.array(neuron_in_assembly_movie_oracle_r_values).flatten(),\n",
    "#             np.array(neuron_no_assembly_movie_oracle_r_values).flatten()]\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "# ax = sns.boxplot(data=all_arr,\n",
    "#                 notch=True, showcaps=True,\n",
    "#                 flierprops={\"marker\": \"x\"},\n",
    "#                 boxprops={\"facecolor\": (.4, .6, .8, .5)},\n",
    "#                 medianprops={\"color\": \"coral\"},\n",
    "#             )\n",
    "# ax.set_xticklabels([\"Assemblies\", \"All Neurons\", \"Assembly Cells\", \"Non-Assembly Cells\"], size = 12)\n",
    "# ax.set_title('Natural Movie Oracle Score', size = 20)\n",
    "# ax.set_ylabel('Oracle Score', size = 15)\n",
    "\n",
    "# medians = np.array(\n",
    "#     [np.median(np.array(movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(neuron_all_movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(neuron_in_assembly_movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(neuron_no_assembly_movie_oracle_r_values).flatten())]\n",
    "# )\n",
    "\n",
    "# vertical_offset = medians * 0.2 # offset from median for display\n",
    "# p_values = [np.nan,\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(movie_oracle_r_values).flatten(), np.array(neuron_all_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5),\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(movie_oracle_r_values).flatten(), np.array(neuron_in_assembly_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5),\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(movie_oracle_r_values).flatten(), np.array(neuron_no_assembly_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5)]\n",
    "\n",
    "# for xtick in ax.get_xticks():\n",
    "#     if xtick != 0:\n",
    "#         ax.text(xtick, medians[xtick] + vertical_offset[xtick], p_values[xtick], \n",
    "#                 horizontalalignment='center', size='small', color='black', weight='semibold')\n",
    "\n",
    "# plt.savefig('./figure4_plots/oracle_scores_histogram_dff.png', dpi = 1200)\n",
    "# plt.show()\n",
    "\n",
    "# # %%\n",
    "# plt.figure(figsize=(14, 10))\n",
    "# all_arr = [np.array(movie_oracle_r_values).flatten(),\n",
    "#             np.array(neuron_all_movie_oracle_r_values).flatten(),\n",
    "#             np.array(neuron_in_assembly_movie_oracle_r_values).flatten(),\n",
    "#             np.array(neuron_no_assembly_movie_oracle_r_values).flatten()]\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "# ax = sns.boxplot(data=all_arr,\n",
    "#                 notch=True, showcaps=True,\n",
    "#                 flierprops={\"marker\": \"x\"},\n",
    "#                 boxprops={\"facecolor\": (.4, .6, .8, .5)},\n",
    "#                 medianprops={\"color\": \"coral\"},\n",
    "#             )\n",
    "# ax.set_xticklabels([\"Assemblies\", \"All Cells\", \"Assembly Cells\", \"Non-Assembly Cells\"], size = 16)\n",
    "# ax.set_title('Natural Movie Oracle Score', size = 20)\n",
    "# ax.set_ylabel('Oracle Score', size = 18)\n",
    "# plt.yticks(fontsize=16)\n",
    "\n",
    "# medians = np.array(\n",
    "#     [np.median(np.array(movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(neuron_all_movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(neuron_in_assembly_movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(neuron_no_assembly_movie_oracle_r_values).flatten())]\n",
    "# )\n",
    "\n",
    "# vertical_offset = medians * 0.2 # offset from median for display\n",
    "# p_values = [np.nan,\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(movie_oracle_r_values).flatten(), np.array(neuron_all_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5),\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(movie_oracle_r_values).flatten(), np.array(neuron_in_assembly_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5),\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(movie_oracle_r_values).flatten(), np.array(neuron_no_assembly_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5)]\n",
    "\n",
    "# for xtick in ax.get_xticks():\n",
    "#     if xtick != 0:\n",
    "#         ax.text(xtick, medians[xtick] + vertical_offset[xtick], p_values[xtick], \n",
    "#                 horizontalalignment='center', size= 12, color='black', weight='semibold')\n",
    "\n",
    "# plt.savefig('./figure4_plots/oracle_scores_histogram_dff.png', dpi = 1200)\n",
    "# plt.show()\n",
    "\n",
    "# # %%\n",
    "# # Get repeated drifting fullscreen gratings\n",
    "# presentation = nwb_f['stimulus']['presentation']\n",
    "# dgc_onsets = np.array(\n",
    "#     presentation['drifting_gratings_full'].get('timestamps'))\n",
    "# dgc_data = np.array(presentation['drifting_gratings_full'].get('data'))\n",
    "# num_samples = presentation['drifting_gratings_full'].get('num_samples')\n",
    "# # stims = daf.get_stim_list(nwb_f=nwb_f)\n",
    "# # dgc_onsets = daf.get_dgc_onset_times(nwb_f, dgc_type='windowed')\n",
    "# # presentation = nwb_f['stimulus']['presentation']\n",
    "# # num_samples = np.array(\n",
    "# #     presentation['drifting_gratings_windowed'].get('num_samples'))\n",
    "# duration_sec = 2\n",
    "# grating_number = 0\n",
    "\n",
    "# # Get Tuning Curves and Oracles from Drfiting Gratings\n",
    "# trial_responses_by_assembly_and_orientation = {}\n",
    "# mean_response_by_assembly_and_orientation = {}\n",
    "# oracle_by_assembly_and_orientation = {}\n",
    "# for assembly_n in range(passing_roi_count):\n",
    "#     trial_responses_by_assembly_and_orientation[assembly_n] = {}\n",
    "#     mean_response_by_assembly_and_orientation[assembly_n] = []\n",
    "#     oracle_by_assembly_and_orientation[assembly_n] = []\n",
    "#     for orientation in [0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 330]:\n",
    "#         trial_responses_by_assembly_and_orientation[assembly_n][orientation] = []\n",
    "#         trials = np.where(dgc_data[:,4] == orientation)[0]\n",
    "#         for trial_id in trials:\n",
    "#             mask = (f_ts < dgc_data[trial_id,1]) * (f_ts >= dgc_data[trial_id,0])\n",
    "#             frames_to_capture = np.where(mask)[0]\n",
    "#             if frames_to_capture.shape[0] > 10:\n",
    "#                 frames_to_capture = frames_to_capture[0:10]\n",
    "#             trial_responses_by_assembly_and_orientation[assembly_n][orientation].append(f[frames_to_capture, assembly_n])\n",
    "#         # Now compute oracle\n",
    "#         fluorescence_across_trials_np = np.array(trial_responses_by_assembly_and_orientation[assembly_n][orientation])\n",
    "#         r_sum = 0\n",
    "#         for trial_idx in range(len(trials)):\n",
    "#             removed_trial = fluorescence_across_trials_np[trial_idx]\n",
    "#             remaining_trials = np.delete(\n",
    "#                 fluorescence_across_trials_np, trial_idx, 0)\n",
    "#             if assembly_n == 3:\n",
    "#                 print(removed_trial)\n",
    "#                 print(np.mean(remaining_trials, 0))\n",
    "#             r, p = scipy.stats.pearsonr(\n",
    "#                 removed_trial, np.mean(remaining_trials, 0))\n",
    "#             r_sum += np.nan_to_num(r)\n",
    "#         oracle_by_assembly_and_orientation[assembly_n].append(r_sum / len(trials))\n",
    "#         # Now compute mean response\n",
    "#         mean_response_by_assembly_and_orientation[assembly_n].append(np.mean(np.array(trial_responses_by_assembly_and_orientation[assembly_n][orientation])))\n",
    "\n",
    "# degree_orientations = [0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 330]\n",
    "\n",
    "# # Plot Assembly Orientation Mean Responses\n",
    "# rows = passing_roi_count // 4\n",
    "# if passing_roi_count % 4 > 0:\n",
    "#     rows += 1\n",
    "# fig, axes = plt.subplots(4, rows, figsize=(15,15))\n",
    "# fig.suptitle('Assembly Tuning Curves: DG Mean Coactivity')\n",
    "# for assembly_n in range(passing_roi_count):\n",
    "#     row = assembly_n // 4\n",
    "#     column = assembly_n % 4\n",
    "#     # axes[row, column].set_theta_direction(-1)\n",
    "#     # axes[row, column].set_theta_offset(np.pi / 2.0)\n",
    "#     axes[row, column].plot(degree_orientations, mean_response_by_assembly_and_orientation[assembly_n])\n",
    "#     axes[row, column].set_title(f'Assembly {assembly_n+1}')\n",
    "# plt.savefig('./figure4_plots/oracle_dists2/assemblies_esteps_150000_affinity_04_session'+str(13)+'_tuning_curves_DG_fullscreen_mean_coactivity.png')\n",
    "# plt.close()\n",
    "\n",
    "# # , subplot_kw={'projection': 'polar'}\n",
    "# # Plot Assembly Orientation Oracle Values\n",
    "# rows = passing_roi_count // 4\n",
    "# if passing_roi_count % 4 > 0:\n",
    "#     rows += 1\n",
    "# fig, axes = plt.subplots(4, rows, figsize=(15,15))\n",
    "# fig.suptitle('Assembly Oracle Score By Orientation: DG Coactivity')\n",
    "# for assembly_n in range(passing_roi_count):\n",
    "#     row = assembly_n // 4\n",
    "#     column = assembly_n % 4\n",
    "#     # axes[row, column].set_theta_direction(-1)\n",
    "#     # axes[row, column].set_theta_offset(np.pi / 2.0)\n",
    "#     axes[row, column].plot(degree_orientations, oracle_by_assembly_and_orientation[assembly_n])\n",
    "#     axes[row, column].set_title(f'Assembly {assembly_n+1}')\n",
    "# plt.savefig('./figure4_plots/oracle_dists2/assemblies_esteps_150000_affinity_04_session'+str(13)+'_oracle_by_orientation_DG_fullscreen_coactivity.png')\n",
    "# plt.close()\n",
    "\n",
    "\n",
    "# print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Sparsity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assembly_time_trace(coactivity_trace):\n",
    "    # Set up subplots\n",
    "    num_assemblies = coactivity_trace.shape[1]\n",
    "    fig, ax = plt.subplots(num_assemblies, 1, figsize=(12, 12))\n",
    "\n",
    "    # plot\n",
    "    for i in range(num_assemblies):\n",
    "        ax[i].plot(coactivity_trace[:, i], color='green')\n",
    "        ax[i].set_ylabel(\"A_{}\".format(i+1))\n",
    "        ax[i].set_xlabel(\"Time Steps\")\n",
    "        ax[i].grid()\n",
    "\n",
    "    fig.suptitle(\"Assembly Time Trace\")\n",
    "    plt.savefig(\"stefan_time_trace.png\")\n",
    "\n",
    "def gini(x):\n",
    "    \"\"\"\n",
    "    Calculate the Gini coefficient for a NumPy array of values.\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): 1D array of values.\n",
    "\n",
    "    Returns:\n",
    "        float: Gini coefficient (ranging from 0 to 1).\n",
    "    \"\"\"\n",
    "\n",
    "    total = 0\n",
    "    for i, xi in enumerate(x[:-1], 1):\n",
    "        total += np.sum(np.abs(xi - x[i:]))\n",
    "    return total / (len(x) ** 2 * np.mean(x))\n",
    "\n",
    "def plot_ginis(coactivity_trace):\n",
    "    num_assemblies = coactivity_trace.shape[1]\n",
    "    print(\"Old version \", coactivity_trace[1].shape)\n",
    "    print(\"New Version \", coactivity_trace[:,1].shape)\n",
    "    gini_values = [gini(coactivity_trace[:,i]) for i in range(num_assemblies)]\n",
    "    labels = [f'A {i+1}' for i in range(num_assemblies)]\n",
    "\n",
    "    df = pd.DataFrame({'gini': gini_values, 'labels': labels})\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    ax = sns.barplot(x = df.labels,\n",
    "                     y = df.gini,\n",
    "                     color= (.4, .6, .8, .5)\n",
    "                     )\n",
    "    ax.set_title('Sparsity of Assembly Co-Activity', size = 20)\n",
    "    ax.set_xticklabels(labels, size = 12)\n",
    "    ax.set_ylabel('Gini Coefficient', size = 15)\n",
    "    ax.set_xlabel('Assemblies', size = 15)\n",
    "\n",
    "    # # Create a base bar plot\n",
    "    # plt.figure()\n",
    "    # plt.bar(x=np.arange(len(gini_values)), height=gini_values, tick_label=labels)\n",
    "\n",
    "    # # Apply Labels:\n",
    "    # plt.title(\"Assembly Sparsity\")            # Add a title\n",
    "    # plt.xlabel(\"Assemblies\")                 # Label the x-axis\n",
    "    # plt.ylabel(\"Gini Coefficient\")                     # Label the y-axis\n",
    "\n",
    "    # # Remove chartjunk (redundant elements)\n",
    "    # plt.tick_params(axis='both', which='both', length=0)  # Hide ticks\n",
    "    plt.gca().spines['top'].set_visible(False)            # Hide top spine\n",
    "    plt.gca().spines['right'].set_visible(False)          # Hide right spine\n",
    "\n",
    "    # Add annotations for clarity (optional)\n",
    "    for i, value in enumerate(gini_values):\n",
    "        plt.text(i, value, f'{value:.2f}', ha='center', va='bottom')\n",
    "    plt.savefig('./figure4_plots/sparsity_with_Gini_coefficient_by_assembly.png', dpi = 1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Figure3/map_ordered_to_sgc_output.pickle', 'rb') as f:\n",
    "    map_ordered_to_sgc_output = pickle.load(f)\n",
    "map_ordered_to_sgc_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITY_RASTER = scipy.io.loadmat(\n",
    "    \"../Data/Session13/Assembly_Files/esteps_150000_affinity_04_sessionM409828_13_ACTIVITY-RASTER.mat\", struct_as_record=True, squeeze_me=True)\n",
    "SGC_ASSEMBLIES = scipy.io.loadmat(\n",
    "    \"../Data/Session13/Assembly_Files/esteps_150000_affinity_04_sessionM409828_13_SGC-ASSEMBLIES.mat\", struct_as_record=True, squeeze_me=True)\n",
    "\n",
    "#print(ACTIVITY_RASTER.keys())\n",
    "\n",
    "activity_raster = ACTIVITY_RASTER['activity_raster']\n",
    "activity_raster_peaks = ACTIVITY_RASTER['activity_raster_peaks']\n",
    "\n",
    "coactivity_trace = activity_raster.mean(axis=1)\n",
    "\n",
    "assemblies = SGC_ASSEMBLIES['assemblies']\n",
    "#print(assemblies)\n",
    "assembly_coactivity_trace = np.vstack(\n",
    "    [activity_raster[:, A-1].mean(axis=1) for A in assemblies]).T\n",
    "\n",
    "scores_in_order = np.load('oracle_dists/assemblies_esteps_150000_affinity_04_session13_natural_movie_oracle_scores.npy')\n",
    "\n",
    "correct_order = []\n",
    "for i in map_ordered_to_sgc_output.values():\n",
    "    correct_order.append(i)\n",
    "correct_order = np.array(correct_order)\n",
    "\n",
    "\n",
    "ordered_assembly_coactivity_trace = assembly_coactivity_trace[:,correct_order - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblies = SGC_ASSEMBLIES['assemblies']\n",
    "ordered_assemblies = assemblies[correct_order- 1]\n",
    "for A in ordered_assemblies:\n",
    "    print(len(A))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_ordered_assembly_coactivity_trace = np.vstack(\n",
    "#     [activity_raster[:, A-1].mean(axis=1) for A in ordered_assemblies]).T\n",
    "# new_ordered_assembly_coactivity_trace.shape\n",
    "\n",
    "#get_assembly_time_trace(assembly_coactivity_trace)\n",
    "plot_ginis(ordered_assembly_coactivity_trace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Setting up Natural Movie Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import v1dd_physiology.data_fetching as daf\n",
    "nwb_f = h5py.File('M409828_13_20181213.nwb', 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_id = daf.get_session_id(nwb_f=nwb_f)\n",
    "print(sess_id)\n",
    "\n",
    "plane_ns = daf.get_plane_names(nwb_f=nwb_f)\n",
    "print(\"Planes: \", plane_ns)\n",
    "\n",
    "for plane_n in plane_ns:\n",
    "    depth = daf.get_plane_depth(nwb_f=nwb_f, plane_n=plane_n)\n",
    "    print(f'depth of {plane_n}: {depth} um')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Repeated Natural Movies (11 NATURAL MOVIES SHOWN 9 TIMES)\n",
    "# trial_fluorescence = []\n",
    "presentation = nwb_f['stimulus']['presentation']\n",
    "nm_timestamps = np.array(\n",
    "    presentation['natural_movie'].get('timestamps'))\n",
    "nm_data = np.array(presentation['natural_movie'].get('data')) # columns name dataset, gives you the start time, the end time, and the frame number for the natural movie data that was presented\n",
    "new_clips = np.where(nm_data[:, 2] == 0)[0] # get the index where a new clip rotation begins (frame numbers repeat)\n",
    "clip_duration = 300  \n",
    "\n",
    "# new_clips[1]-1\n",
    "# for repeat_id in range(new_clips.shape[0]):\n",
    "#     frames_to_capture = np.where(f_ts >= nm_timestamps[new_clips[repeat_id]])[\n",
    "#         0][0:clip_duration]\n",
    "#     trial_fluorescence.append(f[frames_to_capture])\n",
    "#     trial_fluorescence_np = np.array(trial_fluorescence)\n",
    "#     for trial_idx in range(trial_fluorescence_np.shape[0]):\n",
    "#         removed_trial = trial_fluorescence_np[trial_idx]\n",
    "#         remaining_trials = np.delete(\n",
    "#             trial_fluorescence_np, trial_idx, 0)\n",
    "#         r, p = scipy.stats.pearsonr(\n",
    "#             removed_trial, np.mean(remaining_trials, 0))\n",
    "#         oracle_array[current_count, trial_idx] = r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ordered_assembly_coactivity_trace\n",
    "passing_roi_count = f.shape[1] # 15\n",
    "\n",
    "coactivity_during_movie = np.zeros((passing_roi_count, 300))\n",
    "results = {}\n",
    "# print(coactivity_trace)\n",
    "f_ts = np.array(nwb_f['processing']['rois_and_traces_plane0']\n",
    "                ['Fluorescence']['f_raw_subtracted'].get('timestamps'))\n",
    "\n",
    "# Get Repeated Natural Movies\n",
    "trial_fluorescence = []\n",
    "presentation = nwb_f['stimulus']['presentation']\n",
    "nm_timestamps = np.array(\n",
    "    presentation['natural_movie'].get('timestamps'))\n",
    "nm_data = np.array(presentation['natural_movie'].get('data'))[:-900] # don't use the last 900 frames, these are the \"short\" nms which are unrelated\n",
    "new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "clip_duration = 300  # new_clips[1]-1\n",
    "\n",
    "clip_ids = []\n",
    "assembly_coactivity_time_traces = []\n",
    "start_of_nms = np.where(f_ts > nm_data[0,0])[0][0] # find the frames where the natural movies start to be presented\n",
    "end_of_nms = np.where(f_ts > nm_data[-1,1])[0][0] # find the frames where the natural movies finish presenting\n",
    "for time_idx in range(start_of_nms, end_of_nms):\n",
    "    idx_nm = np.where(nm_data[:,1] > f_ts[time_idx])[0][0] # get the first index\n",
    "    total_frame_presented = nm_data[idx_nm, 2]\n",
    "    within_repeats_frame_num = total_frame_presented % 3600\n",
    "    clip_id = within_repeats_frame_num // 300\n",
    "    clip_ids.append(clip_id)\n",
    "    assembly_coactivity_time_traces.append(ordered_assembly_coactivity_trace[time_idx,:])\n",
    "\n",
    "clip_ids = np.array(clip_ids).reshape(-1,1)\n",
    "assembly_coactivity_time_traces = np.array(assembly_coactivity_time_traces)\n",
    "\n",
    "####print(trial_fluorescence_np.shape)\n",
    "# for trial_idx in range(trial_fluorescence_np.shape[0]):\n",
    "#     removed_trial = trial_fluorescence_np[trial_idx]\n",
    "#     remaining_trials = np.delete(\n",
    "#         trial_fluorescence_np, trial_idx, 0)\n",
    "#     r, p = scipy.stats.pearsonr(\n",
    "#         removed_trial, np.mean(remaining_trials, 0))\n",
    "#     movie_oracle_r_values[roi_n, trial_idx] = r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_ids.shape, assembly_coactivity_time_traces.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_dff = np.load(\"../Data/Session13/sessionM409828_13_nm_dff.npy\") # stefan extracted\n",
    "nm_events = np.load(\"../Data/Session13/sessionM409828_13_nm_events.npy\") # stefan extracted\n",
    "nm_stimulus = np.load(\"../Data/Session13/sessionM409828_13_nm_stimulus.npy\") \n",
    "# nm_stimulus is not time locked to the fluorescence presentation, should not use it for defining clip_ids. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Decoding of the Natural Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clip_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# # Initialize lists to store accuracy for each neuron\n",
    "# accuracies = []\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(assembly_coactivity_time_traces, clip_ids, test_size=0.2, random_state=74)\n",
    "\n",
    "# # Initialize and train the Lasso Regression model\n",
    "# alpha = 0.1  # Regularization parameter\n",
    "# lasso_model = Ridge(alpha=alpha)\n",
    "# lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = lasso_model.predict(X_test)\n",
    "\n",
    "# # Round the predictions to the nearest integer to get the decoded clip IDs\n",
    "# decoded_clip_ids = predictions.round().astype(int)\n",
    "\n",
    "# # Print out the accuracy\n",
    "# print(\"Accuracy Score of\", accuracy_score(y_test, decoded_clip_ids))\n",
    "\n",
    "\n",
    "# # # Create a dataframe to store the results\n",
    "# # results_df = pd.DataFrame({'Assembly': range(1, num_assemblies + 1), 'Accuracy': accuracies})\n",
    "\n",
    "# # # Plot the results using Seaborn\n",
    "# # plt.figure(figsize=(10, 6))\n",
    "# # sns.barplot(x='Assembly', y='Accuracy', data=results_df, palette='viridis')\n",
    "# # plt.title('Accuracy of Decoding Natural Movie Clip IDs for Each Assembly')\n",
    "# # plt.xlabel('Neuron')\n",
    "# # plt.ylabel('Accuracy')\n",
    "# # plt.ylim(0, 1)  # Set y-axis limit to [0, 1] for accuracy range\n",
    "# # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # # assuming clip_ids and assembly_coactivity_time_traces are your numpy arrays\n",
    "# # clip_ids = np.random.randint(0, 12, size=(13087, 1)) \n",
    "# # assembly_coactivity_time_traces = np.random.rand(13087, 15)\n",
    "\n",
    "# # split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(assembly_coactivity_time_traces, clip_ids.ravel(), test_size=0.2, random_state=74)\n",
    "\n",
    "# # train the model\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=74)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # predict the clip_ids\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # print out the accuracy\n",
    "# print(\"Accuracy Score of\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# # The columns represent the original or expected class distribution, and the \n",
    "# # rows represent the predicted or output distribution by the classifier.\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sns.heatmap(cm, annot=True, vmax = 100)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Truth')\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(assembly_coactivity_time_traces, clip_ids.ravel(), test_size=0.2, random_state=74)\n",
    "\n",
    "# # define the model\n",
    "# clf = RandomForestClassifier(random_state=74)\n",
    "\n",
    "# # define the parameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# # create the grid search object\n",
    "# grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# # fit the grid search\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # get the best model\n",
    "# best_clf = grid_search.best_estimator_\n",
    "\n",
    "# # predict the clip_ids\n",
    "# y_pred = best_clf.predict(X_test)\n",
    "\n",
    "# # print out the accuracy\n",
    "# print(\"Accuracy Score of\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sns.heatmap(cm, annot=True, vmax = 100,cmap=greymap, annot_kws={\"size\": 35 / np.sqrt(len(cm))})\n",
    "# plt.title(\"Assembly Clip ID Classification with Random Forrest Classifier\", fontsize=16)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Truth')\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(assembly_coactivity_time_traces, clip_ids.ravel(), test_size=0.2, random_state=74)\n",
    "\n",
    "# #SVMs work best when the data is scaled\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# # fit the scaler to the training data and transform it, and apply it to test\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # define the model\n",
    "# clf = SVC(random_state=74)\n",
    "\n",
    "# # define the parameter grid\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1, 10, 100],\n",
    "#     'gamma': [1, 0.1, 0.01, 0.001],\n",
    "#     'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "# }\n",
    "\n",
    "# # create the grid search object\n",
    "# grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# # fit the grid search\n",
    "# grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # get the best model\n",
    "# best_clf = grid_search.best_estimator_\n",
    "\n",
    "# # predict the clip_ids\n",
    "# y_pred = best_clf.predict(X_test_scaled)\n",
    "\n",
    "# # print out the accuracy\n",
    "# print(\"Accuracy Score of\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sns.heatmap(cm, annot=True, vmax = 100, annot_kws={\"size\": 35 / np.sqrt(len(cm))})\n",
    "# plt.title(\"Assembly Clip ID Classification with Support Vector Machine\", fontsize=16)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Truth')\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(assembly_coactivity_time_traces, clip_ids.ravel(), test_size=0.2, random_state=74)\n",
    "\n",
    "# # scale the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # define the model\n",
    "# clf = MLPClassifier(random_state=74)\n",
    "\n",
    "# # define the parameter grid: checked on all and relu provided best score (makes results comparable between assembly and null sets as well)\n",
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (50, 50, 100), (100, 100)],\n",
    "#     'activation': ['relu'],\n",
    "#     'solver': ['sgd', 'adam'],\n",
    "#     'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "#     'learning_rate': ['constant','adaptive', 'invscaling'],\n",
    "#     'batch_size': [64, 128, 256, 512, 1024],\n",
    "#     'max_iter': [500]\n",
    "# }\n",
    "\n",
    "# # create the grid search object\n",
    "# grid_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# # fit the grid search\n",
    "# grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # get the best model\n",
    "# best_clf = grid_search.best_estimator_\n",
    "\n",
    "# # predict the clip_ids\n",
    "# y_pred = best_clf.predict(X_test_scaled)\n",
    "\n",
    "# # print out the accuracy\n",
    "# print(\"Accuracy Score of\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sns.heatmap(cm, annot=True, vmax = 100,cmap=greymap, annot_kws={\"size\": 35 / np.sqrt(len(cm))})\n",
    "# plt.title(\"Assembly Clip ID Classification with MLPClassifier\", fontsize=16)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Truth')\n",
    "# plt.savefig('./figure4_plots/assembly_clip_id_decoder_MLPClassifier.png', dpi = 1200)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n Best estimator:')\n",
    "# print(grid_search.best_estimator_)\n",
    "# print('\\n Best hyperparameters:')\n",
    "# print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Develop normalized Heatmap\n",
    "# cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sns.heatmap(cm_norm, annot=True, vmax = 1, cmap=greymap, fmt=\".3f\", annot_kws={\"size\": 35 / np.sqrt(len(cm))})\n",
    "# plt.title(\"Assembly Clip ID Classification Percentage with MLPClassifier\", fontsize=16)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Truth')\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.savefig('./figure4_plots/assembly_clip_id_percentage_decoder_MLPClassifier.png', dpi = 1200)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clip_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Producing Decoder Null Model\n",
    "\n",
    " Null Model to Assemblies will be Random 'Ensemble' of Neurons of the same sizes\n",
    "\n",
    "\n",
    "\n",
    " Framework:\n",
    "\n",
    " 1. Produce 'random_ensembles' with ids similar to the setup of assemblies\n",
    "\n",
    " 2. Check the raster plots produced by first algorithmic step of SGC to see if we can use that\n",
    "\n",
    " 3. Calculate a co-activity trace of each 'random_ensemble', check the trace to those of assemblies to see differences\n",
    "\n",
    " 4. Prepare Decoding Framework by setting the time scale for coactiivity traces to the same as the clip ids\n",
    "\n",
    " 5. Run decoding framework in the same way that was produced with assemblies and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 1: Produce 'random_ensembles': Random Collections of Neurons that are same sizes as the assemblies. \n",
    "\n",
    "### Set the seed for reproducability, outside of the loop.\n",
    "random.seed(47)\n",
    "np.random.seed(47)\n",
    "\n",
    "### Overlap is fine so we don't have to worry about that.\n",
    "SGC_ASSEMBLIES = scipy.io.loadmat(\n",
    "    \"../Data/Session13/Assembly_Files/esteps_150000_affinity_04_sessionM409828_13_SGC-ASSEMBLIES.mat\", struct_as_record=True, squeeze_me=True)\n",
    "assemblies = SGC_ASSEMBLIES['assemblies']\n",
    "\n",
    "dF_trace = np.load(\"../Data/Session13/sessionM409828_13_CALCIUM-FLUORESCENCE.npy\")\n",
    "num_neurons = dF_trace.shape[1]\n",
    "\n",
    "random_ensembles = []\n",
    "for A in assemblies:\n",
    "    curr_length = len(A)\n",
    "    # get random ids, make sure there are no repeats in the ids for that specific ensemble\n",
    "    random_ensembles.append(np.sort(np.array(random.sample(range(num_neurons), curr_length))))\n",
    "\n",
    "# Order the random ensembles by size\n",
    "random_ensembles.sort(key = len)\n",
    "random_ensembles.reverse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_ensembles.pkl', 'wb') as f:\n",
    "    pickle.dump(random_ensembles, f)\n",
    "random_ensembles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Assembly Lengths\")\n",
    "assemblies = list(assemblies)\n",
    "assemblies.sort(key = len)\n",
    "assemblies.reverse()\n",
    "print([len(A) for A in assemblies])\n",
    "\n",
    "print(\"Random Ensemble Lengths\")\n",
    "print([len(r) for r in random_ensembles])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 2: Use the raster plots produced by SGC\n",
    "### STEP 3: Produce a co-activity trace of each random_ensemble\n",
    "ACTIVITY_RASTER = scipy.io.loadmat(\n",
    "    \"../Data/Session13/Assembly_Files/esteps_150000_affinity_04_sessionM409828_13_ACTIVITY-RASTER.mat\", struct_as_record=True, squeeze_me=True)\n",
    "\n",
    "activity_raster = ACTIVITY_RASTER['activity_raster']\n",
    "#activity_raster_peaks = ACTIVITY_RASTER['activity_raster_peaks']\n",
    "\n",
    "coactivity_trace = activity_raster.mean(axis=1)\n",
    "\n",
    "# Assembly Coactivity Trace\n",
    "assembly_coactivity_trace = np.vstack(\n",
    "    [activity_raster[:, A-1].mean(axis=1) for A in assemblies]).T\n",
    "\n",
    "# Random Ensemble Coactivity Trace\n",
    "random_ensembles_coactivity_trace = np.vstack(\n",
    "    [activity_raster[:, A-1].mean(axis=1) for A in random_ensembles]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_time_trace(coactivity_trace, title):\n",
    "    # Set up three subplots\n",
    "    num_assemblies = coactivity_trace.shape[1]\n",
    "    fig, ax = plt.subplots(num_assemblies, 1, figsize=(12, 12))\n",
    "\n",
    "    # plot\n",
    "    for i in range(num_assemblies):\n",
    "        ax[i].plot(coactivity_trace[:, i], color=(.4, .6, .8, .5))\n",
    "        ax[i].set_ylabel(\"Co-A\")\n",
    "        ax[i].set_xlabel(\"Time Steps\")\n",
    "        ax[i].grid()\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    #plt.savefig(f\"oracle_dists2/assemblies_esteps_150000_affinity_04_{name}.png\", dpi = 1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the assembly coactivity traces in order\n",
    "# scores_in_order = np.load('oracle_dists/assemblies_esteps_150000_affinity_04_session13_natural_movie_oracle_scores.npy')\n",
    "# get_ensemble_time_trace(assembly_coactivity_trace, title = \"Co-Activity Time Trace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the random ensemble coactivity traces in order\n",
    "scores_in_order = np.load('oracle_dists/assemblies_esteps_150000_affinity_04_session13_natural_movie_oracle_scores.npy')\n",
    "get_ensemble_time_trace(random_ensembles_coactivity_trace, title = \"Random Ensemble Time Trace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check UpSet Plot as a sanity check that we got a proper random sample\n",
    "RE = {}\n",
    "for i, ensemble in enumerate(random_ensembles):\n",
    "    print(f'RE {i + 1} Size: {len(ensemble)} Neurons')\n",
    "    RE[f\"RE {i + 1}\"] = ensemble - 1 # Correct the IDs for Python Indexing\n",
    "all_sets = upsetplot.from_contents(RE)\n",
    "\n",
    "ax_dict = upsetplot.UpSet(all_sets, subset_size='count', min_subset_size= 10, \n",
    "                            show_counts = True, show_percentages = False,\n",
    "                            sort_by = 'cardinality', facecolor=\"grey\").plot()\n",
    "plt.title(\"Intersection of Random Ensembles in Scan 1.3 of V1DD\", size = 24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "### Compare the assembly coactivity traces to the random ensemble coactivity traces\n",
    "assembly_traces = assembly_coactivity_trace\n",
    "random_ensemble_traces = random_ensembles_coactivity_trace\n",
    "# scores_in_order\n",
    "# def correlate_assembly_traces_vs_random_ensembles(assembly_traces, random_ensemble_traces, scores_in_order, name='compared_time_traces'):\n",
    "# Set up three subplots\n",
    "num_assemblies = assembly_traces.shape[1]\n",
    "# fig, ax = plt.subplots(num_assemblies, 1, figsize=(12, 12))\n",
    "\n",
    "# plot\n",
    "# for i in range(num_assemblies):\n",
    "#     ax[i].plot(assembly_traces[:, i], color='green', label='Assembly')\n",
    "#     ax[i].plot(random_ensemble_traces[:, i], color='red', label='Random Ensemble')\n",
    "#     ax[i].set_ylabel(f\"{scores_in_order[i]:.2}\")\n",
    "#     ax[i].set_xlabel(\"Time Steps\")\n",
    "#     ax[i].grid()\n",
    "#     ax[i].legend()\n",
    "\n",
    "# fig.suptitle(\"Assembly Time Trace vs Random Ensemble Time Trace\")\n",
    "# Calculate mean r score between all pairs of assemblies\n",
    "assembly_r_scores = []\n",
    "for i in range(num_assemblies):\n",
    "    for j in range(num_assemblies):\n",
    "        if i != j:\n",
    "            assembly_r_scores.append(stats.pearsonr(assembly_traces[:, i], assembly_traces[:, j])[0])\n",
    "print(f\"Mean R Score: {np.mean(assembly_r_scores)}\")\n",
    "# Cacuate mean r score between all pairs of random ensembles\n",
    "null_r_scores = []\n",
    "for i in range(num_assemblies):\n",
    "    for j in range(num_assemblies):\n",
    "        if i != j:\n",
    "            null_r_scores.append(stats.pearsonr(random_ensemble_traces[:, i], random_ensemble_traces[:, j])[0])\n",
    "    # print(stats.pearsonr(random_ensemble_traces[:, i], random_ensemble_traces[:, i])[0])\n",
    "    # print()\n",
    "    # null_r_scores.append(stats.pearsonr(random_ensemble_traces[:, i], random_ensemble_traces[:, j])[0])\n",
    "print(f\"Mean R Score Random Ensemble: {np.mean(null_r_scores)}\")\n",
    "#perform a wicoxen ranksum test on the two sets of r scores\n",
    "print(stats.ranksums(assembly_r_scores, null_r_scores))\n",
    "print(stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(null_r_scores).flatten(), alternative = 'less').pvalue)\n",
    "\n",
    "\n",
    "def calculate_pearson_matrix(data):\n",
    "    # Subtract the mean of each time series\n",
    "    data_mean_subtracted = data - np.mean(data, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute the covariance matrix\n",
    "    covariance_matrix = np.dot(data_mean_subtracted, data_mean_subtracted.T)\n",
    "    \n",
    "    # Compute the standard deviation for each time series\n",
    "    std_devs = np.sqrt(np.sum(data_mean_subtracted**2, axis=1))\n",
    "    \n",
    "    # Compute the Pearson correlation matrix\n",
    "    pearson_matrix = covariance_matrix / np.outer(std_devs, std_devs)\n",
    "    \n",
    "    return pearson_matrix\n",
    "\n",
    "a_cell_r_scores = []\n",
    "no_a_cell_r_scores = []\n",
    "all_cell_r_scores = []\n",
    "print(\"Activity raster shape:\", activity_raster.shape)\n",
    "raster_pearson = calculate_pearson_matrix(activity_raster.T)\n",
    "for i in tqdm(range(activity_raster.shape[1])):\n",
    "    for j in range(activity_raster.shape[1]):\n",
    "        if i != j:\n",
    "            r = raster_pearson[i, j]\n",
    "            all_cell_r_scores.append(r)\n",
    "            if i in assembly_neurons and j in assembly_neurons:\n",
    "                a_cell_r_scores.append(r)\n",
    "            elif i not in assembly_neurons and j not in assembly_neurons:\n",
    "                no_a_cell_r_scores.append(r)\n",
    "\n",
    "# a_cell_r_scores = []\n",
    "# no_a_cell_r_scores = []\n",
    "# all_cell_r_scores = []\n",
    "# for i in tqdm(range(activity_raster.shape[1])):\n",
    "#     for j in range(activity_raster.shape[1]):\n",
    "#         if i != j:\n",
    "#             r = stats.pearsonr(activity_raster[:, i], activity_raster[:, j])[0]\n",
    "#             all_cell_r_scores.append(r)\n",
    "#             if i in assembly_neurons and j in assembly_neurons:\n",
    "#                 a_cell_r_scores.append(r)\n",
    "#             elif i not in assembly_neurons and j not in assembly_neurons:\n",
    "#                 no_a_cell_r_scores.append(r)\n",
    "\n",
    "#plt\n",
    "\n",
    "# plt.figure(figsize=(14, 10))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(14, 10))\n",
    "# # Flatten the arrays\n",
    "# all_arr = [np.array(assembly_r_scores).flatten(), np.array(null_r_scores).flatten()]\n",
    "\n",
    "# # Set the theme to whitegrid for simplicity\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# # Get the Blues_r color palette and shuffle it\n",
    "# palette = sns.color_palette(\"Blues_r\", n_colors=len(all_arr))\n",
    "# random.shuffle(palette)\n",
    "\n",
    "# # Violin Plot\n",
    "# ax = sns.violinplot(data=all_arr, palette=palette, saturation=0.5, linewidth=3, native_scale=True, common_norm=True, inner='box', inner_kws=dict(box_width=5, whis_width=2))\n",
    "\n",
    "# # Remove top and right spines\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.spines['right'].set_visible(False)\n",
    "\n",
    "# # Set the labels and title\n",
    "# ax.set_xticklabels([\"Assemblies\", \"Random Ensembles\"], size=20, weight='bold')\n",
    "# ax.set_title('Correlation of Assemblies and Random Ensembles', size=24, weight='bold')\n",
    "# ax.set_ylabel('Correlation (R)', size=22, weight='bold')\n",
    "# plt.yticks(fontsize=20, weight='bold')\n",
    "\n",
    "# # Calculate medians and vertical offset\n",
    "# medians = np.array([np.median(np.array(assembly_r_scores).flatten()), np.median(np.array(null_r_scores).flatten())])\n",
    "# vertical_offset = medians * 0.01\n",
    "\n",
    "np.save('oracle_dists/assembly_r_scores.npy', assembly_r_scores)\n",
    "np.save('oracle_dists/null_r_scores.npy', null_r_scores)\n",
    "np.save('oracle_dists/raster_pearson.npy', raster_pearson)\n",
    "\n",
    "# # Calculate p-value\n",
    "p_value_assembly_null = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(null_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "p_value_assembly_all = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(all_cell_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "p_value_assembly_a = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(a_cell_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "p_value_assembly_no_a = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(no_a_cell_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "p_value_a_no_a = stats.ranksums(np.array(a_cell_r_scores).flatten(), np.array(no_a_cell_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "p_value_all_v_a = stats.ranksums(np.array(all_cell_r_scores).flatten(), np.array(a_cell_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "\n",
    "p_value_array = np.array([p_value_assembly_null, p_value_assembly_all, p_value_assembly_a, p_value_assembly_no_a, p_value_a_no_a, p_value_all_v_a])\n",
    "corrected_p_value_array = corrected_pvals = multipletests(p_value_array, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "print('\\nOracle P-Values and Test Statistics')\n",
    "print(f'Assembly vs Null: {stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(null_r_scores).flatten(), alternative=\"two-sided\")}') \n",
    "print(f'Assembly vs All: {stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(all_cell_r_scores).flatten(), alternative=\"two-sided\")}')\n",
    "print(f'Assembly vs Assembly Cells: {stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(a_cell_r_scores).flatten(), alternative=\"two-sided\")}')\n",
    "print(f'Assembly vs Non-Assembly Cells: {stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(no_a_cell_r_scores).flatten(), alternative=\"two-sided\")}')\n",
    "print(f'Assembly Cells vs Non-Assembly Cells: {stats.ranksums(np.array(a_cell_r_scores).flatten(), np.array(no_a_cell_r_scores).flatten(), alternative=\"two-sided\")}')\n",
    "\n",
    "print('\\nBH Corrected P-Values')\n",
    "print(f'Assembly vs Null: {corrected_p_value_array[1][0]}')\n",
    "print(f'Assembly vs All: {corrected_p_value_array[1][1]}')\n",
    "print(f'Assembly vs Assembly Cells: {corrected_p_value_array[1][2]}')\n",
    "print(f'Assembly vs Non-Assembly Cells: {corrected_p_value_array[1][3]}')\n",
    "print(f'Assembly Cells vs Non-Assembly Cells: {corrected_p_value_array[1][4]}')\n",
    "print(f'All vs Assembly Cells: {corrected_p_value_array[1][5]}')\n",
    "\n",
    "p_value_assembly_null = corrected_p_value_array[1][0]\n",
    "p_value_assembly_all = corrected_p_value_array[1][1]\n",
    "p_value_assembly_a = corrected_p_value_array[1][2]\n",
    "p_value_assembly_no_a = corrected_p_value_array[1][3]\n",
    "p_value_a_no_a = corrected_p_value_array[1][4]\n",
    "p_value_all_v_a = corrected_p_value_array[1][5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ptitprince as pt\n",
    "# Create a figure\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Prepare data for raincloud plot\n",
    "# print(all_arr)\n",
    "print(len(assembly_r_scores))\n",
    "print(len(null_r_scores))\n",
    "data = pd.DataFrame({\n",
    "    \"Values\": np.concatenate((np.array(assembly_r_scores), np.array(null_r_scores))),\n",
    "    \"Group\": [f\"Assembly\\n(n={len(assembly_r_scores)})\"] * len(assembly_r_scores) + \\\n",
    "                [f\"Null Ensembles\\n(n={len(null_r_scores)})\"] * len(null_r_scores)\n",
    "})\n",
    "\n",
    "# Prepare data for raincloud plot\n",
    "data = pd.DataFrame({\n",
    "    \"Activity Correlations\": np.concatenate((np.array(assembly_r_scores), np.array(null_r_scores), np.array(all_cell_r_scores), np.array(a_cell_r_scores), np.array(no_a_cell_r_scores))),\n",
    "    \"Cell Grouping\": [f\"Assemblies\\nn={len(assembly_r_scores)}\"] * len(assembly_r_scores) + \\\n",
    "                [f\"Random\\nEnsembles\\nn={len(null_r_scores)}\"] * len(null_r_scores) + \\\n",
    "                [f\"All\\nCells\\nn={len(all_cell_r_scores)}\"] * len(all_cell_r_scores) + \\\n",
    "                [f\"Assembly\\nCells\\nn={len(a_cell_r_scores)}\"] * len(a_cell_r_scores) + \\\n",
    "                [f\"Non-Assembly\\nCells\\nn={len(no_a_cell_r_scores)}\"] * len(no_a_cell_r_scores)\n",
    "})\n",
    "\n",
    "# x_labels = [f\"Assemblies\\nn={len(assembly_r_scores)}\", f\"Random\\nEnsembles\\nn={len(null_r_scores)}\", f\"All\\nCells\\nn={len(all_cell_r_scores)}\", f\"Assembly\\nCells\\nn={len(a_cell_r_scores)}\", f\"Non-Assembly\\nCells\\nn={len(no_a_cell_r_scores)}\"]\n",
    "\n",
    "print('Raincloud...')\n",
    "# Create the raincloud plot\n",
    "ax = pt.RainCloud(\n",
    "    y=\"Activity Correlations\",\n",
    "    x=\"Cell Grouping\",\n",
    "    data=data,\n",
    "    palette=[(.4, .6, .8, .5), 'grey'],\n",
    "    width_viol=0.6,  # Adjust violin width\n",
    "    alpha=0.8,  # Transparency of the cloud\n",
    "    move=0.1,  # Adjust position of violins\n",
    "    point_size = 6,\n",
    "    orient=\"v\",  # Horizontal orientation\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "\n",
    "# # Calculate p-value\n",
    "p_value_assembly_null = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(null_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "p_value_assembly_all = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(all_cell_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "p_value_assembly_a = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(a_cell_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "p_value_assembly_no_a = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(no_a_cell_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "p_value_a_no_a = stats.ranksums(np.array(a_cell_r_scores).flatten(), np.array(no_a_cell_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "p_value_all_v_a = stats.ranksums(np.array(all_cell_r_scores).flatten(), np.array(a_cell_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "\n",
    "p_value_array = np.array([p_value_assembly_null, p_value_assembly_all, p_value_assembly_a, p_value_assembly_no_a, p_value_a_no_a, p_value_all_v_a])\n",
    "corrected_p_value_array = multipletests(p_value_array, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "# print(p_value_array.shape)\n",
    "# print(p_value_array)\n",
    "# # print(corrected_p_value_array.shape)\n",
    "# print(corrected_p_value_array)\n",
    "\n",
    "print('\\nCorrelation P-Values and Test Statistics')\n",
    "print(f'Assembly vs Null: {stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(null_r_scores).flatten(), alternative=\"two-sided\")}')\n",
    "print(f'Assembly vs All: {stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(all_cell_r_scores).flatten(), alternative=\"two-sided\")}')\n",
    "print(f'Assembly vs Assembly Cells: {stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(a_cell_r_scores).flatten(), alternative=\"two-sided\")}')\n",
    "print(f'Assembly vs Non-Assembly Cells: {stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(no_a_cell_r_scores).flatten(), alternative=\"two-sided\")}')\n",
    "print(f'Assembly Cells vs Non-Assembly Cells: {stats.ranksums(np.array(a_cell_r_scores).flatten(), np.array(no_a_cell_r_scores).flatten(), alternative=\"two-sided\")}')\n",
    "print(f'All vs Assembly Cells: {stats.ranksums(np.array(all_cell_r_scores).flatten(), np.array(a_cell_r_scores).flatten(), alternative=\"two-sided\")}')\n",
    "\n",
    "print(corrected_p_value_array)\n",
    "\n",
    "print('\\nBH Corrected P-Values')\n",
    "print(f'Assembly vs Null: {corrected_p_value_array[1][0]}')\n",
    "print(f'Assembly vs All: {corrected_p_value_array[1][1]}')\n",
    "print(f'Assembly vs Assembly Cells: {corrected_p_value_array[1][2]}')\n",
    "print(f'Assembly vs Non-Assembly Cells: {corrected_p_value_array[1][3]}')\n",
    "print(f'Assembly Cells vs Non-Assembly Cells: {corrected_p_value_array[1][4]}')\n",
    "print(f'All vs Assembly Cells: {corrected_p_value_array[1][5]}')\n",
    "\n",
    "p_value_assembly_null = corrected_p_value_array[1][0]\n",
    "p_value_assembly_all = corrected_p_value_array[1][1]\n",
    "p_value_assembly_a = corrected_p_value_array[1][2]\n",
    "p_value_assembly_no_a = corrected_p_value_array[1][3]\n",
    "p_value_a_no_a = corrected_p_value_array[1][4]\n",
    "p_value_all_v_a = corrected_p_value_array[1][5]\n",
    "\n",
    "plt.ylim(-0.1, 1.2)  # Set y-axis limit to [0, 1] for accuracy range\n",
    "\n",
    "# # Add significance stars\n",
    "for p_index, p_value in enumerate([p_value_assembly_null, p_value_assembly_all, p_value_assembly_a, p_value_assembly_no_a, p_value_all_v_a, p_value_a_no_a]):\n",
    "    if p_value < 0.001:\n",
    "        significance = 'P < 0.001\\n***'\n",
    "    elif p_value < 0.01:\n",
    "        significance = 'P < 0.01\\n**'\n",
    "    elif p_value < 0.05:\n",
    "        significance = 'P < 0.05\\n*'\n",
    "    else:\n",
    "        significance = 'ns'\n",
    "\n",
    "    # Add p-value annotation\n",
    "    if p_index == 0:\n",
    "        p_offset = 0.5\n",
    "        p_height = 1.05\n",
    "    elif p_index > 0 and p_index < 4:\n",
    "        p_offset = 1.0 + p_index\n",
    "        p_height = 1.05\n",
    "    elif p_index == 4:\n",
    "        p_offset = 2.5\n",
    "        p_height = 0.5\n",
    "    elif p_index == 5:\n",
    "        p_offset = 3.5\n",
    "        p_height = 0.5\n",
    "\n",
    "    ax.text(p_offset, p_height, f'{significance}', \n",
    "                color='black', ha='center', va='bottom', fontsize=16, weight='bold')\n",
    "\n",
    "# ax.annotate(f'P-Val: {p_value:.3g}', xy=(1, medians[1] + vertical_offset[1]), xytext=(1, medians[1] + vertical_offset[1] + 0.05),\n",
    "#             arrowprops=dict(facecolor='black', shrink=0.05), horizontalalignment='center', size=18, color='black', weight='semibold')\n",
    "\n",
    "# BREADCRUMB: RESTORE BELOW TO END OF CELL\n",
    "# print('Finishing off figure...')\n",
    "# # Add a multiline title to include the p-value, add y_label\n",
    "# title = f'Correlation'\n",
    "# plt.title(title, size=24)\n",
    "# plt.xlabel('Correlation (r)', size=20)\n",
    "# plt.xticks(fontsize=20)  # Adjust size of xticks\n",
    "# plt.yticks(fontsize=20)  # Adjust size of yticks\n",
    "# plt.ylabel(\"Groups\", size=20)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('./figure4_plots/correlations_assemblies_vs_random_ensembles_raincould_plot.png', dpi=1200)\n",
    "# plt.savefig('./figure4_plots/correlations_assemblies_vs_random_ensembles_raincould_plot.pdf', dpi=1200)\n",
    "# plt.savefig('./figure4_plots/correlations_assemblies_vs_random_ensembles_raincould_plot.svg', dpi=1200)\n",
    "# plt.close()\n",
    "\n",
    "# Early Exit Breakoff Breadcrumb\n",
    "# exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare correlation of asesmblies and random ensembles\n",
    "# correlate_assembly_traces_vs_random_ensembles(assembly_coactivity_trace, random_ensembles_coactivity_trace, scores_in_order, name='assemblies_vs_ensembles')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Prepare Decoding Framework in the same way as Assemblies\n",
    "print('Decoding...')\n",
    "nwb_f = h5py.File('M409828_13_20181213.nwb', 'r')\n",
    "\n",
    "# Get Repeated Natural Movies (12 NATURAL MOVIES SHOWN 9 TIMES)\n",
    "# trial_fluorescence = []\n",
    "presentation = nwb_f['stimulus']['presentation']\n",
    "nm_timestamps = np.array(\n",
    "    presentation['natural_movie'].get('timestamps'))\n",
    "nm_data = np.array(presentation['natural_movie'].get('data')) # columns name dataset, gives you the start time, the end time, and the frame number for the natural movie data that was presented\n",
    "new_clips = np.where(nm_data[:, 2] == 0)[0] # get the index where a new clip rotation begins (frame numbers repeat)\n",
    "clip_duration = 300  \n",
    "\n",
    "f = random_ensembles_coactivity_trace\n",
    "passing_roi_count = f.shape[1] # 15\n",
    "\n",
    "coactivity_during_movie = np.zeros((passing_roi_count, 300))\n",
    "results = {}\n",
    "# print(coactivity_trace)\n",
    "f_ts = np.array(nwb_f['processing']['rois_and_traces_plane0']\n",
    "                ['Fluorescence']['f_raw_subtracted'].get('timestamps'))\n",
    "\n",
    "# Get Repeated Natural Movies\n",
    "trial_fluorescence = []\n",
    "presentation = nwb_f['stimulus']['presentation']\n",
    "nm_timestamps = np.array(\n",
    "    presentation['natural_movie'].get('timestamps'))\n",
    "nm_data = np.array(presentation['natural_movie'].get('data'))[:-900] # don't use the last 900 frames, these are the \"short\" nms which are unrelated\n",
    "new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "clip_duration = 300  # new_clips[1]-1\n",
    "\n",
    "clip_ids = []\n",
    "random_ensemble_coactivity_time_traces = []\n",
    "start_of_nms = np.where(f_ts > nm_data[0,0])[0][0] # find the frames where the natural movies start to be presented\n",
    "end_of_nms = np.where(f_ts > nm_data[-1,1])[0][0] # find the frames where the natural movies finish presenting\n",
    "for time_idx in tqdm(range(start_of_nms, end_of_nms)):\n",
    "    idx_nm = np.where(nm_data[:,1] > f_ts[time_idx])[0][0] # get the first index\n",
    "    total_frame_presented = nm_data[idx_nm, 2]\n",
    "    within_repeats_frame_num = total_frame_presented % 3600\n",
    "    clip_id = within_repeats_frame_num // 300\n",
    "    clip_ids.append(clip_id)\n",
    "    random_ensemble_coactivity_time_traces.append(random_ensembles_coactivity_trace[time_idx,:])\n",
    "\n",
    "clip_ids = np.array(clip_ids).reshape(-1,1)\n",
    "random_ensemble_coactivity_time_traces = np.array(random_ensemble_coactivity_time_traces)\n",
    "\n",
    "print(clip_ids.shape, random_ensemble_coactivity_time_traces.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Step 5: Run Decoding Metric with Random Ensembles\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# # Initialize lists to store accuracy for each neuron\n",
    "# accuracies = []\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(random_ensemble_coactivity_time_traces, clip_ids, test_size=0.2, random_state=74)\n",
    "\n",
    "# # Initialize and train the Lasso Regression model\n",
    "# alpha = 0.1  # Regularization parameter\n",
    "# lasso_model = Ridge(alpha=alpha)\n",
    "# lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = lasso_model.predict(X_test)\n",
    "\n",
    "# # Round the predictions to the nearest integer to get the decoded clip IDs\n",
    "# decoded_clip_ids = predictions.round().astype(int)\n",
    "\n",
    "# # Print out the accuracy\n",
    "# print(\"Accuracy Score of\", accuracy_score(y_test, decoded_clip_ids))\n",
    "\n",
    "\n",
    "# # # Create a dataframe to store the results\n",
    "# # results_df = pd.DataFrame({'Assembly': range(1, num_assemblies + 1), 'Accuracy': accuracies})\n",
    "\n",
    "# # # Plot the results using Seaborn\n",
    "# # plt.figure(figsize=(10, 6))\n",
    "# # sns.barplot(x='Assembly', y='Accuracy', data=results_df, palette='viridis')\n",
    "# # plt.title('Accuracy of Decoding Natural Movie Clip IDs for Each Assembly')\n",
    "# # plt.xlabel('Neuron')\n",
    "# # plt.ylabel('Accuracy')\n",
    "# # plt.ylim(0, 1)  # Set y-axis limit to [0, 1] for accuracy range\n",
    "# # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(random_ensemble_coactivity_time_traces, clip_ids.ravel(), test_size=0.2, random_state=74)\n",
    "\n",
    "# # define the model\n",
    "# clf = RandomForestClassifier(random_state=74)\n",
    "\n",
    "# # define the parameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# # create the grid search object\n",
    "# grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# # fit the grid search\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # get the best model\n",
    "# best_clf = grid_search.best_estimator_\n",
    "\n",
    "# # predict the clip_ids\n",
    "# y_pred = best_clf.predict(X_test)\n",
    "\n",
    "# # print out the accuracy\n",
    "# print(\"Accuracy Score of\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sns.heatmap(cm, annot=True, vmax = 100,cmap=greymap, annot_kws={\"size\": 35 / np.sqrt(len(cm))})\n",
    "# plt.title(\"Random Ensemble Clip ID Classification with Random Forrest Classifier\", fontsize=16)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Truth')\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(random_ensemble_coactivity_time_traces, clip_ids.ravel(), test_size=0.2, random_state=74)\n",
    "\n",
    "# # scale the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # define the model\n",
    "# clf = MLPClassifier(random_state=74)\n",
    "\n",
    "# # define the parameter grid: checked on all and relu provided best score (makes results comparable between assembly and null sets as well)\n",
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (50, 50, 100), (100, 100)],\n",
    "#     'activation': ['relu'],\n",
    "#     'solver': ['sgd', 'adam'],\n",
    "#     'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "#     'learning_rate': ['constant','adaptive', 'invscaling'],\n",
    "#     'batch_size': [64, 128, 256, 512, 1024],\n",
    "#     'max_iter': [500]\n",
    "# }\n",
    "\n",
    "# # create the grid search object\n",
    "# grid_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# # fit the grid search\n",
    "# grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # get the best model\n",
    "# best_clf = grid_search.best_estimator_\n",
    "\n",
    "# # predict the clip_ids\n",
    "# y_pred = best_clf.predict(X_test_scaled)\n",
    "\n",
    "# # print out the accuracy\n",
    "# print(\"Accuracy Score of\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sns.heatmap(cm, annot=True, vmax = 100,cmap=greymap, annot_kws={\"size\": 35 / np.sqrt(len(cm))})\n",
    "# plt.title(\"Random Ensemble Clip ID Classification with MLPClassifier\", fontsize=16)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Truth')\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.savefig('./figure4_plots/random_ensemble_clip_id_decoder_MLPClassifier.png', dpi = 1200)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n Best estimator:')\n",
    "# print(grid_search.best_estimator_)\n",
    "# print('\\n Best hyperparameters:')\n",
    "# print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Develop normalized Heatmap\n",
    "# cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sns.heatmap(cm_norm, annot=True, cmap=greymap, vmax = 1, fmt=\".3f\", annot_kws={\"size\": 35 / np.sqrt(len(cm))})\n",
    "# plt.title(\"Random Ensemble Clip ID Classification Percentage with MLPClassifier\", fontsize=16)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Truth')\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.savefig('./figure4_plots/random_ensemble_clip_id_percentage_decoder_MLPClassifier.png', dpi = 1200)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Reproduce with Balanced Clip IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_ids.shape, assembly_coactivity_time_traces.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique clip_ids and their counts\n",
    "unique_clip_ids, counts = np.unique(clip_ids, return_counts=True)\n",
    "\n",
    "# Determine the minimum count of any clip_id\n",
    "min_count = np.min(counts)\n",
    "\n",
    "# Create new lists for balanced clip_ids and corresponding assembly_coactivations\n",
    "balanced_clip_ids = []\n",
    "balanced_assembly_coactivations = []\n",
    "\n",
    "# Sample min_count indices for each clip_id\n",
    "for clip_id in unique_clip_ids:\n",
    "    # Get indices of the current clip_id\n",
    "    indices = np.where(clip_ids == clip_id)[0]\n",
    "    # Randomly sample min_count indices\n",
    "    np.random.seed(747)\n",
    "    sampled_indices = np.random.choice(indices, min_count, replace=False)\n",
    "    # Append the sampled indices' values to the new lists\n",
    "    balanced_clip_ids.extend(clip_ids[sampled_indices])\n",
    "    balanced_assembly_coactivations.extend(assembly_coactivity_time_traces[sampled_indices])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "balanced_clip_ids = np.array(balanced_clip_ids)\n",
    "balanced_assembly_coactivations = np.array(balanced_assembly_coactivations)\n",
    "\n",
    "# Shuffle to ensure random distribution\n",
    "shuffled_indices = np.random.default_rng(seed=747).permutation(len(balanced_clip_ids))\n",
    "balanced_clip_ids = balanced_clip_ids[shuffled_indices]\n",
    "balanced_assembly_coactivations = balanced_assembly_coactivations[shuffled_indices]\n",
    "\n",
    "# Output the balanced arrays\n",
    "print(\"Balanced clip_ids:\", balanced_clip_ids)\n",
    "print(\"Balanced assembly_coactivations:\", balanced_assembly_coactivations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_clip_ids.shape, balanced_assembly_coactivations.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(balanced_clip_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stefan Breadcrumb: Run from here\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_assembly_coactivations, balanced_clip_ids.ravel(), test_size=0.2, random_state=747)\n",
    "\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (50, 50, 100), (100, 100)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive', 'invscaling'],\n",
    "    'batch_size': [64, 128, 256, 512, 1024],\n",
    "    'max_iter': [500]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "clf = MLPClassifier(random_state=747, early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# grid_search = RandomizedSearchCV(clf, param_distributions=param_grid, n_iter=100, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "# predict the clip_ids\n",
    "y_pred = best_clf.predict(X_test_scaled)\n",
    "\n",
    "# print out the accuracy\n",
    "print(\"Accuracy Score of\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# plot the confusion matrix\n",
    "assembly_cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(assembly_cm, annot=True, vmax = 100, cmap=greymap, annot_kws={\"size\": 55 / np.sqrt(len(assembly_cm)), \"color\": 'white'})\n",
    "plt.title(\"Assembly Clip ID Classification Count\", fontsize=22)\n",
    "plt.xlabel('Predicted', fontsize=20)\n",
    "plt.ylabel('Truth', fontsize=20)\n",
    "plt.yticks(rotation=0, fontsize=18) \n",
    "plt.xticks(fontsize=18)\n",
    "plt.savefig('./figure4_plots/assembly_balanced_clip_id_decoder_MLPClassifier.png', dpi = 1200)\n",
    "plt.savefig('./figure4_plots/assembly_balanced_clip_id_decoder_MLPClassifier.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n Best estimator:')\n",
    "# print(grid_search.best_estimator_)\n",
    "# print('\\n Best hyperparameters:')\n",
    "# print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Develop normalized Heatmap\n",
    "assembly_cm_norm = np.round((assembly_cm.astype('float') / assembly_cm.sum(axis=1)[:, np.newaxis])*100)\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(assembly_cm_norm, annot=True, vmax = 100, cmap=greymap, annot_kws={\"size\": 55 / np.sqrt(len(assembly_cm)), \"color\": 'white'})\n",
    "ax.figure.axes[-1].tick_params(labelsize=18)\n",
    "plt.title(\"Assembly Clip ID Classification Percent\", fontsize=24)\n",
    "plt.xlabel('Predicted', fontsize=20)\n",
    "plt.ylabel('Truth', fontsize=20)\n",
    "plt.yticks(rotation=0, fontsize=18)\n",
    "plt.xticks(fontsize=18) \n",
    "plt.savefig('./figure4_plots/assembly_balanced_clip_id_percentage_decoder_MLPClassifier.png', dpi = 1200)\n",
    "plt.savefig('./figure4_plots/assembly_balanced_clip_id_percentage_decoder_MLPClassifier.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clip_ids.shape, random_ensemble_coactivity_time_traces.shape)\n",
    "\n",
    "# Get unique clip_ids and their counts\n",
    "unique_clip_ids, counts = np.unique(clip_ids, return_counts=True)\n",
    "\n",
    "# Determine the minimum count of any clip_id\n",
    "min_count = np.min(counts)\n",
    "\n",
    "# Create new lists for balanced clip_ids and corresponding assembly_coactivations\n",
    "balanced_clip_ids = []\n",
    "balanced_random_ensemble_coactivations = []\n",
    "\n",
    "# Sample min_count indices for each clip_id\n",
    "for clip_id in unique_clip_ids:\n",
    "    # Get indices of the current clip_id\n",
    "    indices = np.where(clip_ids == clip_id)[0]\n",
    "    # Randomly sample min_count indices\n",
    "    np.random.seed(747)\n",
    "    sampled_indices = np.random.choice(indices, min_count, replace=False)\n",
    "    # Append the sampled indices' values to the new lists\n",
    "    balanced_clip_ids.extend(clip_ids[sampled_indices])\n",
    "    balanced_random_ensemble_coactivations.extend(random_ensemble_coactivity_time_traces[sampled_indices])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "balanced_clip_ids = np.array(balanced_clip_ids)\n",
    "balanced_random_ensemble_coactivations = np.array(balanced_random_ensemble_coactivations)\n",
    "\n",
    "# Shuffle to ensure random distribution\n",
    "shuffled_indices = np.random.default_rng(seed=747).permutation(len(balanced_clip_ids))\n",
    "balanced_clip_ids = balanced_clip_ids[shuffled_indices]\n",
    "balanced_random_ensemble_coactivations = balanced_random_ensemble_coactivations[shuffled_indices]\n",
    "\n",
    "# Output the balanced arrays\n",
    "print(\"Balanced clip_ids:\", balanced_clip_ids)\n",
    "print(\"Balanced random_ensemble_coactivations:\", balanced_random_ensemble_coactivations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_clip_ids.shape, balanced_random_ensemble_coactivations.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_random_ensemble_coactivations, balanced_clip_ids.ravel(), test_size=0.2, random_state=747)\n",
    "\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# define the model\n",
    "clf = MLPClassifier(random_state=747)\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (50, 50, 100), (100, 100)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive', 'invscaling'],\n",
    "    'batch_size': [64, 128, 256, 512, 1024],\n",
    "    'max_iter': [500]\n",
    "}\n",
    "\n",
    "clf = MLPClassifier(random_state=747, early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# grid_search = RandomizedSearchCV(clf, param_distributions=param_grid, n_iter=300, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "# predict the clip_ids\n",
    "y_pred = best_clf.predict(X_test_scaled)\n",
    "\n",
    "# print out the accuracy\n",
    "print(\"Accuracy Score of\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# plot the confusion matrix\n",
    "rand_ensemble_cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(rand_ensemble_cm, annot=True, vmax = 100, cmap=greymap, annot_kws={\"size\": 55 / np.sqrt(len(rand_ensemble_cm)), \"color\": 'white'})\n",
    "plt.title(\"Random Ensemble Clip ID Classification Count\", fontsize=20)\n",
    "ax.figure.axes[-1].tick_params(labelsize=18)\n",
    "plt.xlabel('Predicted', fontsize=20)\n",
    "plt.ylabel('Truth', fontsize=20)\n",
    "plt.yticks(rotation=0, fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.savefig('./figure4_plots/random_ensemble_balanced_clip_id_decoder_MLPClassifier.png', dpi = 1200)\n",
    "plt.savefig('./figure4_plots/random_ensemble_balanced_clip_id_decoder_MLPClassifier.pdf')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the confusion matrix\n",
    "rand_ensemble_cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(rand_ensemble_cm, annot=True, vmax = 100, cmap=greymap, annot_kws={\"size\": 50 / np.sqrt(len(rand_ensemble_cm))})\n",
    "plt.title(\"Random Ensemble Balanced Clip ID Classification\", fontsize=20)\n",
    "ax.figure.axes[-1].tick_params(labelsize=18)\n",
    "plt.xlabel('Predicted', fontsize=20)\n",
    "plt.ylabel('Truth', fontsize=20)\n",
    "plt.yticks(rotation=0, fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.savefig('./figure4_plots/asdf_random_ensemble_balanced_clip_id_decoder_MLPClassifier.png', dpi = 1200)\n",
    "plt.savefig('./figure4_plots/asdf_random_ensemble_balanced_clip_id_decoder_MLPClassifier.pdf')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Best estimator:')\n",
    "print(grid_search.best_estimator_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Develop normalized Heatmap\n",
    "rand_ensemble_cm_norm = np.round((rand_ensemble_cm.astype('float') / rand_ensemble_cm.sum(axis=1)[:, np.newaxis])*100)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(rand_ensemble_cm_norm, annot=True, vmax = 100, cmap=greymap, annot_kws={\"size\": 55 / np.sqrt(len(rand_ensemble_cm)), \"color\": 'white'})\n",
    "plt.title(\"Random Ensemble Clip ID Classification Percent\", fontsize=24)\n",
    "ax.figure.axes[-1].tick_params(labelsize=18)\n",
    "plt.xlabel('Predicted', fontsize=20)\n",
    "plt.ylabel('Truth', fontsize=20)\n",
    "plt.yticks(rotation=0, fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.savefig('./figure4_plots/random_ensemble_balanced_clip_id_percentage_decoder_MLPClassifier.png', dpi = 1200)\n",
    "plt.savefig('./figure4_plots/random_ensemble_balanced_clip_id_percentage_decoder_MLPClassifier.pdf')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Produce Statistical Test Between Two Heatmaps\n",
    "assembly_cm_norm_mean = np.mean(assembly_cm_norm.flatten())\n",
    "assembly_cm_norm_std = np.std(assembly_cm_norm.flatten())\n",
    "rand_ensemble_cm_mean = np.mean(rand_ensemble_cm_norm.flatten())\n",
    "rand_ensemble_cm_std = np.std(rand_ensemble_cm_norm.flatten())\n",
    "# print(f\"Assembly Heatmap: Mean = {assembly_cm_norm_mean}, Std = {assembly_cm_norm_std}\")\n",
    "# print(f\"Random Ensemble Heatmap: Mean = {rand_ensemble_cm_mean}, Std = {rand_ensemble_cm_std}\")\n",
    "\n",
    "# Mann-Whitney U test\n",
    "mw_u_stat, mw_p_value = stats.mannwhitneyu(assembly_cm_norm.flatten(), rand_ensemble_cm_norm.flatten())\n",
    "print(f\"Mann-Whitney U test: u_stat = {mw_u_stat}, p_value = {mw_p_value}\")\n",
    "\n",
    "# Rank-biserial correlation\n",
    "rank_biserial = 1 - 2 * mw_u_stat / (len(assembly_cm_norm.flatten()) * len(rand_ensemble_cm_norm.flatten()))\n",
    "print(f\"Mann-Whitney U Test Rank-biserial correlation = {rank_biserial}\")\n",
    "\n",
    "# Wilcoxon rank-sum\n",
    "wrs_stat, wrs_p_value = stats.ranksums(assembly_cm_norm.flatten(), rand_ensemble_cm_norm.flatten())\n",
    "print(f\"Wilcoxon Rank-Sum U test: u_stat = {wrs_stat}, p_value = {wrs_p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Statistical test of just the Diagonal, Implying Accuracy\n",
    "assembly_accuracy = np.diag(assembly_cm_norm)\n",
    "rand_ensemble_accuracy = np.diag(rand_ensemble_cm_norm)\n",
    "\n",
    "assembly_accuracy_mean = np.mean(assembly_accuracy)\n",
    "assembly_accuracy_std = np.std(assembly_accuracy)\n",
    "rand_ensemble_accuracy_mean = np.mean(rand_ensemble_accuracy)\n",
    "rand_ensemble_accuracy_std = np.std(rand_ensemble_accuracy)\n",
    "# print(f\"Assembly Heatmap: Mean = {assembly_accuracy_mean}, Std = {assembly_accuracy_std}\")\n",
    "# print(f\"Random Ensemble Heatmap: Mean = {rand_ensemble_accuracy_mean}, Std = {rand_ensemble_cm_std}\")\n",
    "\n",
    "# Mann-Whitney U test\n",
    "mw_u_stat, mw_p_value = stats.mannwhitneyu(assembly_accuracy, rand_ensemble_accuracy, alternative = 'greater')\n",
    "print(f\"Mann-Whitney U test: u_stat = {mw_u_stat}, p_value = {mw_p_value}\")\n",
    "\n",
    "# Rank-biserial correlation\n",
    "rank_biserial = 1 - 2 * mw_u_stat / (len(assembly_accuracy) * len(rand_ensemble_accuracy))\n",
    "print(f\"Mann-Whitney U Test Rank-biserial correlation = {rank_biserial}\")\n",
    "\n",
    "# Wilcoxon rank-sum\n",
    "wrs_stat, wrs_p_value = stats.ranksums(assembly_accuracy, rand_ensemble_accuracy, alternative = 'greater')\n",
    "print(f\"Wilcoxon Rank-Sum U test: u_stat = {wrs_stat}, p_value = {wrs_p_value}\")\n",
    "\n",
    "# # %% [markdown]\n",
    "# ### Reproduce Oracle Score Analysis with Random Ensembles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pika_rois_all_dict = {}\n",
    "pika_rois_in_assembly_dict = {}\n",
    "pika_rois_no_assembly_dict = {}\n",
    "for plane_n, roi_ns in rois_dict.items():\n",
    "    pika_rois = []\n",
    "    pika_rois_in_assembly = []\n",
    "    pika_rois_no_assembly = []\n",
    "    # print(plane_n)\n",
    "    for roi_n in roi_ns:\n",
    "        score = daf.get_pika_classifier_score(nwb_f=nwb_f, plane_n=plane_n, roi_n=roi_n)\n",
    "        if score > 0.5:  # Using the threshold from team PIKA, per https://github.com/zhuangjun1981/v1dd_physiology/blob/main/v1dd_physiology/example_notebooks/2022-06-27-data-fetching-basic.ipynb\n",
    "            pika_rois.append(roi_n)\n",
    "            if int(roi_n[4:]) in assembly_neurons:\n",
    "                pika_rois_in_assembly.append(roi_n)\n",
    "            else:\n",
    "                pika_rois_no_assembly.append(roi_n)\n",
    "    pika_rois_all_dict[plane_n] = pika_rois\n",
    "    pika_rois_in_assembly_dict[plane_n] = pika_rois_in_assembly\n",
    "    pika_rois_no_assembly_dict[plane_n] = pika_rois_no_assembly\n",
    "\n",
    "total_rois = np.sum([len(val) for val in pika_rois_all_dict.values()])\n",
    "neuron_all_movie_oracle_r_values = np.zeros((total_rois, 9))\n",
    "\n",
    "total_rois = np.sum([len(val) for val in pika_rois_in_assembly_dict.values()])\n",
    "neuron_in_assembly_movie_oracle_r_values = np.zeros((total_rois, 9))\n",
    "\n",
    "total_rois = np.sum([len(val) for val in pika_rois_no_assembly_dict.values()])\n",
    "neuron_no_assembly_movie_oracle_r_values = np.zeros((total_rois, 9))\n",
    "\n",
    "count_n_all = -1\n",
    "count_n_in_assembly = -1\n",
    "count_n_no_assembly = -1\n",
    "for curr_dict, oracle_array, c in zip([pika_rois_all_dict, pika_rois_in_assembly_dict, pika_rois_no_assembly_dict], \n",
    "        [neuron_all_movie_oracle_r_values, neuron_in_assembly_movie_oracle_r_values, neuron_no_assembly_movie_oracle_r_values],\n",
    "        [1,2,3]):\n",
    "    for plane_n, pika_roi_ns in curr_dict.items():\n",
    "        for roi_n in pika_roi_ns:\n",
    "            if c == 1:\n",
    "                count_n_all += 1\n",
    "                current_count = count_n_all\n",
    "            elif c == 2:\n",
    "                count_n_in_assembly += 1\n",
    "                current_count = count_n_in_assembly\n",
    "            elif c == 3:\n",
    "                count_n_no_assembly += 1\n",
    "                current_count = count_n_no_assembly\n",
    "            ### Get Time Trace\n",
    "            dff, dff_ts = daf.get_single_trace(nwb_f=nwb_f, plane_n=plane_n, roi_n=roi_n, trace_type='dff')\n",
    "            f_binary_raster = activity_raster[:, int(roi_n[4:])-1]\n",
    "            \n",
    "            # Get Repeated Natural Movies\n",
    "            trial_fluorescence = []\n",
    "            presentation = nwb_f['stimulus']['presentation']\n",
    "            nm_timestamps = np.array(\n",
    "                presentation['natural_movie'].get('timestamps'))\n",
    "            nm_data = np.array(presentation['natural_movie'].get('data'))\n",
    "            new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "            clip_duration = 300  # new_clips[1]-1\n",
    "            for repeat_id in range(new_clips.shape[0]):\n",
    "                frames_to_capture = np.where(dff_ts >= nm_timestamps[new_clips[repeat_id]])[\n",
    "                    0][0:clip_duration]\n",
    "                trial_fluorescence.append(f_binary_raster[frames_to_capture])\n",
    "            trial_fluorescence_np = np.array(trial_fluorescence)\n",
    "            for trial_idx in range(trial_fluorescence_np.shape[0]):\n",
    "                removed_trial = trial_fluorescence_np[trial_idx]\n",
    "                remaining_trials = np.delete(\n",
    "                    trial_fluorescence_np, trial_idx, 0)\n",
    "                r, p = scipy.stats.pearsonr(\n",
    "                    removed_trial, np.mean(remaining_trials, 0))\n",
    "                oracle_array[current_count, trial_idx] = r\n",
    "\n",
    "assembly_movie_oracle_r_values = np.zeros((passing_roi_count, 9))\n",
    "f = assembly_coactivity_trace\n",
    "# print(coactivity_trace)\n",
    "f_ts = np.array(nwb_f['processing']['rois_and_traces_plane0']\n",
    "                ['Fluorescence']['f_raw_subtracted'].get('timestamps'))\n",
    "for roi_n in range(passing_roi_count):\n",
    "\n",
    "    # Get Repeated Natural Movies\n",
    "    trial_fluorescence = []\n",
    "    presentation = nwb_f['stimulus']['presentation']\n",
    "    nm_timestamps = np.array(\n",
    "        presentation['natural_movie'].get('timestamps'))\n",
    "    nm_data = np.array(presentation['natural_movie'].get('data'))\n",
    "    new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "    clip_duration = 300  # new_clips[1]-1\n",
    "    for repeat_id in range(new_clips.shape[0]):\n",
    "        frames_to_capture = np.where(f_ts >= nm_timestamps[new_clips[repeat_id]])[\n",
    "            0][0:clip_duration]\n",
    "        trial_fluorescence.append(f[frames_to_capture, roi_n])\n",
    "    trial_fluorescence_np = np.array(trial_fluorescence)\n",
    "    for trial_idx in range(trial_fluorescence_np.shape[0]):\n",
    "        removed_trial = trial_fluorescence_np[trial_idx]\n",
    "        remaining_trials = np.delete(\n",
    "            trial_fluorescence_np, trial_idx, 0)\n",
    "        r, p = scipy.stats.pearsonr(\n",
    "            removed_trial, np.mean(remaining_trials, 0))\n",
    "        assembly_movie_oracle_r_values[roi_n, trial_idx] = r\n",
    "\n",
    "# Plot Movie Oracles\n",
    "mean_over_holdouts = np.mean(assembly_movie_oracle_r_values, 1)\n",
    "fig = plt.figure()\n",
    "plt.title('Assembly natural movie oracle score')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.hist(mean_over_holdouts[:], bins=50)\n",
    "#plt.savefig('./figure4_plots/oracle_dists2/assemblies_esteps_150000__affinity_04_session'+str(13)+'_movies.png')\n",
    "plt.show()\n",
    "\n",
    "random_ensemble_movie_oracle_r_values = np.zeros((passing_roi_count, 9))\n",
    "f = random_ensembles_coactivity_trace\n",
    "# print(coactivity_trace)\n",
    "f_ts = np.array(nwb_f['processing']['rois_and_traces_plane0']\n",
    "                ['Fluorescence']['f_raw_subtracted'].get('timestamps'))\n",
    "for roi_n in range(passing_roi_count):\n",
    "\n",
    "    # Get Repeated Natural Movies\n",
    "    trial_fluorescence = []\n",
    "    presentation = nwb_f['stimulus']['presentation']\n",
    "    nm_timestamps = np.array(\n",
    "        presentation['natural_movie'].get('timestamps'))\n",
    "    nm_data = np.array(presentation['natural_movie'].get('data'))\n",
    "    new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "    clip_duration = 300  # new_clips[1]-1\n",
    "    for repeat_id in range(new_clips.shape[0]):\n",
    "        frames_to_capture = np.where(f_ts >= nm_timestamps[new_clips[repeat_id]])[\n",
    "            0][0:clip_duration]\n",
    "        trial_fluorescence.append(f[frames_to_capture, roi_n])\n",
    "    trial_fluorescence_np = np.array(trial_fluorescence)\n",
    "    for trial_idx in range(trial_fluorescence_np.shape[0]):\n",
    "        removed_trial = trial_fluorescence_np[trial_idx]\n",
    "        remaining_trials = np.delete(\n",
    "            trial_fluorescence_np, trial_idx, 0)\n",
    "        r, p = scipy.stats.pearsonr(\n",
    "            removed_trial, np.mean(remaining_trials, 0))\n",
    "        random_ensemble_movie_oracle_r_values[roi_n, trial_idx] = r\n",
    "\n",
    "import ptitprince as pt\n",
    "# Plot Movie Oracles\n",
    "mean_over_holdouts = np.mean(random_ensemble_movie_oracle_r_values, 1)\n",
    "fig = plt.figure()\n",
    "plt.title('Random Ensemble natural movie oracle score')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.hist(mean_over_holdouts[:], bins=50)\n",
    "#plt.savefig('./figure4_plots/oracle_dists2/assemblies_esteps_150000__affinity_04_session'+str(13)+'_movies.png')\n",
    "plt.show()\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "all_arr = [np.array(assembly_movie_oracle_r_values).flatten(),\n",
    "            np.array(random_ensemble_movie_oracle_r_values).flatten(),\n",
    "            np.array(neuron_all_movie_oracle_r_values).flatten(),\n",
    "            np.array(neuron_in_assembly_movie_oracle_r_values).flatten(),\n",
    "            np.array(neuron_no_assembly_movie_oracle_r_values).flatten()]\n",
    "\n",
    "# Prepare data for raincloud plot\n",
    "data = pd.DataFrame({\n",
    "    \"Values\": np.concatenate(all_arr),\n",
    "    \"Group\": [f\"Assemblies\"] * len(assembly_movie_oracle_r_values.flatten()) + \\\n",
    "                [f\"Random\\nEnsembles\"] * len(random_ensemble_movie_oracle_r_values.flatten()) + \\\n",
    "                [f\"All\\nCells\"] * len(neuron_all_movie_oracle_r_values.flatten()) + \\\n",
    "                [f\"Assembly\\nCells\"] * len(neuron_in_assembly_movie_oracle_r_values.flatten()) + \\\n",
    "                [f\"Non-Assembly\\nCells\"] * len(neuron_no_assembly_movie_oracle_r_values.flatten())\n",
    "})\n",
    "\n",
    "# Create the raincloud plot\n",
    "ax = pt.RainCloud(\n",
    "    y=\"Values\",\n",
    "    x=\"Group\",\n",
    "    data=data,\n",
    "    palette=[(.4, .6, .8, .5), 'grey'],\n",
    "    width_viol=0.6,  # Adjust violin width\n",
    "    alpha=0.8,  # Transparency of the cloud\n",
    "    move=0.1,  # Adjust position of violins\n",
    "    point_size = 4,\n",
    "    orient=\"v\"  # Horizontal orientation\n",
    ")\n",
    "\n",
    "plt.ylim(-0.2, 1.2)\n",
    "\n",
    "# Calculate p-values for Wilcoxon rank-sum tests\n",
    "groups = [\"Random\\nEnsembles\", \"All\\nCells\", \"Assembly\\nCells\", \"Non-Assembly\\nCells\"]\n",
    "p_values = []\n",
    "for group in groups:\n",
    "    group_values = data[data[\"Group\"] == group][\"Values\"]\n",
    "    p_value = stats.ranksums(data[data[\"Group\"] == \"Assemblies\"][\"Values\"], group_values, alternative='two-sided').pvalue\n",
    "    print(f\"p-value for Assemblies greater than {group}: {p_value:.3g}\")\n",
    "    p_values.append(p_value)\n",
    "\n",
    "# Add significance markers as number of asterisks\n",
    "for i, p_value in enumerate(p_values):\n",
    "    if p_value < 0.001:\n",
    "        significance = 'p < 0.001\\n***'\n",
    "    elif p_value < 0.01:\n",
    "        significance = 'p < 0.01\\n**'\n",
    "    elif p_value < 0.05:\n",
    "        significance = 'p < 0.05\\n*'\n",
    "    else:\n",
    "        significance = 'ns'\n",
    "    ax.annotate(significance, xy=(i + 1,  1.1), #data[data[\"Group\"] == groups[i]][\"Values\"].max()\n",
    "                horizontalalignment='center', size=16, color='black', weight='bold')\n",
    "\n",
    "# Add a multiline title to include the p-value, add y_label\n",
    "title = f'Natural Movie Oracle Scores'\n",
    "plt.title(title, size=24)\n",
    "# plt.xlabel('Cell Grouping', size=20)\n",
    "plt.xticks(fontsize=20)  # Adjust size of xticks\n",
    "plt.yticks(fontsize=20)  # Adjust size of yticks\n",
    "plt.ylabel(\"Oracle Score\", size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./figure4_plots/oracle_scores_dff_all_sets_raincloud.png', dpi = 1200)\n",
    "plt.savefig('./figure4_plots/oracle_scores_dff_all_sets_raincloud.pdf')\n",
    "plt.savefig('./figure4_plots/oracle_scores_dff_all_sets_raincloud.svg')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "# ax = sns.boxplot(data=all_arr,\n",
    "#                 notch=True, showcaps=True,\n",
    "#                 flierprops={\"marker\": \"x\"},\n",
    "#                 boxprops={\"facecolor\": (.4, .6, .8, .5)},\n",
    "#                 medianprops={\"color\": \"coral\"},\n",
    "#             )\n",
    "# ax.set_xticklabels([\"Assemblies\", \"Random Ensembles\", \"All Cells\", \"Assembly Cells\", \"Non-Assembly Cells\"], size = 14)\n",
    "# ax.set_title('Natural Movie Oracle Score', size = 20)\n",
    "# ax.set_ylabel('Oracle Score', size = 15)\n",
    "\n",
    "# medians = np.array(\n",
    "#     [np.median(np.array(assembly_movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(random_ensemble_movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(neuron_all_movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(neuron_in_assembly_movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(neuron_no_assembly_movie_oracle_r_values).flatten())]\n",
    "# )\n",
    "\n",
    "# vertical_offset = (medians * 0.0)+1.05  # offset from median for display\n",
    "# p_values = [np.nan,\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(assembly_movie_oracle_r_values).flatten(), np.array(random_ensemble_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5),\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(assembly_movie_oracle_r_values).flatten(), np.array(neuron_all_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5),\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(assembly_movie_oracle_r_values).flatten(), np.array(neuron_in_assembly_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5),\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(assembly_movie_oracle_r_values).flatten(), np.array(neuron_no_assembly_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5)]\n",
    "\n",
    "# for xtick in ax.get_xticks():\n",
    "#     if xtick != 0:\n",
    "#         ax.text(xtick, medians[xtick] + vertical_offset[xtick], p_values[xtick], \n",
    "#                 horizontalalignment='center', size='small', color='black', weight='semibold')\n",
    "\n",
    "# plt.savefig('./figure4_plots/oracle_scores_histogram_dff_all_sets.png', dpi = 1200)\n",
    "# # plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# # %%\n",
    "# # Plot Movie Oracles\n",
    "# plt.figure(figsize=(14, 10))\n",
    "\n",
    "# all_arr = [np.array(assembly_movie_oracle_r_values).flatten(),\n",
    "#             np.array(random_ensemble_movie_oracle_r_values).flatten(),\n",
    "#             np.array(neuron_all_movie_oracle_r_values).flatten(),\n",
    "#             np.array(neuron_in_assembly_movie_oracle_r_values).flatten(),\n",
    "#             np.array(neuron_no_assembly_movie_oracle_r_values).flatten()]\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "# ax = sns.boxplot(data=all_arr,\n",
    "#                 notch=True, showcaps=True,\n",
    "#                 flierprops={\"marker\": \"x\"},\n",
    "#                 boxprops={\"facecolor\": (.4, .6, .8, .5)},\n",
    "#                 medianprops={\"color\": \"coral\"},\n",
    "#             )\n",
    "# ax.set_xticklabels([\"Assemblies\", \"Random Ensembles\", \"All Cells\", \"Assembly Cells\", \"Non-Assembly Cells\"], size = 15)\n",
    "# ax.set_title('Natural Movie Oracle Score', size = 20)\n",
    "# ax.set_ylabel('Oracle Score', size = 16)\n",
    "# plt.yticks(fontsize=15)\n",
    "\n",
    "# medians = np.array(\n",
    "#     [np.median(np.array(assembly_movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(random_ensemble_movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(neuron_all_movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(neuron_in_assembly_movie_oracle_r_values).flatten()),\n",
    "#      np.median(np.array(neuron_no_assembly_movie_oracle_r_values).flatten())]\n",
    "# )\n",
    "\n",
    "# vertical_offset = medians * 0.15 # offset from median for display\n",
    "# p_values = [np.nan,\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(assembly_movie_oracle_r_values).flatten(), np.array(random_ensemble_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5),\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(assembly_movie_oracle_r_values).flatten(), np.array(neuron_all_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5),\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(assembly_movie_oracle_r_values).flatten(), np.array(neuron_in_assembly_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5),\n",
    "#             'P-Val: {:.3g}'.format(stats.ranksums(np.array(assembly_movie_oracle_r_values).flatten(), np.array(neuron_no_assembly_movie_oracle_r_values).flatten(), alternative = 'greater').pvalue, 5)]\n",
    "\n",
    "# for xtick in ax.get_xticks():\n",
    "#     if xtick != 0:\n",
    "#         ax.text(xtick, medians[xtick] + vertical_offset[xtick], p_values[xtick], \n",
    "#                 horizontalalignment='center', size= 12, color='black', weight='semibold')\n",
    "\n",
    "# plt.savefig('./figure4_plots/oracle_scores_histogram_dff_all_sets.png', dpi = 1200)\n",
    "# plt.show()\n",
    "\n",
    "# wrs_p_value = stats.ranksums(np.array(neuron_all_movie_oracle_r_values).flatten(), np.array(neuron_in_assembly_movie_oracle_r_values).flatten())\n",
    "# print(f\"Rank-Sum (All Cells vs Assembly Cells): p_value = {wrs_p_value}\")\n",
    "# wrs_p_value = stats.ranksums(np.array(neuron_all_movie_oracle_r_values).flatten(), np.array(neuron_no_assembly_movie_oracle_r_values).flatten())\n",
    "# print(f\"Rank-Sum (All Cells vs Non-Assembly Cells): p_value = {wrs_p_value}\")\n",
    "# wrs_p_value = stats.ranksums(np.array(neuron_in_assembly_movie_oracle_r_values).flatten(), np.array(neuron_no_assembly_movie_oracle_r_values).flatten())\n",
    "# print(f\"Rank-Sum (Assembly Cells vs Non-Assembly Cells): p_value = {wrs_p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Reproduce Sparsity Plot with Random Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ginis2(coactivity_trace, null_trace=None, all_trace=None, a_trace=None, no_a_trace=None):\n",
    "    num_assemblies = coactivity_trace.shape[1]\n",
    "    gini_values = [gini(coactivity_trace[:,i]) for i in range(num_assemblies)]\n",
    "    labels = [f'A {i+1}' for i in range(num_assemblies)]\n",
    "    \n",
    "    if no_a_trace is not None:\n",
    "        gini_values_no_a = [gini(no_a_trace[:,i]) for i in range(no_a_trace.shape[1])]\n",
    "        mean_gini_no_a = np.nanmean(gini_values_no_a)\n",
    "        # labels.insert(0, 'Nonassembly\\nCells')\n",
    "    \n",
    "    if a_trace is not None:\n",
    "        gini_values_a = [gini(a_trace[:,i]) for i in range(a_trace.shape[1])]\n",
    "        mean_gini_a = np.nanmean(gini_values_a)\n",
    "        # labels.insert(0, 'Assembly\\nCells')\n",
    "    \n",
    "    # if all_trace is not None:\n",
    "    #     gini_values_all = [gini(all_trace[:,i]) for i in range(all_trace.shape[1])]\n",
    "    #     mean_gini_all = np.nanmean(gini_values_all)\n",
    "    #     labels.insert(0, 'All\\nCells')\n",
    "    \n",
    "    if null_trace is not None:\n",
    "        gini_values_null = [gini(null_trace[:,i]) for i in range(num_assemblies)]\n",
    "        mean_gini_null = np.nanmean(gini_values_null)\n",
    "        # labels.insert(0, 'Null')\n",
    "        \n",
    "    print(gini_values)\n",
    "    print(gini_values_null)\n",
    "    print(labels)\n",
    "\n",
    "    # Create a DataFrame for the grouped bar plot\n",
    "    gini_df = pd.DataFrame({\n",
    "        'Assembly Gini Values': gini_values,\n",
    "        'Random Ensemble Gini Values': gini_values_null,\n",
    "        'Labels': labels\n",
    "    })\n",
    "\n",
    "    # print(gini_df)\n",
    "\n",
    "    # Melt the DataFrame to long format\n",
    "    gini_df_melted = gini_df.melt(id_vars='Labels', value_vars=['Assembly Gini Values', 'Random Ensemble Gini Values'], \n",
    "                                  var_name='Type', value_name='Gini Coefficient')\n",
    "\n",
    "    p_value = stats.ranksums(np.array(gini_values_null).flatten(), np.array(gini_values).flatten(), alternative='two-sided').pvalue\n",
    "\n",
    "    # Create a grouped bar plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.title(f'Sparsity of Activity\\np={p_value:0.4g}', size=24, size=24)\n",
    "    ax = sns.barplot(x='Labels', y='Gini Coefficient', hue='Type', data=gini_df_melted, palette='Paired')\n",
    "    # ax.set_title()\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xticks(fontsize=20, rotation=45)    \n",
    "    ax.set_ylabel('Gini Coefficient', size=24)\n",
    "    ax.set_xlabel('Assembly or Matched Random Ensemble', size=24)\n",
    "    plt.ylim((0, 1.0))\n",
    "\n",
    "    # Add horizontal lines for mean_gini_a and mean_gini_no_a\n",
    "    if a_trace is not None:\n",
    "        ax.axhline(mean_gini_a, color='cornflowerblue', linestyle='--', linewidth=2)\n",
    "        ax.text(len(labels)-5.75, mean_gini_a-0.04, f'Assembly Cells Mean: {mean_gini_a:.3f}', \n",
    "                color='cornflowerblue', ha='left', va='bottom', fontsize=16, weight='bold')\n",
    "\n",
    "    if no_a_trace is not None:\n",
    "        ax.axhline(mean_gini_no_a, color='slategray', linestyle='--', linewidth=2)\n",
    "        ax.text(len(labels)-5.75, mean_gini_no_a, f'Nonassembly Cells Mean: {mean_gini_no_a:.3f}', \n",
    "                color='slategray', ha='left', va='bottom', fontsize=16, weight='bold')\n",
    "\n",
    "    # Add annotations for clarity (optional)\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha='center', va='center', xytext=(0, 30.5), textcoords='offset points', size=18, weight='bold', rotation=90)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(0, 0.95), loc='upper left', frameon=True, fontsize=16)\n",
    "    # plt.legend(, loc='upper left', borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./figure4_plots/sparsity_with_Gini_coefficient_by_assembly_and_random_ensembles.png')\n",
    "    plt.savefig('./figure4_plots/sparsity_with_Gini_coefficient_by_assembly_and_random_ensembles.pdf')\n",
    "    plt.savefig('./figure4_plots/sparsity_with_Gini_coefficient_by_assembly_and_random_ensembles.svg')\n",
    "\n",
    "    # gini_values_all = gini_values_a + gini_values_no_a\n",
    "    \n",
    "    # # Create a figure\n",
    "    # plt.figure(figsize=(24,8))\n",
    "    # sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # all_arr = [np.array(gini_values).flatten(),\n",
    "    #         np.array(gini_values_null).flatten(),\n",
    "    #         np.array(gini_values_all).flatten(),\n",
    "    #         np.array(gini_values_a).flatten(),\n",
    "    #         np.array(gini_values_no_a).flatten()]\n",
    "\n",
    "    # print(np.min(gini_values))\n",
    "    # print(np.max(gini_values))\n",
    "    # print(np.min(gini_values_all))\n",
    "    # print(np.max(gini_values_all))\n",
    "    # print(np.min(gini_values_a))\n",
    "    # print(np.max(gini_values_a))\n",
    "    # print(np.min(gini_values_no_a))\n",
    "    # print(np.max(gini_values_no_a))\n",
    "\n",
    "    # # Prepare data for raincloud plot\n",
    "    # data = pd.DataFrame({\n",
    "    #     \"Gini Coefficients\": np.concatenate(all_arr),\n",
    "    #     \"Cell Grouping\": [f\"Assemblies\\nn={len(gini_values)}\"] * len(gini_values) + \\\n",
    "    #                 [f\"Random\\nEnsembles\\nn={len(gini_values_null)}\"] * len(gini_values_null) + \\\n",
    "    #                 [f\"All\\nCells\\nn={len(gini_values_all)}\"] * len(gini_values_all) + \\\n",
    "    #                 [f\"Assembly\\nCells\\nn={len(gini_values_a)}\"] * len(gini_values_a) + \\\n",
    "    #                 [f\"Non-Assembly\\nCells\\nn={len(gini_values_no_a)}\"] * len(gini_values_no_a)\n",
    "    # })\n",
    "\n",
    "    # # Create the raincloud plot\n",
    "    # ax = pt.RainCloud(\n",
    "    #     y=\"Gini Coefficients\",\n",
    "    #     x=\"Cell Grouping\",\n",
    "    #     data=data,\n",
    "    #     palette=[(.4, .6, .8, .5), 'grey'],\n",
    "    #     width_viol=0.6,  # Adjust violin width\n",
    "    #     alpha=0.8,  # Transparency of the cloud\n",
    "    #     move=0.1,  # Adjust position of violins\n",
    "    #     point_size = 6,\n",
    "    #     orient=\"v\",  # Horizontal orientation\n",
    "    #     linewidth=2\n",
    "    # )\n",
    "\n",
    "    \n",
    "    #  # # Calculate p-value\n",
    "    # p_value = stats.ranksums(np.array(gini_values_null).flatten(), np.array(gini_values).flatten(), alternative='two-sided').pvalue\n",
    "\n",
    "    # # # Add significance stars\n",
    "    # if p_value < 0.001:\n",
    "    #     significance = 'P < 0.001\\n***'\n",
    "    # elif p_value < 0.01:\n",
    "    #     significance = 'P < 0.01\\n**'\n",
    "    # elif p_value < 0.05:\n",
    "    #     significance = 'P < 0.05\\n*'\n",
    "    # else:\n",
    "    #     significance = 'ns'\n",
    "\n",
    "    # # Add p-value annotation\n",
    "\n",
    "    # ax.text(0.5, 0.9, f'{significance}', \n",
    "    #             color='black', ha='center', va='bottom', fontsize=20, weight='bold')\n",
    "    \n",
    "    # ax.annotate(f'P-Val: {p_value:.3g}', xy=(1, medians[1] + vertical_offset[1]), xytext=(1, medians[1] + vertical_offset[1] + 0.05),\n",
    "    #             arrowprops=dict(facecolor='black', shrink=0.05), horizontalalignment='center', size=18, color='black', weight='semibold')\n",
    "\n",
    "    \n",
    "\n",
    "    # plt.ylabel('Gini Coefficient', size=20, weight='bold')\n",
    "    # plt.xticks(fontsize=20, weight='bold')\n",
    "    # plt.yticks(fontsize=20, weight='bold')\n",
    "    # # plt.xlabel(\"Groups\", size=20, weight='bold')\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig('./figure4_plots/gini_coefficients_raincloud_plot.png', dpi=1200)\n",
    "    # plt.savefig('./figure4_plots/gini_coefficients_raincloud_plot.pdf', dpi=1200)\n",
    "    # plt.savefig('./figure4_plots/gini_coefficients_raincloud_plot.svg', dpi=1200)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(activity_raster.shape)\n",
    "# print(np.min(no_assembly_neurons))\n",
    "# print(np.min(assembly_neurons))\n",
    "plot_ginis2(coactivity_trace=assembly_coactivity_trace, null_trace=random_ensembles_coactivity_trace, all_trace=activity_raster, a_trace=activity_raster[:,assembly_neurons], no_a_trace=activity_raster[:,no_assembly_neurons])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
