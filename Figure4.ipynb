{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 4 Code to Produce Figures\n",
    "\n",
    "This figure will focus on the presentation of Further Analysis of Extracted Assemblies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster, cophenet\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from difflib import diff_bytes\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "import upsetplot\n",
    "import ptitprince\n",
    "import matplotlib.cm as cm\n",
    "from tqdm import tqdm\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "import v1dd_physiology.data_fetching as daf\n",
    "from statannotations.Annotator import Annotator\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "paired_blue0 = cm.get_cmap('Paired')(0)\n",
    "paired_blue1 = cm.get_cmap('Paired')(1)\n",
    "\n",
    "colors1 = ['grey', (.4, .6, .8, .5)]\n",
    "colors2 = ['grey', (.2, .3, .5, .5), (.4, .6, .8, .5), 'white']\n",
    "colors3 = [(.2, .3, .5, .5), 'white']\n",
    "colors4 = ['grey', (.2, .3, .5, .5), (.4, .6, .8, .5),]\n",
    "colors5 = [(.2, .3, .5, .5), (.4, .6, .8, .5), 'white']\n",
    "colors6 = ['grey', 'steelblue', (.4, .6, .8, .5),]\n",
    "colors7 = ['grey', 'steelblue']\n",
    "colors8 = ['grey', 'cornflowerblue']\n",
    "colors9 = ['grey', paired_blue1]\n",
    "colors10 = ['dimgrey', paired_blue0]\n",
    "greymap = LinearSegmentedColormap.from_list(\n",
    "        \"Custom\", colors10, N=80) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Suppress ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle Scores Assemblies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions relevant for Oracle Score Analysis\n",
    "\n",
    "def distance(x, y):\n",
    "    return math.sqrt((x[0]-y[0])**2 + (x[1]-y[1])**2)\n",
    "\n",
    "# def process(img):\n",
    "#     img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     img_canny = cv2.Canny(img_gray, 0, 50)\n",
    "#     img_dilate = cv2.dilate(img_canny, None, iterations=1)\n",
    "#     img_erode = cv2.erode(img_dilate, None, iterations=1)\n",
    "#     return img_erode\n",
    "\n",
    "def correlate_assembly_traces_vs_random_ensembles(assembly_traces, random_ensemble_traces, scores_in_order, name='compared_time_traces'):\n",
    "    # Set up three subplots\n",
    "    num_assemblies = assembly_traces.shape[1]\n",
    "    # fig, ax = plt.subplots(num_assemblies, 1, figsize=(12, 12))\n",
    "\n",
    "    # plot\n",
    "    # for i in range(num_assemblies):\n",
    "    #     ax[i].plot(assembly_traces[:, i], color='green', label='Assembly')\n",
    "    #     ax[i].plot(random_ensemble_traces[:, i], color='red', label='Random Ensemble')\n",
    "    #     ax[i].set_ylabel(f\"{scores_in_order[i]:.2}\")\n",
    "    #     ax[i].set_xlabel(\"Time Steps\")\n",
    "    #     ax[i].grid()\n",
    "    #     ax[i].legend()\n",
    "\n",
    "    fig.suptitle(\"Assembly Time Trace vs Random Ensemble Time Trace\")\n",
    "    # Calculate mean r score between all pairs of assemblies\n",
    "    assembly_r_scores = []\n",
    "    assembly_r_scores_array = np.zeros((num_assemblies, num_assemblies))\n",
    "    for i in range(num_assemblies):\n",
    "        for j in range(num_assemblies):\n",
    "            if i != j:\n",
    "                assembly_r_scores.append(stats.pearsonr(assembly_traces[:, i], assembly_traces[:, j])[0])\n",
    "                assembly_r_scores_array[i, j] = stats.pearsonr(assembly_traces[:, i], assembly_traces[:, j])[0]\n",
    "    print(f\"Mean R^2 Score: {np.mean(assembly_r_scores)}\")\n",
    "    # Cacuate mean r score between all pairs of random ensembles\n",
    "    null_r_scores = []\n",
    "    for i in range(num_assemblies):\n",
    "        for j in range(num_assemblies):\n",
    "            if i != j:\n",
    "                null_r_scores.append(stats.pearsonr(random_ensemble_traces[:, i], random_ensemble_traces[:, j])[0])\n",
    "        # print(stats.pearsonr(random_ensemble_traces[:, i], random_ensemble_traces[:, i])[0])\n",
    "        # print()\n",
    "        # null_r_scores.append(stats.pearsonr(random_ensemble_traces[:, i], random_ensemble_traces[:, j])[0])\n",
    "    print(f\"Mean R^2 Score Random Ensemble: {np.mean(null_r_scores)}\")\n",
    "    #perform a wicoxen ranksum test on the two sets of r scores\n",
    "    print(stats.ranksums(assembly_r_scores, null_r_scores))\n",
    "    print(stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(null_r_scores).flatten(), alternative = 'less').pvalue)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(14, 10))    \n",
    "    all_arr = [np.array(assembly_r_scores).flatten(),\n",
    "                np.array(null_r_scores).flatten()]\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    ax = sns.boxplot(data=all_arr,\n",
    "                    notch=True, showcaps=True,\n",
    "                    flierprops={\"marker\": \"x\"},\n",
    "                    boxprops={\"facecolor\": (.4, .6, .8, .5)},\n",
    "                    medianprops={\"color\": \"coral\"},\n",
    "                )\n",
    "    ax.set_xticklabels([\"Assemblies\", \"Random Ensembles\"], size = 16)\n",
    "    ax.set_title('Correlation of Assemblies and Random Ensembles', size = 20)\n",
    "    ax.set_ylabel('Correlation (R)', size = 18)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    medians = np.array(\n",
    "        [np.median(np.array(assembly_r_scores).flatten()),\n",
    "        np.median(np.array(null_r_scores).flatten())]\n",
    "    )\n",
    "\n",
    "    vertical_offset = medians * 0.02 # offset from median for display\n",
    "    p_values = [np.nan,\n",
    "                'P-Val: {:.3g}'.format(stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(null_r_scores).flatten(), alternative = 'less').pvalue, 5)]\n",
    "\n",
    "    for xtick in ax.get_xticks():\n",
    "        if xtick != 0:\n",
    "            ax.text(xtick, medians[xtick] + vertical_offset[xtick], p_values[xtick], \n",
    "                    horizontalalignment='center', size= 12, color='black', weight='semibold')\n",
    "\n",
    "    plt.savefig('assemblies_vs_random_ensembles_boxplot.png', dpi = 1200)\n",
    "    plt.show()\n",
    "\n",
    "def get_assembly_time_trace(coactivity_trace, scores_in_order, name='scored_time_trace'):\n",
    "    # Set up three subplots\n",
    "    num_assemblies = coactivity_trace.shape[1]\n",
    "    fig, ax = plt.subplots(num_assemblies, 1, figsize=(12, 12))\n",
    "\n",
    "    # plot\n",
    "    for i in range(num_assemblies):\n",
    "        ax[i].plot(coactivity_trace[:, i], color='green')\n",
    "        ax[i].set_ylabel(f\"{scores_in_order[i]:.2}\")\n",
    "        ax[i].set_xlabel(\"Time Steps\")\n",
    "        ax[i].grid()\n",
    "\n",
    "    fig.suptitle(\"Assembly Time Trace\")\n",
    "    #plt.savefig(f\"oracle_dists2/assemblies_esteps_150000_affinity_04_{name}.png\", dpi = 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITY_RASTER = scipy.io.loadmat(\n",
    "    \"../Data/Session13/Assembly_Files/esteps_150000_affinity_04_sessionM409828_13_ACTIVITY-RASTER.mat\", struct_as_record=True, squeeze_me=True)\n",
    "SGC_ASSEMBLIES = scipy.io.loadmat(\n",
    "    \"../Data/Session13/Assembly_Files/esteps_150000_affinity_04_sessionM409828_13_SGC-ASSEMBLIES.mat\", struct_as_record=True, squeeze_me=True)\n",
    "\n",
    "#print(ACTIVITY_RASTER.keys())\n",
    "\n",
    "activity_raster = ACTIVITY_RASTER['activity_raster']\n",
    "activity_raster_peaks = ACTIVITY_RASTER['activity_raster_peaks']\n",
    "\n",
    "coactivity_trace = activity_raster.mean(axis=1)\n",
    " \n",
    "assemblies = SGC_ASSEMBLIES['assemblies']\n",
    "#print(assemblies)\n",
    "assembly_coactivity_trace = np.vstack(\n",
    "    [activity_raster[:, A-1].mean(axis=1) for A in assemblies]).T\n",
    "\n",
    "for A in assemblies:\n",
    "    print(A)\n",
    "    print(activity_raster[:, A-1].shape)\n",
    "# print(SGC_ASSEMBLIES['assemblies'].shape)\n",
    "\n",
    "scores_in_order = np.load('../Data/Session13/Assembly_Files/assemblies_esteps_150000_affinity_04_session13_natural_movie_oracle_scores.npy')\n",
    "get_assembly_time_trace(assembly_coactivity_trace, scores_in_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITY_RASTER = scipy.io.loadmat(\n",
    "    \"../Data/Session13/Assembly_Files/esteps_150000_affinity_04_sessionM409828_13_ACTIVITY-RASTER.mat\", struct_as_record=True, squeeze_me=True)\n",
    "SGC_ASSEMBLIES = scipy.io.loadmat(\n",
    "    \"../Data/Session13/Assembly_Files/esteps_150000_affinity_04_sessionM409828_13_SGC-ASSEMBLIES.mat\", struct_as_record=True, squeeze_me=True)\n",
    "\n",
    "#print(ACTIVITY_RASTER.keys())\n",
    "\n",
    "activity_raster = ACTIVITY_RASTER['activity_raster']\n",
    "activity_raster_peaks = ACTIVITY_RASTER['activity_raster_peaks']\n",
    "\n",
    "coactivity_trace = activity_raster.mean(axis=1)\n",
    "\n",
    "assemblies = SGC_ASSEMBLIES['assemblies']\n",
    "#print(assemblies)\n",
    "assembly_coactivity_trace = np.vstack(\n",
    "    [activity_raster[:, A-1].mean(axis=1) for A in assemblies]).T\n",
    "\n",
    "scores_in_order = np.load('oracle_dists/assemblies_esteps_150000_affinity_04_session13_natural_movie_oracle_scores.npy')\n",
    "\n",
    "correct_order = []\n",
    "for i in map_ordered_to_sgc_output.values():\n",
    "    correct_order.append(i)\n",
    "correct_order = np.array(correct_order)\n",
    "\n",
    "\n",
    "ordered_assembly_coactivity_trace = assembly_coactivity_trace[:,correct_order - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblies = SGC_ASSEMBLIES['assemblies']\n",
    "ordered_assemblies = assemblies[correct_order- 1]\n",
    "for A in ordered_assemblies:\n",
    "    print(len(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ns = daf.get_plane_names(nwb_f=nwb_f)\n",
    "plane_ns\n",
    "rois_dict = {}\n",
    "for plane_n in plane_ns:\n",
    "    roi_ns = daf.get_roi_ns(nwb_f=nwb_f, plane_n=plane_n)\n",
    "    print(f'there are {len(roi_ns)} in {plane_n} of session: {sess_id}:')\n",
    "    print('\\nnames of first 100 rois:\\n')\n",
    "    print(roi_ns[0:100])\n",
    "    rois_dict[plane_n] = roi_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_neurons = set()\n",
    "for assembly in assemblies:\n",
    "    assembly_neurons.update(set(assembly))\n",
    "assembly_neurons = np.array(list(assembly_neurons))\n",
    "len(assembly_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import v1dd_physiology.data_fetching as daf\n",
    "nwb_f = h5py.File('M409828_13_20181213.nwb', 'r')\n",
    "\n",
    "# %%\n",
    "sess_id = daf.get_session_id(nwb_f=nwb_f)\n",
    "print(sess_id)\n",
    "\n",
    "plane_ns = daf.get_plane_names(nwb_f=nwb_f)\n",
    "print(\"Planes: \", plane_ns)\n",
    "\n",
    "for plane_n in plane_ns:\n",
    "    depth = daf.get_plane_depth(nwb_f=nwb_f, plane_n=plane_n)\n",
    "    print(f'depth of {plane_n}: {depth} um')\n",
    "\n",
    "# %%\n",
    "# Get Repeated Natural Movies (11 NATURAL MOVIES SHOWN 9 TIMES)\n",
    "# trial_fluorescence = []\n",
    "presentation = nwb_f['stimulus']['presentation']\n",
    "nm_timestamps = np.array(\n",
    "    presentation['natural_movie'].get('timestamps'))\n",
    "nm_data = np.array(presentation['natural_movie'].get('data')) # columns name dataset, gives you the start time, the end time, and the frame number for the natural movie data that was presented\n",
    "new_clips = np.where(nm_data[:, 2] == 0)[0] # get the index where a new clip rotation begins (frame numbers repeat)\n",
    "clip_duration = 300  \n",
    "\n",
    "# new_clips[1]-1\n",
    "# for repeat_id in range(new_clips.shape[0]):\n",
    "#     frames_to_capture = np.where(f_ts >= nm_timestamps[new_clips[repeat_id]])[\n",
    "#         0][0:clip_duration]\n",
    "#     trial_fluorescence.append(f[frames_to_capture])\n",
    "#     trial_fluorescence_np = np.array(trial_fluorescence)\n",
    "#     for trial_idx in range(trial_fluorescence_np.shape[0]):\n",
    "#         removed_trial = trial_fluorescence_np[trial_idx]\n",
    "#         remaining_trials = np.delete(\n",
    "#             trial_fluorescence_np, trial_idx, 0)\n",
    "#         r, p = scipy.stats.pearsonr(\n",
    "#             removed_trial, np.mean(remaining_trials, 0))\n",
    "#         oracle_array[current_count, trial_idx] = r\n",
    "\n",
    "# %%\n",
    "f = ordered_assembly_coactivity_trace\n",
    "passing_roi_count = f.shape[1] # 15\n",
    "\n",
    "coactivity_during_movie = np.zeros((passing_roi_count, 300))\n",
    "results = {}\n",
    "# print(coactivity_trace)\n",
    "f_ts = np.array(nwb_f['processing']['rois_and_traces_plane0']\n",
    "                ['Fluorescence']['f_raw_subtracted'].get('timestamps'))\n",
    "\n",
    "# Get Repeated Natural Movies\n",
    "trial_fluorescence = []\n",
    "presentation = nwb_f['stimulus']['presentation']\n",
    "nm_timestamps = np.array(\n",
    "    presentation['natural_movie'].get('timestamps'))\n",
    "nm_data = np.array(presentation['natural_movie'].get('data'))[:-900] # don't use the last 900 frames, these are the \"short\" nms which are unrelated\n",
    "new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "clip_duration = 300  # new_clips[1]-1\n",
    "\n",
    "clip_ids = []\n",
    "assembly_coactivity_time_traces = []\n",
    "start_of_nms = np.where(f_ts > nm_data[0,0])[0][0] # find the frames where the natural movies start to be presented\n",
    "end_of_nms = np.where(f_ts > nm_data[-1,1])[0][0] # find the frames where the natural movies finish presenting\n",
    "for time_idx in range(start_of_nms, end_of_nms):\n",
    "    idx_nm = np.where(nm_data[:,1] > f_ts[time_idx])[0][0] # get the first index\n",
    "    total_frame_presented = nm_data[idx_nm, 2]\n",
    "    within_repeats_frame_num = total_frame_presented % 3600\n",
    "    clip_id = within_repeats_frame_num // 300\n",
    "    clip_ids.append(clip_id)\n",
    "    assembly_coactivity_time_traces.append(ordered_assembly_coactivity_trace[time_idx,:])\n",
    "\n",
    "clip_ids = np.array(clip_ids).reshape(-1,1)\n",
    "assembly_coactivity_time_traces = np.array(assembly_coactivity_time_traces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_dff = np.load(\"../Data/Session13/sessionM409828_13_nm_dff.npy\") # stefan extracted\n",
    "nm_events = np.load(\"../Data/Session13/sessionM409828_13_nm_events.npy\") # stefan extracted\n",
    "nm_stimulus = np.load(\"../Data/Session13/sessionM409828_13_nm_stimulus.npy\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 1: Produce 'random_ensembles': Random Collections of Neurons that are same sizes as the assemblies. \n",
    "\n",
    "### Set the seed for reproducability, outside of the loop.\n",
    "random.seed(47)\n",
    "np.random.seed(47)\n",
    "\n",
    "### Overlap is fine so we don't have to worry about that.\n",
    "SGC_ASSEMBLIES = scipy.io.loadmat(\n",
    "    \"../Data/Session13/Assembly_Files/esteps_150000_affinity_04_sessionM409828_13_SGC-ASSEMBLIES.mat\", struct_as_record=True, squeeze_me=True)\n",
    "assemblies = SGC_ASSEMBLIES['assemblies']\n",
    "\n",
    "dF_trace = np.load(\"../Data/Session13/sessionM409828_13_CALCIUM-FLUORESCENCE.npy\")\n",
    "num_neurons = dF_trace.shape[1]\n",
    "\n",
    "random_ensembles = []\n",
    "for A in assemblies:\n",
    "    curr_length = len(A)\n",
    "    # get random ids, make sure there are no repeats in the ids for that specific ensemble\n",
    "    random_ensembles.append(np.sort(np.array(random.sample(range(num_neurons), curr_length))))\n",
    "\n",
    "# Order the random ensembles by size\n",
    "random_ensembles.sort(key = len)\n",
    "random_ensembles.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_ensembles.pkl', 'wb') as f:\n",
    "    pickle.dump(random_ensembles, f)\n",
    "random_ensembles\n",
    "\n",
    "# %%\n",
    "print(\"Assembly Lengths\")\n",
    "assemblies = list(assemblies)\n",
    "assemblies.sort(key = len)\n",
    "assemblies.reverse()\n",
    "print([len(A) for A in assemblies])\n",
    "\n",
    "print(\"Random Ensemble Lengths\")\n",
    "print([len(r) for r in random_ensembles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 2: Use the raster plots produced by SGC\n",
    "### STEP 3: Produce a co-activity trace of each random_ensemble\n",
    "ACTIVITY_RASTER = scipy.io.loadmat(\n",
    "    \"../Data/Session13/Assembly_Files/esteps_150000_affinity_04_sessionM409828_13_ACTIVITY-RASTER.mat\", struct_as_record=True, squeeze_me=True)\n",
    "\n",
    "activity_raster = ACTIVITY_RASTER['activity_raster']\n",
    "#activity_raster_peaks = ACTIVITY_RASTER['activity_raster_peaks']\n",
    "\n",
    "coactivity_trace = activity_raster.mean(axis=1)\n",
    "\n",
    "# Assembly Coactivity Trace\n",
    "assembly_coactivity_trace = np.vstack(\n",
    "    [activity_raster[:, A-1].mean(axis=1) for A in assemblies]).T\n",
    "\n",
    "# Random Ensemble Coactivity Trace\n",
    "random_ensembles_coactivity_trace = np.vstack(\n",
    "    [activity_raster[:, A-1].mean(axis=1) for A in random_ensembles]).T\n",
    "\n",
    "# %%\n",
    "def get_ensemble_time_trace(coactivity_trace, title):\n",
    "    # Set up three subplots\n",
    "    num_assemblies = coactivity_trace.shape[1]\n",
    "    fig, ax = plt.subplots(num_assemblies, 1, figsize=(12, 12))\n",
    "\n",
    "    # plot\n",
    "    for i in range(num_assemblies):\n",
    "        ax[i].plot(coactivity_trace[:, i], color=(.4, .6, .8, .5))\n",
    "        ax[i].set_ylabel(\"Co-A\")\n",
    "        ax[i].set_xlabel(\"Time Steps\")\n",
    "        ax[i].grid()\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    #plt.savefig(f\"oracle_dists2/assemblies_esteps_150000_affinity_04_{name}.png\", dpi = 1200)\n",
    "\n",
    "# %%\n",
    "### Plot the assembly coactivity traces in order\n",
    "scores_in_order = np.load('oracle_dists/assemblies_esteps_150000_affinity_04_session13_natural_movie_oracle_scores.npy')\n",
    "get_ensemble_time_trace(assembly_coactivity_trace, title = \"Co-Activity Time Trace\")\n",
    "\n",
    "# %%\n",
    "### Plot the random ensemble coactivity traces in order\n",
    "scores_in_order = np.load('oracle_dists/assemblies_esteps_150000_affinity_04_session13_natural_movie_oracle_scores.npy')\n",
    "get_ensemble_time_trace(random_ensembles_coactivity_trace, title = \"Random Ensemble Time Trace\")\n",
    "\n",
    "# %%\n",
    "# Check UpSet Plot as a sanity check that we got a proper random sample\n",
    "RE = {}\n",
    "for i, ensemble in enumerate(random_ensembles):\n",
    "    print(f'RE {i + 1} Size: {len(ensemble)} Neurons')\n",
    "    RE[f\"RE {i + 1}\"] = ensemble - 1 # Correct the IDs for Python Indexing\n",
    "all_sets = upsetplot.from_contents(RE)\n",
    "\n",
    "ax_dict = upsetplot.UpSet(all_sets, subset_size='count', min_subset_size= 10, \n",
    "                            show_counts = True, show_percentages = False,\n",
    "                            sort_by = 'cardinality', facecolor=\"grey\").plot()\n",
    "plt.title(\"Intersection of Random Ensembles in Scan 1.3 of V1DD\", size = 24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_traces = assembly_coactivity_trace\n",
    "random_ensemble_traces = random_ensembles_coactivity_trace\n",
    "num_assemblies = assembly_traces.shape[1]\n",
    "\n",
    "assembly_r_scores = []\n",
    "for i in range(num_assemblies):\n",
    "    for j in range(num_assemblies):\n",
    "        if i != j:\n",
    "            assembly_r_scores.append(stats.pearsonr(assembly_traces[:, i], assembly_traces[:, j])[0])\n",
    "print(f\"Mean R Score: {np.mean(assembly_r_scores)}\")\n",
    "# Cacuate mean r score between all pairs of random ensembles\n",
    "null_r_scores = []\n",
    "for i in range(num_assemblies):\n",
    "    for j in range(num_assemblies):\n",
    "        if i != j:\n",
    "            null_r_scores.append(stats.pearsonr(random_ensemble_traces[:, i], random_ensemble_traces[:, j])[0])\n",
    "    # print(stats.pearsonr(random_ensemble_traces[:, i], random_ensemble_traces[:, i])[0])\n",
    "    # print()\n",
    "    # null_r_scores.append(stats.pearsonr(random_ensemble_traces[:, i], random_ensemble_traces[:, j])[0])\n",
    "print(f\"Mean R Score Random Ensemble: {np.mean(null_r_scores)}\")\n",
    "#perform a wicoxen ranksum test on the two sets of r scores\n",
    "print(stats.ranksums(assembly_r_scores, null_r_scores))\n",
    "print(stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(null_r_scores).flatten(), alternative = 'less').pvalue)\n",
    "\n",
    "\n",
    "def calculate_pearson_matrix(data):\n",
    "    # Subtract the mean of each time series\n",
    "    data_mean_subtracted = data - np.mean(data, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute the covariance matrix\n",
    "    covariance_matrix = np.dot(data_mean_subtracted, data_mean_subtracted.T)\n",
    "    \n",
    "    # Compute the standard deviation for each time series\n",
    "    std_devs = np.sqrt(np.sum(data_mean_subtracted**2, axis=1))\n",
    "    \n",
    "    # Compute the Pearson correlation matrix\n",
    "    pearson_matrix = covariance_matrix / np.outer(std_devs, std_devs)\n",
    "    \n",
    "    return pearson_matrix\n",
    "\n",
    "a_cell_r_scores = []\n",
    "no_a_cell_r_scores = []\n",
    "all_cell_r_scores = []\n",
    "print(\"Activity raster shape:\", activity_raster.shape)\n",
    "raster_pearson = calculate_pearson_matrix(activity_raster.T)\n",
    "for i in tqdm(range(activity_raster.shape[1])):\n",
    "    for j in range(activity_raster.shape[1]):\n",
    "        if i != j:\n",
    "            r = raster_pearson[i, j]\n",
    "            all_cell_r_scores.append(r)\n",
    "            if i in assembly_neurons and j in assembly_neurons:\n",
    "                a_cell_r_scores.append(r)\n",
    "            elif i not in assembly_neurons and j not in assembly_neurons:\n",
    "                no_a_cell_r_scores.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate p-value\n",
    "p_value_assembly_null = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(null_r_scores).flatten(), alternative='greater').pvalue\n",
    "p_value_assembly_all = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(all_cell_r_scores).flatten(), alternative='greater').pvalue\n",
    "p_value_assembly_a = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(a_cell_r_scores).flatten(), alternative='greater').pvalue\n",
    "p_value_assembly_no_a = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(no_a_cell_r_scores).flatten(), alternative='greater').pvalue\n",
    "p_value_a_no_a = stats.ranksums(np.array(a_cell_r_scores).flatten(), np.array(no_a_cell_r_scores).flatten(), alternative='greater').pvalue\n",
    "\n",
    "\n",
    "import ptitprince as pt\n",
    "# Create a figure\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Prepare data for raincloud plot\n",
    "# print(all_arr)\n",
    "print(len(assembly_r_scores))\n",
    "print(len(null_r_scores))\n",
    "data = pd.DataFrame({\n",
    "    \"Values\": np.concatenate((np.array(assembly_r_scores), np.array(null_r_scores))),\n",
    "    \"Group\": [f\"Assembly\\n(n={len(assembly_r_scores)})\"] * len(assembly_r_scores) + \\\n",
    "                [f\"Null Ensembles\\n(n={len(null_r_scores)})\"] * len(null_r_scores)\n",
    "})\n",
    "\n",
    "# Prepare data for raincloud plot\n",
    "data = pd.DataFrame({\n",
    "    \"Activity Correlations\": np.concatenate((np.array(assembly_r_scores), np.array(null_r_scores), np.array(all_cell_r_scores), np.array(a_cell_r_scores), np.array(no_a_cell_r_scores))),\n",
    "    \"Cell Grouping\": [f\"Assemblies\\nn={len(assembly_r_scores)}\"] * len(assembly_r_scores) + \\\n",
    "                [f\"Random\\nEnsembles\\nn={len(null_r_scores)}\"] * len(null_r_scores) + \\\n",
    "                [f\"All\\nCells\\nn={len(all_cell_r_scores)}\"] * len(all_cell_r_scores) + \\\n",
    "                [f\"Assembly\\nCells\\nn={len(a_cell_r_scores)}\"] * len(a_cell_r_scores) + \\\n",
    "                [f\"Non-Assembly\\nCells\\nn={len(no_a_cell_r_scores)}\"] * len(no_a_cell_r_scores)\n",
    "})\n",
    "\n",
    "# x_labels = [f\"Assemblies\\nn={len(assembly_r_scores)}\", f\"Random\\nEnsembles\\nn={len(null_r_scores)}\", f\"All\\nCells\\nn={len(all_cell_r_scores)}\", f\"Assembly\\nCells\\nn={len(a_cell_r_scores)}\", f\"Non-Assembly\\nCells\\nn={len(no_a_cell_r_scores)}\"]\n",
    "\n",
    "print('Raincloud...')\n",
    "# Create the raincloud plot\n",
    "ax = pt.RainCloud(\n",
    "    y=\"Activity Correlations\",\n",
    "    x=\"Cell Grouping\",\n",
    "    data=data,\n",
    "    palette=[(.4, .6, .8, .5), 'grey'],\n",
    "    width_viol=0.6,  # Adjust violin width\n",
    "    alpha=0.8,  # Transparency of the cloud\n",
    "    move=0.1,  # Adjust position of violins\n",
    "    point_size = 6,\n",
    "    orient=\"v\",  # Horizontal orientation\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "print('P-Values')\n",
    "# # Calculate p-value\n",
    "p_value_assembly_null = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(null_r_scores).flatten(), alternative='less').pvalue\n",
    "p_value_assembly_all = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(all_cell_r_scores).flatten(), alternative='greater').pvalue\n",
    "p_value_assembly_a = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(a_cell_r_scores).flatten(), alternative='greater').pvalue\n",
    "p_value_assembly_no_a = stats.ranksums(np.array(assembly_r_scores).flatten(), np.array(no_a_cell_r_scores).flatten(), alternative='greater').pvalue\n",
    "print('Between A and No A')\n",
    "p_value_a_no_a = stats.ranksums(np.array(a_cell_r_scores).flatten(), np.array(no_a_cell_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "print('Between All and A')\n",
    "p_value_all_v_a = stats.ranksums(np.array(all_cell_r_scores).flatten(), np.array(a_cell_r_scores).flatten(), alternative='two-sided').pvalue\n",
    "\n",
    "plt.ylim(-0.1, 1.2)  # Set y-axis limit to [0, 1] for accuracy range\n",
    "\n",
    "# # Add significance stars\n",
    "for p_index, p_value in enumerate([p_value_assembly_null, p_value_assembly_all, p_value_assembly_a, p_value_assembly_no_a, p_value_all_v_a, p_value_a_no_a]):\n",
    "    if p_value < 0.001:\n",
    "        significance = 'P < 0.001\\n***'\n",
    "    elif p_value < 0.01:\n",
    "        significance = 'P < 0.01\\n**'\n",
    "    elif p_value < 0.05:\n",
    "        significance = 'P < 0.05\\n*'\n",
    "    else:\n",
    "        significance = 'ns'\n",
    "\n",
    "    # Add p-value annotation\n",
    "    if p_index == 0:\n",
    "        p_offset = 0.5\n",
    "        p_height = 1.05\n",
    "    elif p_index > 0 and p_index < 4:\n",
    "        p_offset = 1.0 + p_index\n",
    "        p_height = 1.05\n",
    "    elif p_index == 4:\n",
    "        p_offset = 2.5\n",
    "        p_height = 0.5\n",
    "    elif p_index == 5:\n",
    "        p_offset = 3.5\n",
    "        p_height = 0.5\n",
    "\n",
    "    ax.text(p_offset, p_height, f'{significance}', \n",
    "                color='black', ha='center', va='bottom', fontsize=16, weight='bold')\n",
    "\n",
    "# ax.annotate(f'P-Val: {p_value:.3g}', xy=(1, medians[1] + vertical_offset[1]), xytext=(1, medians[1] + vertical_offset[1] + 0.05),\n",
    "#             arrowprops=dict(facecolor='black', shrink=0.05), horizontalalignment='center', size=18, color='black', weight='semibold')\n",
    "\n",
    "print('Finishing off figure...')\n",
    "# Add a multiline title to include the p-value, add y_label\n",
    "title = f'Correlation'\n",
    "plt.title(title, size=24)\n",
    "plt.xlabel('Correlation (R)', size=20)\n",
    "plt.xticks(fontsize=20)  # Adjust size of xticks\n",
    "plt.yticks(fontsize=20)  # Adjust size of yticks\n",
    "plt.ylabel(\"Groups\", size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlations_assemblies_vs_random_ensembles_raincould_plot.png', dpi=1200)\n",
    "plt.savefig('correlations_assemblies_vs_random_ensembles_raincould_plot.pdf', dpi=1200)\n",
    "plt.savefig('correlations_assemblies_vs_random_ensembles_raincould_plot.svg', dpi=1200)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Prepare Decoding Framework in the same way as Assemblies\n",
    "print('Decoding...')\n",
    "nwb_f = h5py.File('M409828_13_20181213.nwb', 'r')\n",
    "\n",
    "# Get Repeated Natural Movies (11 NATURAL MOVIES SHOWN 9 TIMES)\n",
    "# trial_fluorescence = []\n",
    "presentation = nwb_f['stimulus']['presentation']\n",
    "nm_timestamps = np.array(\n",
    "    presentation['natural_movie'].get('timestamps'))\n",
    "nm_data = np.array(presentation['natural_movie'].get('data')) # columns name dataset, gives you the start time, the end time, and the frame number for the natural movie data that was presented\n",
    "new_clips = np.where(nm_data[:, 2] == 0)[0] # get the index where a new clip rotation begins (frame numbers repeat)\n",
    "clip_duration = 300  \n",
    "\n",
    "f = random_ensembles_coactivity_trace\n",
    "passing_roi_count = f.shape[1] # 15\n",
    "\n",
    "coactivity_during_movie = np.zeros((passing_roi_count, 300))\n",
    "results = {}\n",
    "# print(coactivity_trace)\n",
    "f_ts = np.array(nwb_f['processing']['rois_and_traces_plane0']\n",
    "                ['Fluorescence']['f_raw_subtracted'].get('timestamps'))\n",
    "\n",
    "# Get Repeated Natural Movies\n",
    "trial_fluorescence = []\n",
    "presentation = nwb_f['stimulus']['presentation']\n",
    "nm_timestamps = np.array(\n",
    "    presentation['natural_movie'].get('timestamps'))\n",
    "nm_data = np.array(presentation['natural_movie'].get('data'))[:-900] # don't use the last 900 frames, these are the \"short\" nms which are unrelated\n",
    "new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "clip_duration = 300  # new_clips[1]-1\n",
    "\n",
    "clip_ids = []\n",
    "random_ensemble_coactivity_time_traces = []\n",
    "start_of_nms = np.where(f_ts > nm_data[0,0])[0][0] # find the frames where the natural movies start to be presented\n",
    "end_of_nms = np.where(f_ts > nm_data[-1,1])[0][0] # find the frames where the natural movies finish presenting\n",
    "for time_idx in tqdm(range(start_of_nms, end_of_nms)):\n",
    "    idx_nm = np.where(nm_data[:,1] > f_ts[time_idx])[0][0] # get the first index\n",
    "    total_frame_presented = nm_data[idx_nm, 2]\n",
    "    within_repeats_frame_num = total_frame_presented % 3600\n",
    "    clip_id = within_repeats_frame_num // 300\n",
    "    clip_ids.append(clip_id)\n",
    "    random_ensemble_coactivity_time_traces.append(random_ensembles_coactivity_trace[time_idx,:])\n",
    "\n",
    "clip_ids = np.array(clip_ids).reshape(-1,1)\n",
    "random_ensemble_coactivity_time_traces = np.array(random_ensemble_coactivity_time_traces)\n",
    "\n",
    "print(clip_ids.shape, random_ensemble_coactivity_time_traces.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique clip_ids and their counts\n",
    "unique_clip_ids, counts = np.unique(clip_ids, return_counts=True)\n",
    "\n",
    "# Determine the minimum count of any clip_id\n",
    "min_count = np.min(counts)\n",
    "\n",
    "# Create new lists for balanced clip_ids and corresponding assembly_coactivations\n",
    "balanced_clip_ids = []\n",
    "balanced_assembly_coactivations = []\n",
    "\n",
    "# Sample min_count indices for each clip_id\n",
    "for clip_id in unique_clip_ids:\n",
    "    # Get indices of the current clip_id\n",
    "    indices = np.where(clip_ids == clip_id)[0]\n",
    "    # Randomly sample min_count indices\n",
    "    np.random.seed(747)\n",
    "    sampled_indices = np.random.choice(indices, min_count, replace=False)\n",
    "    # Append the sampled indices' values to the new lists\n",
    "    balanced_clip_ids.extend(clip_ids[sampled_indices])\n",
    "    balanced_assembly_coactivations.extend(assembly_coactivity_time_traces[sampled_indices])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "balanced_clip_ids = np.array(balanced_clip_ids)\n",
    "balanced_assembly_coactivations = np.array(balanced_assembly_coactivations)\n",
    "\n",
    "# Shuffle to ensure random distribution\n",
    "shuffled_indices = np.random.default_rng(seed=747).permutation(len(balanced_clip_ids))\n",
    "balanced_clip_ids = balanced_clip_ids[shuffled_indices]\n",
    "balanced_assembly_coactivations = balanced_assembly_coactivations[shuffled_indices]\n",
    "\n",
    "# Output the balanced arrays\n",
    "print(\"Balanced clip_ids:\", balanced_clip_ids)\n",
    "print(\"Balanced assembly_coactivations:\", balanced_assembly_coactivations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "balanced_clip_ids.shape, balanced_assembly_coactivations.shape\n",
    "\n",
    "# %%\n",
    "plt.hist(balanced_clip_ids)\n",
    "\n",
    "# %%\n",
    "# # Stefan Breadcrumb: Run from here\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_assembly_coactivations, balanced_clip_ids.ravel(), test_size=0.2, random_state=747)\n",
    "\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (50, 50, 100), (100, 100)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive', 'invscaling'],\n",
    "    'batch_size': [64, 128, 256, 512, 1024],\n",
    "    'max_iter': [300]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "clf = MLPClassifier(random_state=747, early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "grid_search = RandomizedSearchCV(clf, param_distributions=param_grid, n_iter=100, cv=5, n_jobs=-1, verbose=1)\n",
    "# grid_search = GridSearchCV(clf, param_distributions=param_grid, n_iter=300, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "# predict the clip_ids\n",
    "y_pred = best_clf.predict(X_test_scaled)\n",
    "\n",
    "# print out the accuracy\n",
    "print(\"Accuracy Score of\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# plot the confusion matrix\n",
    "assembly_cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(assembly_cm, annot=True, vmax = 100, cmap=greymap, annot_kws={\"size\": 55 / np.sqrt(len(assembly_cm)), \"color\": 'white'})\n",
    "plt.title(\"Assembly Clip ID Classification Count\", fontsize=22)\n",
    "plt.xlabel('Predicted', fontsize=20)\n",
    "plt.ylabel('Truth', fontsize=20)\n",
    "plt.yticks(rotation=0, fontsize=18) \n",
    "plt.xticks(fontsize=18)\n",
    "plt.savefig('assembly_balanced_clip_id_decoder_MLPClassifier.png', dpi = 1200)\n",
    "plt.savefig('assembly_balanced_clip_id_decoder_MLPClassifier.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Develop normalized Heatmap\n",
    "assembly_cm_norm = np.round((assembly_cm.astype('float') / assembly_cm.sum(axis=1)[:, np.newaxis])*100)\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(assembly_cm_norm, annot=True, vmax = 100, cmap=greymap, annot_kws={\"size\": 55 / np.sqrt(len(assembly_cm)), \"color\": 'white'})\n",
    "plt.title(\"Assembly Clip ID Classification Percent\", fontsize=20)\n",
    "plt.xlabel('Predicted', fontsize=16)\n",
    "plt.ylabel('Truth', fontsize=16)\n",
    "plt.yticks(rotation=0, fontsize=18)\n",
    "plt.xticks(fontsize=18) \n",
    "plt.savefig('assembly_balanced_clip_id_percentage_decoder_MLPClassifier.png', dpi = 1200)\n",
    "plt.savefig('assembly_balanced_clip_id_percentage_decoder_MLPClassifier.pdf')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "print(clip_ids.shape, random_ensemble_coactivity_time_traces.shape)\n",
    "\n",
    "# Get unique clip_ids and their counts\n",
    "unique_clip_ids, counts = np.unique(clip_ids, return_counts=True)\n",
    "\n",
    "# Determine the minimum count of any clip_id\n",
    "min_count = np.min(counts)\n",
    "\n",
    "# Create new lists for balanced clip_ids and corresponding assembly_coactivations\n",
    "balanced_clip_ids = []\n",
    "balanced_random_ensemble_coactivations = []\n",
    "\n",
    "# Sample min_count indices for each clip_id\n",
    "for clip_id in unique_clip_ids:\n",
    "    # Get indices of the current clip_id\n",
    "    indices = np.where(clip_ids == clip_id)[0]\n",
    "    # Randomly sample min_count indices\n",
    "    np.random.seed(747)\n",
    "    sampled_indices = np.random.choice(indices, min_count, replace=False)\n",
    "    # Append the sampled indices' values to the new lists\n",
    "    balanced_clip_ids.extend(clip_ids[sampled_indices])\n",
    "    balanced_random_ensemble_coactivations.extend(random_ensemble_coactivity_time_traces[sampled_indices])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "balanced_clip_ids = np.array(balanced_clip_ids)\n",
    "balanced_random_ensemble_coactivations = np.array(balanced_random_ensemble_coactivations)\n",
    "\n",
    "# Shuffle to ensure random distribution\n",
    "shuffled_indices = np.random.default_rng(seed=747).permutation(len(balanced_clip_ids))\n",
    "balanced_clip_ids = balanced_clip_ids[shuffled_indices]\n",
    "balanced_random_ensemble_coactivations = balanced_random_ensemble_coactivations[shuffled_indices]\n",
    "\n",
    "# Output the balanced arrays\n",
    "print(\"Balanced clip_ids:\", balanced_clip_ids)\n",
    "print(\"Balanced random_ensemble_coactivations:\", balanced_random_ensemble_coactivations)\n",
    "\n",
    "# %%\n",
    "balanced_clip_ids.shape, balanced_random_ensemble_coactivations.shape\n",
    "\n",
    "# %%\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_random_ensemble_coactivations, balanced_clip_ids.ravel(), test_size=0.2, random_state=747)\n",
    "\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# define the model\n",
    "clf = MLPClassifier(random_state=747)\n",
    "\n",
    "# define the parameter grid: checked on all and relu provided best score (makes results comparable between assembly and null sets as well)\n",
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(50,50,50), (100,)],\n",
    "#     'activation': ['relu'],\n",
    "#     'solver': ['adam'],\n",
    "#     'alpha': [0.0001, 0.001],\n",
    "#     'learning_rate': ['constant', 'adaptive'],\n",
    "#     'batch_size': [64, 128],\n",
    "#     'max_iter': [300]\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (50, 50, 100), (100, 100)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive', 'invscaling'],\n",
    "    'batch_size': [64, 128, 256, 512, 1024],\n",
    "    'max_iter': [300]\n",
    "}\n",
    "\n",
    "\n",
    "clf = MLPClassifier(random_state=747, early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "grid_search = RandomizedSearchCV(clf, param_distributions=param_grid, n_iter=300, cv=5, n_jobs=-1, verbose=1)\n",
    "# grid_search = GridSearchCV(clf, param_distributions=param_grid, n_iter=300, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_clf = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predict the clip_ids\n",
    "y_pred = best_clf.predict(X_test_scaled)\n",
    "\n",
    "# print out the accuracy\n",
    "print(\"Accuracy Score of\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# plot the confusion matrix\n",
    "rand_ensemble_cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(rand_ensemble_cm, annot=True, vmax = 100, cmap=greymap, annot_kws={\"size\": 55 / np.sqrt(len(rand_ensemble_cm)), \"color\": 'white'})\n",
    "plt.title(\"Random Ensemble Clip ID Classification Count\", fontsize=20)\n",
    "plt.xlabel('Predicted', fontsize=16)\n",
    "plt.ylabel('Truth', fontsize=16)\n",
    "plt.yticks(rotation=0, fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.savefig('random_ensemble_balanced_clip_id_decoder_MLPClassifier.png', dpi = 1200)\n",
    "plt.savefig('random_ensemble_balanced_clip_id_decoder_MLPClassifier.pdf')\n",
    "plt.close()\n",
    "\n",
    "# %%\n",
    "\n",
    "# plot the confusion matrix\n",
    "rand_ensemble_cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(rand_ensemble_cm, annot=True, vmax = 100, cmap=greymap, annot_kws={\"size\": 50 / np.sqrt(len(rand_ensemble_cm))})\n",
    "plt.title(\"Random Ensemble Balanced Clip ID Classification\", fontsize=20)\n",
    "plt.xlabel('Predicted', fontsize=16)\n",
    "plt.ylabel('Truth', fontsize=16)\n",
    "plt.yticks(rotation=0, fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.savefig('asdf_random_ensemble_balanced_clip_id_decoder_MLPClassifier.png', dpi = 1200)\n",
    "plt.savefig('asdf_random_ensemble_balanced_clip_id_decoder_MLPClassifier.pdf')\n",
    "plt.close()\n",
    "\n",
    "# %%\n",
    "print('\\n Best estimator:')\n",
    "print(grid_search.best_estimator_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# %%\n",
    "#### Develop normalized Heatmap\n",
    "rand_ensemble_cm_norm = np.round((rand_ensemble_cm.astype('float') / rand_ensemble_cm.sum(axis=1)[:, np.newaxis])*100)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(rand_ensemble_cm_norm, annot=True, vmax = 100, cmap=greymap, annot_kws={\"size\": 55 / np.sqrt(len(rand_ensemble_cm)), \"color\": 'white'})\n",
    "plt.title(\"Random Ensemble Clip ID Classification Percent\", fontsize=20)\n",
    "plt.xlabel('Predicted', fontsize=16)\n",
    "plt.ylabel('Truth', fontsize=16)\n",
    "plt.yticks(rotation=0, fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.savefig('random_ensemble_balanced_clip_id_percentage_decoder_MLPClassifier.png', dpi = 1200)\n",
    "plt.savefig('random_ensemble_balanced_clip_id_percentage_decoder_MLPClassifier.pdf')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "pika_rois_all_dict = {}\n",
    "pika_rois_in_assembly_dict = {}\n",
    "pika_rois_no_assembly_dict = {}\n",
    "for plane_n, roi_ns in rois_dict.items():\n",
    "    pika_rois = []\n",
    "    pika_rois_in_assembly = []\n",
    "    pika_rois_no_assembly = []\n",
    "    # print(plane_n)\n",
    "    for roi_n in roi_ns:\n",
    "        score = daf.get_pika_classifier_score(nwb_f=nwb_f, plane_n=plane_n, roi_n=roi_n)\n",
    "        if score > 0.5:  # Using the threshold from team PIKA, per https://github.com/zhuangjun1981/v1dd_physiology/blob/main/v1dd_physiology/example_notebooks/2022-06-27-data-fetching-basic.ipynb\n",
    "            pika_rois.append(roi_n)\n",
    "            if int(roi_n[4:]) in assembly_neurons:\n",
    "                pika_rois_in_assembly.append(roi_n)\n",
    "            else:\n",
    "                pika_rois_no_assembly.append(roi_n)\n",
    "    pika_rois_all_dict[plane_n] = pika_rois\n",
    "    pika_rois_in_assembly_dict[plane_n] = pika_rois_in_assembly\n",
    "    pika_rois_no_assembly_dict[plane_n] = pika_rois_no_assembly\n",
    "\n",
    "total_rois = np.sum([len(val) for val in pika_rois_all_dict.values()])\n",
    "neuron_all_movie_oracle_r_values = np.zeros((total_rois, 9))\n",
    "\n",
    "total_rois = np.sum([len(val) for val in pika_rois_in_assembly_dict.values()])\n",
    "neuron_in_assembly_movie_oracle_r_values = np.zeros((total_rois, 9))\n",
    "\n",
    "total_rois = np.sum([len(val) for val in pika_rois_no_assembly_dict.values()])\n",
    "neuron_no_assembly_movie_oracle_r_values = np.zeros((total_rois, 9))\n",
    "\n",
    "count_n_all = -1\n",
    "count_n_in_assembly = -1\n",
    "count_n_no_assembly = -1\n",
    "for curr_dict, oracle_array, c in zip([pika_rois_all_dict, pika_rois_in_assembly_dict, pika_rois_no_assembly_dict], \n",
    "        [neuron_all_movie_oracle_r_values, neuron_in_assembly_movie_oracle_r_values, neuron_no_assembly_movie_oracle_r_values],\n",
    "        [1,2,3]):\n",
    "    for plane_n, pika_roi_ns in curr_dict.items():\n",
    "        for roi_n in pika_roi_ns:\n",
    "            if c == 1:\n",
    "                count_n_all += 1\n",
    "                current_count = count_n_all\n",
    "            elif c == 2:\n",
    "                count_n_in_assembly += 1\n",
    "                current_count = count_n_in_assembly\n",
    "            elif c == 3:\n",
    "                count_n_no_assembly += 1\n",
    "                current_count = count_n_no_assembly\n",
    "            ### Get Time Trace\n",
    "            dff, dff_ts = daf.get_single_trace(nwb_f=nwb_f, plane_n=plane_n, roi_n=roi_n, trace_type='dff')\n",
    "            f_binary_raster = activity_raster[:, int(roi_n[4:])-1]\n",
    "            \n",
    "            # Get Repeated Natural Movies\n",
    "            trial_fluorescence = []\n",
    "            presentation = nwb_f['stimulus']['presentation']\n",
    "            nm_timestamps = np.array(\n",
    "                presentation['natural_movie'].get('timestamps'))\n",
    "            nm_data = np.array(presentation['natural_movie'].get('data'))\n",
    "            new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "            clip_duration = 300  # new_clips[1]-1\n",
    "            for repeat_id in range(new_clips.shape[0]):\n",
    "                frames_to_capture = np.where(dff_ts >= nm_timestamps[new_clips[repeat_id]])[\n",
    "                    0][0:clip_duration]\n",
    "                trial_fluorescence.append(f_binary_raster[frames_to_capture])\n",
    "            trial_fluorescence_np = np.array(trial_fluorescence)\n",
    "            for trial_idx in range(trial_fluorescence_np.shape[0]):\n",
    "                removed_trial = trial_fluorescence_np[trial_idx]\n",
    "                remaining_trials = np.delete(\n",
    "                    trial_fluorescence_np, trial_idx, 0)\n",
    "                r, p = scipy.stats.pearsonr(\n",
    "                    removed_trial, np.mean(remaining_trials, 0))\n",
    "                oracle_array[current_count, trial_idx] = r\n",
    "\n",
    "assembly_movie_oracle_r_values = np.zeros((passing_roi_count, 9))\n",
    "f = assembly_coactivity_trace\n",
    "# print(coactivity_trace)\n",
    "f_ts = np.array(nwb_f['processing']['rois_and_traces_plane0']\n",
    "                ['Fluorescence']['f_raw_subtracted'].get('timestamps'))\n",
    "for roi_n in range(passing_roi_count):\n",
    "\n",
    "    # Get Repeated Natural Movies\n",
    "    trial_fluorescence = []\n",
    "    presentation = nwb_f['stimulus']['presentation']\n",
    "    nm_timestamps = np.array(\n",
    "        presentation['natural_movie'].get('timestamps'))\n",
    "    nm_data = np.array(presentation['natural_movie'].get('data'))\n",
    "    new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "    clip_duration = 300  # new_clips[1]-1\n",
    "    for repeat_id in range(new_clips.shape[0]):\n",
    "        frames_to_capture = np.where(f_ts >= nm_timestamps[new_clips[repeat_id]])[\n",
    "            0][0:clip_duration]\n",
    "        trial_fluorescence.append(f[frames_to_capture, roi_n])\n",
    "    trial_fluorescence_np = np.array(trial_fluorescence)\n",
    "    for trial_idx in range(trial_fluorescence_np.shape[0]):\n",
    "        removed_trial = trial_fluorescence_np[trial_idx]\n",
    "        remaining_trials = np.delete(\n",
    "            trial_fluorescence_np, trial_idx, 0)\n",
    "        r, p = scipy.stats.pearsonr(\n",
    "            removed_trial, np.mean(remaining_trials, 0))\n",
    "        assembly_movie_oracle_r_values[roi_n, trial_idx] = r\n",
    "\n",
    "# Plot Movie Oracles\n",
    "mean_over_holdouts = np.mean(assembly_movie_oracle_r_values, 1)\n",
    "fig = plt.figure()\n",
    "plt.title('Assembly natural movie oracle score')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.hist(mean_over_holdouts[:], bins=50)\n",
    "#plt.savefig('oracle_dists2/assemblies_esteps_150000__affinity_04_session'+str(13)+'_movies.png')\n",
    "plt.show()\n",
    "\n",
    "random_ensemble_movie_oracle_r_values = np.zeros((passing_roi_count, 9))\n",
    "f = random_ensembles_coactivity_trace\n",
    "# print(coactivity_trace)\n",
    "f_ts = np.array(nwb_f['processing']['rois_and_traces_plane0']\n",
    "                ['Fluorescence']['f_raw_subtracted'].get('timestamps'))\n",
    "for roi_n in range(passing_roi_count):\n",
    "\n",
    "    # Get Repeated Natural Movies\n",
    "    trial_fluorescence = []\n",
    "    presentation = nwb_f['stimulus']['presentation']\n",
    "    nm_timestamps = np.array(\n",
    "        presentation['natural_movie'].get('timestamps'))\n",
    "    nm_data = np.array(presentation['natural_movie'].get('data'))\n",
    "    new_clips = np.where(nm_data[:, 2] == 0)[0]\n",
    "    clip_duration = 300  # new_clips[1]-1\n",
    "    for repeat_id in range(new_clips.shape[0]):\n",
    "        frames_to_capture = np.where(f_ts >= nm_timestamps[new_clips[repeat_id]])[\n",
    "            0][0:clip_duration]\n",
    "        trial_fluorescence.append(f[frames_to_capture, roi_n])\n",
    "    trial_fluorescence_np = np.array(trial_fluorescence)\n",
    "    for trial_idx in range(trial_fluorescence_np.shape[0]):\n",
    "        removed_trial = trial_fluorescence_np[trial_idx]\n",
    "        remaining_trials = np.delete(\n",
    "            trial_fluorescence_np, trial_idx, 0)\n",
    "        r, p = scipy.stats.pearsonr(\n",
    "            removed_trial, np.mean(remaining_trials, 0))\n",
    "        random_ensemble_movie_oracle_r_values[roi_n, trial_idx] = r\n",
    "\n",
    "import ptitprince as pt\n",
    "# Plot Movie Oracles\n",
    "mean_over_holdouts = np.mean(random_ensemble_movie_oracle_r_values, 1)\n",
    "fig = plt.figure()\n",
    "plt.title('Random Ensemble natural movie oracle score')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.hist(mean_over_holdouts[:], bins=50)\n",
    "#plt.savefig('oracle_dists2/assemblies_esteps_150000__affinity_04_session'+str(13)+'_movies.png')\n",
    "plt.show()\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "all_arr = [np.array(assembly_movie_oracle_r_values).flatten(),\n",
    "            np.array(random_ensemble_movie_oracle_r_values).flatten(),\n",
    "            np.array(neuron_all_movie_oracle_r_values).flatten(),\n",
    "            np.array(neuron_in_assembly_movie_oracle_r_values).flatten(),\n",
    "            np.array(neuron_no_assembly_movie_oracle_r_values).flatten()]\n",
    "\n",
    "# Prepare data for raincloud plot\n",
    "data = pd.DataFrame({\n",
    "    \"Values\": np.concatenate(all_arr),\n",
    "    \"Group\": [f\"Assemblies\"] * len(assembly_movie_oracle_r_values.flatten()) + \\\n",
    "                [f\"Random\\nEnsembles\"] * len(random_ensemble_movie_oracle_r_values.flatten()) + \\\n",
    "                [f\"All\\nCells\"] * len(neuron_all_movie_oracle_r_values.flatten()) + \\\n",
    "                [f\"Assembly\\nCells\"] * len(neuron_in_assembly_movie_oracle_r_values.flatten()) + \\\n",
    "                [f\"Non-Assembly\\nCells\"] * len(neuron_no_assembly_movie_oracle_r_values.flatten())\n",
    "})\n",
    "\n",
    "# Create the raincloud plot\n",
    "ax = pt.RainCloud(\n",
    "    y=\"Values\",\n",
    "    x=\"Group\",\n",
    "    data=data,\n",
    "    palette=[(.4, .6, .8, .5), 'grey'],\n",
    "    width_viol=0.6,  # Adjust violin width\n",
    "    alpha=0.8,  # Transparency of the cloud\n",
    "    move=0.1,  # Adjust position of violins\n",
    "    point_size = 4,\n",
    "    orient=\"v\"  # Horizontal orientation\n",
    ")\n",
    "\n",
    "plt.ylim(-0.2, 1.2)\n",
    "\n",
    "# Calculate p-values for Wilcoxon rank-sum tests\n",
    "groups = [\"Random\\nEnsembles\", \"All\\nCells\", \"Assembly\\nCells\", \"Non-Assembly\\nCells\"]\n",
    "p_values = []\n",
    "for group in groups:\n",
    "    group_values = data[data[\"Group\"] == group][\"Values\"]\n",
    "    p_value = stats.ranksums(data[data[\"Group\"] == \"Assemblies\"][\"Values\"], group_values, alternative='greater').pvalue\n",
    "    p_values.append(p_value)\n",
    "\n",
    "# Add significance markers as number of asterisks\n",
    "for i, p_value in enumerate(p_values):\n",
    "    if p_value < 0.001:\n",
    "        significance = 'p < 0.001\\n***'\n",
    "    elif p_value < 0.01:\n",
    "        significance = 'p < 0.01\\n**'\n",
    "    elif p_value < 0.05:\n",
    "        significance = 'p < 0.05\\n*'\n",
    "    else:\n",
    "        significance = ''\n",
    "    ax.annotate(significance, xy=(i + 1, data[data[\"Group\"] == groups[i]][\"Values\"].max() + 0.1),\n",
    "                horizontalalignment='center', size=16, color='black', weight='bold')\n",
    "\n",
    "# Add a multiline title to include the p-value, add y_label\n",
    "title = f'Natural Movie Oracle Scores'\n",
    "plt.title(title, size=24)\n",
    "# plt.xlabel('Cell Grouping', size=20)\n",
    "plt.xticks(fontsize=20)  # Adjust size of xticks\n",
    "plt.yticks(fontsize=20)  # Adjust size of yticks\n",
    "plt.ylabel(\"Oracle Score\", size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('oracle_scores_histogram_dff_all_sets_raincloud.png', dpi = 1200)\n",
    "plt.savefig('oracle_scores_histogram_dff_all_sets_raincloud.pdf')\n",
    "plt.savefig('oracle_scores_histogram_dff_all_sets_raincloud.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ginis2(coactivity_trace, null_trace=None, all_trace=None, a_trace=None, no_a_trace=None):\n",
    "    num_assemblies = coactivity_trace.shape[1]\n",
    "    gini_values = [gini(coactivity_trace[:,i]) for i in range(num_assemblies)]\n",
    "    labels = [f'A {i+1}' for i in range(num_assemblies)]\n",
    "    \n",
    "    if no_a_trace is not None:\n",
    "        gini_values_no_a = [gini(no_a_trace[:,i]) for i in range(no_a_trace.shape[1])]\n",
    "        mean_gini_no_a = np.nanmean(gini_values_no_a)\n",
    "        # labels.insert(0, 'Nonassembly\\nCells')\n",
    "    \n",
    "    if a_trace is not None:\n",
    "        gini_values_a = [gini(a_trace[:,i]) for i in range(a_trace.shape[1])]\n",
    "        mean_gini_a = np.nanmean(gini_values_a)\n",
    "        # labels.insert(0, 'Assembly\\nCells')\n",
    "    \n",
    "    # if all_trace is not None:\n",
    "    #     gini_values_all = [gini(all_trace[:,i]) for i in range(all_trace.shape[1])]\n",
    "    #     mean_gini_all = np.nanmean(gini_values_all)\n",
    "    #     labels.insert(0, 'All\\nCells')\n",
    "    \n",
    "    if null_trace is not None:\n",
    "        gini_values_null = [gini(null_trace[:,i]) for i in range(num_assemblies)]\n",
    "        mean_gini_null = np.nanmean(gini_values_null)\n",
    "        # labels.insert(0, 'Null')\n",
    "        \n",
    "    print(gini_values)\n",
    "    print(gini_values_null)\n",
    "    print(labels)\n",
    "\n",
    "    # Create a DataFrame for the grouped bar plot\n",
    "    gini_df = pd.DataFrame({\n",
    "        'Assembly Gini Values': gini_values,\n",
    "        'Random Ensemble Gini Values': gini_values_null,\n",
    "        'Labels': labels\n",
    "    })\n",
    "\n",
    "    # print(gini_df)\n",
    "\n",
    "    # Melt the DataFrame to long format\n",
    "    gini_df_melted = gini_df.melt(id_vars='Labels', value_vars=['Assembly Gini Values', 'Random Ensemble Gini Values'], \n",
    "                                  var_name='Type', value_name='Gini Coefficient')\n",
    "\n",
    "    p_value = stats.ranksums(np.array(gini_values_null).flatten(), np.array(gini_values).flatten(), alternative='less').pvalue\n",
    "\n",
    "    # Create a grouped bar plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    ax = sns.barplot(x='Labels', y='Gini Coefficient', hue='Type', data=gini_df_melted, palette='Paired')\n",
    "    ax.set_title(f'Sparsity of Activity\\np={p_value:0.4g}', size=24)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xticks(fontsize=20, rotation=45)    \n",
    "    ax.set_ylabel('Gini Coefficient', size=24)\n",
    "    ax.set_xlabel('Assembly or Matched Random Ensemble', size=24)\n",
    "    plt.ylim((0, 1.0))\n",
    "\n",
    "    # Add horizontal lines for mean_gini_a and mean_gini_no_a\n",
    "    if a_trace is not None:\n",
    "        ax.axhline(mean_gini_a, color='cornflowerblue', linestyle='--', linewidth=2)\n",
    "        ax.text(len(labels)-5.75, mean_gini_a-0.04, f'Assembly Cells Mean: {mean_gini_a:.3f}', \n",
    "                color='cornflowerblue', ha='left', va='bottom', fontsize=16, weight='bold')\n",
    "\n",
    "    if no_a_trace is not None:\n",
    "        ax.axhline(mean_gini_no_a, color='slategray', linestyle='--', linewidth=2)\n",
    "        ax.text(len(labels)-5.75, mean_gini_no_a, f'Nonassembly Cells Mean: {mean_gini_no_a:.3f}', \n",
    "                color='slategray', ha='left', va='bottom', fontsize=16, weight='bold')\n",
    "\n",
    "    # Add annotations for clarity (optional)\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha='center', va='center', xytext=(0, 30.5), textcoords='offset points', size=18, weight='bold', rotation=90)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(0, 0.95), loc='upper left', frameon=True, fontsize=16)\n",
    "    # plt.legend(, loc='upper left', borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sparsity_with_Gini_coefficient_by_assembly_and_random_ensembles.png')\n",
    "    plt.savefig('sparsity_with_Gini_coefficient_by_assembly_and_random_ensembles.pdf')\n",
    "    plt.savefig('sparsity_with_Gini_coefficient_by_assembly_and_random_ensembles.svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(activity_raster.shape)\n",
    "# print(np.min(no_assembly_neurons))\n",
    "# print(np.min(assembly_neurons))\n",
    "plot_ginis2(coactivity_trace=assembly_coactivity_trace, null_trace=random_ensembles_coactivity_trace, all_trace=activity_raster, a_trace=activity_raster[:,assembly_neurons], no_a_trace=activity_raster[:,no_assembly_neurons])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allensdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
