{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 5 Master Freeze Document\n",
    "\n",
    "This is the master document for Figure 5, which includes all code that will be frozen before receiving the final set of co-registrated cells. All frozen code is tasked to directly test a hypothesis that was made in Hebb's \"The Organization of Behavior\".\n",
    "\n",
    "The methodology for all written code is provided in the *Methods* section of our paper. While the code includes the ability to test between other sets, the main sets of comparison will be coregistered cells which have 'shared' assembly membership to those who have 'nonassembly' membership. Futher clarification is found in the *Methods* section. \n",
    "\n",
    "To analyze for probability of connections and strength of connections, we have specified these tests:\n",
    "\n",
    "1. **Monosynaptic Pairs** - \n",
    "    1. Chi-squared test to binary connectivity\n",
    "    2. Wilcoxon rank-sum test to summed Post Synaptic Density (PSD)\n",
    "2. **Per-cell Outbound and Inbound**\n",
    "    1. Wilcoxon rank-sum test to probability of connection\n",
    "3. **Per-cell Nonzero Outbound and Inbound**\n",
    "    1. Wilcoxon signed rank test to summed PSD volumes\n",
    "    2. Wilcoxon rank-sum to summed PSD volumes\n",
    "4. **Centrality Measurements**\n",
    "    1. Wilcoxon rank-sum to Out-Degree Centrality\n",
    "    2. Wilcoxon rank-sum to In-Degree Centrality\n",
    "    3. Wilcoxon rank-sum to Betweenness Centrality\n",
    "    4. Wilcoxon rank-sum to Closeness Centrality\n",
    "5. Repeat 1-3 for **Multisynaptic (3-Neuron) Chains**\n",
    "6. Repeat 1-3 for **Multisynaptic Chains with a middle interneuron**\n",
    "\n",
    "We additionally perform a **Tail Analysis**, where we perform a **Chi-Squared Test of Goodness-of-Fit** for differences in proportion of connection type comparing all to \"tail\" connections. \n",
    "\n",
    "Any other analysis that will be explored later are presented in the other Figure 5 Master document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import matplotlib.pyplot as plt\n",
    "import ptitprince as pt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import itertools\n",
    "from dotmotif import Motif, GrandIsoExecutor\n",
    "from scipy.stats import kruskal, f_oneway, levene, ranksums, ttest_ind, wilcoxon, norm, chi2_contingency, chisquare\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn import mixture\n",
    "from scipy.interpolate import interp1d\n",
    "from tabulate import tabulate\n",
    "from statannotations.Annotator import Annotator\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "random.seed(747)\n",
    "\n",
    "# Import Stefan's Library for Data Management of V1DD\n",
    "from lsmm_data import LSMMData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-Wise Comparison Functions: Determining the intersection of assembly assignment of two pyramidal cells \n",
    "# These comparison functions map to C in the statistical methods section.\n",
    "def shared(pre, post, A):\n",
    "    try:\n",
    "        return not A[pre].isnonassembly(A[post])\n",
    "    except KeyError:\n",
    "        return False\n",
    "\n",
    "# def nonassembly(pre, post, A):\n",
    "#     try:\n",
    "#         return A[pre].isnonassembly(A[post])\n",
    "#     except KeyError:\n",
    "#         return False\n",
    "\n",
    "def shared_no_a(pre, post, A):\n",
    "    return (pre in no_A) and (post in no_A) # type: ignore\n",
    "\n",
    "def no_a_a(pre, post, A):\n",
    "    return (pre in no_A) and (post not in no_A) # type: ignore\n",
    "\n",
    "def a_no_a(pre, post, A):\n",
    "    return (pre not in no_A) and (post in no_A) # type: ignore\n",
    "\n",
    "def no_a_to_any(pre, _, A):\n",
    "    return (pre in no_A) # type: ignore\n",
    "\n",
    "def a_to_any(pre, _, A):\n",
    "    return (pre not in no_A) # type: ignore\n",
    "\n",
    "comparison_functions = [shared, shared_no_a, no_a_a, a_no_a, no_a_to_any, a_to_any]\n",
    "groups = ['Shared Assembly', 'Nonassembly to Nonassembly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monosynaptic Analysis on Pyramidal Cell Rectangular Connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(figure_name):\n",
    "    plt.savefig(\n",
    "        f\"./draft_figures/{figure_name}.png\",\n",
    "        dpi=500,\n",
    "        bbox_inches=\"tight\")\n",
    "    \n",
    "def plot_shared_vs_nonassembly(shared_values, nonassembly_values, title, y_lab, p_val, save=False, figure_name=None):\n",
    "    \"\"\"\n",
    "    Plots a raincloud plot for two connection type groups, with sample sizes in the y-axis labels.\n",
    "\n",
    "    Parameters:\n",
    "        shared_values (list or array): Data for shared assembly group.\n",
    "        nonassembly_values (list or array): Data for nonassembly assembly group.\n",
    "        title (str): Title of the plot.\n",
    "        y_lab (str): Label for the x-axis.\n",
    "        p_val (float): P-value for significance annotation.\n",
    "        save_fig (bool): Whether to save the figure.\n",
    "        folder (str): Folder to save the figure if save_fig is True.\n",
    "    \"\"\"\n",
    "    # Calculate sample sizes\n",
    "    n_shared = len(shared_values)\n",
    "    n_nonassembly = len(nonassembly_values)\n",
    "\n",
    "    y_labels = [f\"Shared\\n(n={n_shared})\", f\"Nonassembly\\n(n={n_nonassembly})\"]\n",
    "\n",
    "    # Data frame for easier plotting\n",
    "    data = pd.DataFrame({\n",
    "        \"Values\": np.concatenate([shared_values, nonassembly_values]),\n",
    "        \"Group\": [y_labels[0]] * n_shared + [y_labels[1]] * n_nonassembly\n",
    "    })\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Create the raincloud plot\n",
    "    ax = pt.RainCloud(\n",
    "        y=\"Values\",\n",
    "        x=\"Group\",\n",
    "        data=data,  \n",
    "        palette=[(.4, .6, .8, .5), 'grey'],\n",
    "        width_viol=0.3,  \n",
    "        alpha=0.8,  \n",
    "        move=0.25,\n",
    "        point_size = 6,  \n",
    "        orient=\"v\" \n",
    "    )\n",
    "\n",
    "    # Set markings for significance\n",
    "    pairs = [(y_labels[0], y_labels[1])]\n",
    "    annot = Annotator(ax, \n",
    "                    pairs,\n",
    "                    data=data,\n",
    "                    x=\"Group\",\n",
    "                    y=\"Values\",\n",
    "                    order=y_labels # Force the order\n",
    "                    )\n",
    "    annot.set_pvalues([p_val])\n",
    "    annot.configure(text_format=\"star\", loc=\"inside\", fontsize=30)\n",
    "    annot.annotate()\n",
    "\n",
    "    # Add plot title and labels\n",
    "    plt.title(title, size=30)\n",
    "    plt.xlabel(\"Connection Type\", size=24)\n",
    "    plt.ylabel(y_lab, size=24)\n",
    "    plt.xticks(fontsize = 22)\n",
    "    plt.yticks(fontsize = 22)\n",
    "\n",
    "    if save == True:\n",
    "        save_figure(figure_name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_shared_vs_nonassembly_with_side_plot(shared_values, nonassembly_values, title, \n",
    "                                           y_lab, p_val, save=False, figure_name=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a raincloud plot comparing connection types, \n",
    "    plus a smaller side subplot summarizing mean ± SEM for each group.\n",
    "\n",
    "    Parameters:\n",
    "        shared_values (list or array): Data for shared assembly group.\n",
    "        nonassembly_values (list or array): Data for nonassembly assembly group.\n",
    "        title (str): Title of the plot.\n",
    "        y_lab (str): Label for the x-axis.\n",
    "        p_val (float): P-value for significance annotation.\n",
    "        save_fig (bool): Whether to save the figure.\n",
    "        folder (str): Folder to save the figure if save_fig is True.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate sample sizes\n",
    "    n_shared = len(shared_values)\n",
    "    n_nonassembly = len(nonassembly_values)\n",
    "\n",
    "    y_labels = [f\"Shared\\n(n={n_shared})\", f\"Nonassembly\\n(n={n_nonassembly})\"]\n",
    "\n",
    "    # Build a frame for easier plotting\n",
    "    data = pd.DataFrame({\n",
    "        \"Values\": np.concatenate([shared_values, nonassembly_values]),\n",
    "        \"Group\": [y_labels[0]] * n_shared + [y_labels[1]] * n_nonassembly\n",
    "    })\n",
    "\n",
    "    # Compute the statistics for the side plot\n",
    "    # (Assuming values > 0 for simplicity; modify if needed.)\n",
    "    shared_log = np.log10(shared_values)\n",
    "    nonassembly_log = np.log10(nonassembly_values)\n",
    "\n",
    "    mean_shared_log = np.mean(shared_log)\n",
    "    mean_nonassembly_log = np.mean(nonassembly_log)\n",
    "    sem_shared_log = stats.sem(shared_log, ddof=1) if n_shared > 1 else 0\n",
    "    sem_nonassembly_log = stats.sem(nonassembly_log, ddof=1) if n_nonassembly > 1 else 0\n",
    "\n",
    "    # Set up a figure with two subplots\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    # Allocate 2 columns with a narrower column on the right\n",
    "    gs = fig.add_gridspec(nrows=1, ncols=2, width_ratios=[3, 1], wspace=0.3)\n",
    "    \n",
    "    # Set up styling\n",
    "    ax_main = fig.add_subplot(gs[0])\n",
    "    ax_side = fig.add_subplot(gs[1])\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # --- Main plot (original RainCloud) ---\n",
    "    pt.RainCloud(\n",
    "        y=\"Values\",\n",
    "        x=\"Group\",\n",
    "        data=data,\n",
    "        palette=[(.4, .6, .8, .5), 'grey'],\n",
    "        width_viol=0.3,\n",
    "        alpha=0.8,\n",
    "        move=0.25,\n",
    "        point_size=6,\n",
    "        orient=\"v\",\n",
    "        ax=ax_main\n",
    "    )\n",
    "\n",
    "    # Annotate significance\n",
    "    pairs = [(y_labels[0], y_labels[1])]\n",
    "    annot = Annotator(ax_main, \n",
    "                      pairs,\n",
    "                      data=data,\n",
    "                      x=\"Group\",\n",
    "                      y=\"Values\",\n",
    "                      order=y_labels # Force the order\n",
    "                      )\n",
    "    annot.set_pvalues([p_val])\n",
    "    annot.configure(text_format=\"star\", loc=\"inside\", fontsize=28)\n",
    "    annot.annotate()\n",
    "\n",
    "    # Axis title and labels\n",
    "    ax_main.set_title(title, size=24)\n",
    "    ax_main.set_xlabel(\"Connection Type\", size=20)\n",
    "    ax_main.set_ylabel(y_lab, size=20)\n",
    "    ax_main.tick_params(labelsize=20)\n",
    "\n",
    "    # --- Side plot (Mean ± SEM of log(data))---\n",
    "    # Currently place two horizontal lines and use fill_between for each ± sem region.\n",
    "\n",
    "    x_vals = [1, 2]  # x positions for shared and nonassembly\n",
    "    mean_logs = [mean_shared_log, mean_nonassembly_log]\n",
    "    sem_logs = [sem_shared_log, sem_nonassembly_log]\n",
    "    colors = [(0.4, 0.6, 0.8, 0.8), 'grey']\n",
    "\n",
    "    for i, x in enumerate(x_vals):\n",
    "        m_log = mean_logs[i]\n",
    "        s_log = sem_logs[i]\n",
    "        c = colors[i]\n",
    "\n",
    "        # Horizontal line for mean\n",
    "        ax_side.hlines(\n",
    "            y = m_log, \n",
    "            xmin = x - 0.15, \n",
    "            xmax = x + 0.15, \n",
    "            color = c, \n",
    "            linewidth = 3\n",
    "        )\n",
    "        # Shaded area for ± SEM\n",
    "        ax_side.fill_betweenx(\n",
    "            y = [m_log - s_log, m_log + s_log],\n",
    "            x1 = x - 0.15,\n",
    "            x2 = x + 0.15,\n",
    "            color = c,\n",
    "            alpha = 0.4\n",
    "        )\n",
    "\n",
    "    # Tidy up side axis\n",
    "    ax_side.set_title(\"Mean ± SEM (log-transformed)\", size=20)\n",
    "    ax_side.set_xlim(0.5, 2.5)  \n",
    "    ax_side.set_xticks(x_vals)\n",
    "    ax_side.set_xticklabels([\"Shared\", \"Nonassembly\"], fontsize=20)\n",
    "    ax_side.tick_params(axis='y', labelsize=20)\n",
    "    # Show that y-values are on a log base 10 scale\n",
    "    ax_side.set_ylabel(r\"$\\log_{10}(\\mathrm{\\mu m^3})$\", size=20)\n",
    "\n",
    "    if save and figure_name is not None:\n",
    "        save_figure(figure_name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def chi_squared_analysis(data, save=False, figure_name=None):\n",
    "    \"\"\"\n",
    "    Perform an overall chi-squared test of independence on a contingency table and display\n",
    "    observed and expected values as pretty tables with test results.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): A DataFrame representing the contingency table.\n",
    "\n",
    "    Returns:\n",
    "    None: Prints the tables and results directly.\n",
    "    \"\"\"\n",
    "    # Perform chi-squared test\n",
    "    chi2, p, dof, expected = chi2_contingency(data)\n",
    "    expected_df = pd.DataFrame(expected, index=data.index, columns=data.columns)\n",
    "\n",
    "    # Create pretty tables\n",
    "    observed_table = tabulate(\n",
    "        [[row] + list(data.loc[row]) for row in data.index],\n",
    "        headers=[\"Connection Type\"] + list(data.columns),\n",
    "        tablefmt=\"pretty\"\n",
    "    )\n",
    "    expected_table = tabulate(\n",
    "        [[row] + [f\"{val:.2f}\" for val in expected_df.loc[row]] for row in expected_df.index],\n",
    "        headers=[\"Connection Type\"] + list(expected_df.columns),\n",
    "        tablefmt=\"pretty\"\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Observed Contingency Table:\")\n",
    "    print(observed_table, \"\\n\")\n",
    "    print(\"Expected Contingency Table:\")\n",
    "    print(expected_table, \"\\n\")\n",
    "    print(\"Chi-squared Test Results:\")\n",
    "    print(f\"Chi-squared Statistic: {chi2:.2f}\")\n",
    "    print(f\"Degrees of Freedom: {dof}\")\n",
    "    print(f\"P-value: {p:.4g}\")\n",
    "\n",
    "    # Plot the heatmap with updated annotation and tick font sizes\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Create a custom uniform heatmap\n",
    "    ax = sns.heatmap(\n",
    "        data,\n",
    "        annot=True,               # Add annotations for the counts\n",
    "        fmt=\"d\",                  # Integer format for annotations\n",
    "        cmap=sns.color_palette([\"lightgrey\"], as_cmap=True),  # All cells the same light gray color\n",
    "        cbar=False,               # Remove the color bar\n",
    "        annot_kws={\"fontsize\": 22},  # Set font size for annotations\n",
    "        linewidths=2,             # Add grid lines\n",
    "        linecolor='black'         # Grid line color\n",
    "    )\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(f\"Probability of Connection\\nChi-squared P-value: {p:.4g}\", size=24)\n",
    "    plt.xlabel(\"Connection Status\", size=22)\n",
    "    plt.ylabel(\"Connection Type\", size=22)\n",
    "    plt.xticks(fontsize=22)\n",
    "    ax.set_yticklabels(data.index, rotation=90, va='center', fontsize=22)\n",
    "\n",
    "    if save==True:\n",
    "        save_figure(figure_name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def chi_squared_analysis_v2(data, save=False, figure_name=None):\n",
    "    \"\"\"\n",
    "    Perform an overall chi-squared test of independence on a contingency table and display\n",
    "    observed and expected values as pretty tables with test results. This version plots a \n",
    "    heatmap of the *cell-wise chi-square contributions* (rather than the raw counts), \n",
    "    to visualize which cells contribute most to the chi-square statistic.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): A DataFrame representing the contingency table constructed from the\n",
    "                         `construct_contingency_table` function.\n",
    "    save (bool): Whether to save the resulting plot.\n",
    "    figure_name (str or None): The filename to use if saving the plot.\n",
    "\n",
    "    Returns:\n",
    "    None: Prints the tables and results directly.\n",
    "    \"\"\"\n",
    "    # Perform chi-squared test\n",
    "    chi2, p, dof, expected = chi2_contingency(data)\n",
    "    expected_df = pd.DataFrame(expected, index=data.index, columns=data.columns)\n",
    "\n",
    "    # Create pretty tables\n",
    "    observed_table = tabulate(\n",
    "        [[row] + list(data.loc[row]) for row in data.index],\n",
    "        headers=[\"Connection Type\"] + list(data.columns),\n",
    "        tablefmt=\"pretty\"\n",
    "    )\n",
    "    expected_table = tabulate(\n",
    "        [[row] + [f\"{val:.2f}\" for val in expected_df.loc[row]] for row in expected_df.index],\n",
    "        headers=[\"Connection Type\"] + list(expected_df.columns),\n",
    "        tablefmt=\"pretty\"\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Observed Contingency Table:\")\n",
    "    print(observed_table, \"\\n\")\n",
    "    print(\"Expected Contingency Table:\")\n",
    "    print(expected_table, \"\\n\")\n",
    "    print(\"Chi-squared Test Results:\")\n",
    "    print(f\"Chi-squared Statistic: {chi2:.2f}\")\n",
    "    print(f\"Degrees of Freedom: {dof}\")\n",
    "    print(f\"P-value: {p:.4g}\")\n",
    "\n",
    "    # Calculate the cell-wise contributions\n",
    "    contributions = (data - expected_df) ** 2 / expected_df\n",
    "    contributions = contributions.fillna(0)  # Replace NaN with 0 for cells with no expected count\n",
    "\n",
    "    # Plot the heatmap with updated annotation and tick font sizes\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Create a custom uniform heatmap\n",
    "    ax = sns.heatmap(\n",
    "        contributions,\n",
    "        annot=True,               # Add annotations for the counts\n",
    "        fmt=\".4f\",                  # Integer format for annotations\n",
    "        cmap=sns.color_palette([\"lightgrey\"], as_cmap=True),  # All cells the same light gray color\n",
    "        cbar=False,               # Remove the color bar\n",
    "        annot_kws={\"fontsize\": 22},  # Set font size for annotations\n",
    "        linewidths=2,             # Add grid lines\n",
    "        linecolor='black'         # Grid line color\n",
    "    )\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(f\"Cell-wise Chi-Square Contributions\\nP-value: {p:.4g}\",size=24)\n",
    "    plt.xlabel(\"Connection Status\", size=22)\n",
    "    plt.ylabel(\"Connection Type\", size=22)\n",
    "    plt.xticks(fontsize=22)\n",
    "    ax.set_yticklabels(data.index, rotation=90, va='center', fontsize=22)\n",
    "\n",
    "\n",
    "    if save==True:\n",
    "        save_figure(figure_name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def construct_contingency_table(data_dict, groups):\n",
    "    # Generate lists for connected and not connected counts\n",
    "    connected_counts = [sum(1 for _, val in data_dict[group].items() if val == 1) for group in groups]\n",
    "    not_connected_counts = [sum(1 for _, val in data_dict[group].items() if val == 0) for group in groups]\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    return pd.DataFrame({\n",
    "        'Connected': connected_counts,\n",
    "        'Not Connected': not_connected_counts\n",
    "    }, index=[group.capitalize() for group in groups])\n",
    "\n",
    "def ranksum_signedrank_two_group_comparison(comparison_dict, aggregation_method=\"by connection\", directionality=None, data_type=\"binary\", \n",
    "                            paired=False, non_zero=False, chain_test = False, chain_description = \"Excitatory\", save=True, figure_name=None):\n",
    "    \"\"\"\n",
    "    Compares 'shared' and 'nonassembly' groups based on connection type and data type.\n",
    "    Uses a one-sided Wilcoxon rank-sum test and performs a Wilcoxon signed-rank test if paired=True.\n",
    "\n",
    "    Parameters:\n",
    "    - comparison_dict (dict): Dictionary with 'shared' and 'nonassembly' data.\n",
    "    - aggregation_method (str): Type of connection ('connection' for pairwise, 'cell' for inbound/outbound by cell).\n",
    "    - directionality (str): Direction of connectivity for 'cell' type ('inbound' or 'outbound').\n",
    "    - data_type (str): Data type ('binary' for connectivity, 'summed_psd' for nonzero PSD).\n",
    "    - paired (bool): If True, performs an additional Wilcoxon signed-rank test on paired data.\n",
    "    - non_zero (bool): If True, filters out zero entries for summed PSD.\n",
    "    - chain_test (bool): If True, the test is considering chains.\n",
    "    - chain_description (str): Type of intermediate cell in chain ('excitatory' or 'inhibitory')\n",
    "    \"\"\"\n",
    "\n",
    "    # Set title and labels based on connection_type and data_type\n",
    "    if aggregation_method == \"connection\":  # Pairwise connections\n",
    "        if data_type == \"binary\":\n",
    "            title = \"Binary Connectivity\"\n",
    "            y_lab = \"Binary Connections\"\n",
    "            folder = \"pairwise_binary_connectivity\"\n",
    "        elif data_type == \"summed_psd\":\n",
    "            if non_zero == True:\n",
    "                title = \"Nonzero Summed PSD\"\n",
    "                y_lab = \"Nonzero Summed PSD (\\u03bcm$^3$)\"\n",
    "                folder = \"pairwise_nonzero_summed_psd\"\n",
    "            else:\n",
    "                title = \"Summed PSD\"\n",
    "                y_lab = \"Summed PSD (\\u03bcm$^3$)\"\n",
    "                folder = \"pairwise_summed_psd\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid data_type for pairwise connection.\")\n",
    "\n",
    "    elif aggregation_method == \"cell\":  # By cell with inbound/outbound directionality\n",
    "        if directionality not in [\"inbound\", \"outbound\"]:\n",
    "            raise ValueError(\"For 'cell' connection_type, directionality must be 'inbound' or 'outbound'.\")\n",
    "        \n",
    "        if data_type == \"binary\":\n",
    "            title = f\"Probability of {directionality.capitalize()} Connection by Cell\"\n",
    "            y_lab = f\"Probability of {directionality.capitalize()} Connection\"\n",
    "            folder = f\"{directionality}_connection_probability\"\n",
    "        elif data_type == \"summed_psd\":\n",
    "            title = f\"Average Nonzero {directionality.capitalize()} PSD by Cell\"\n",
    "            y_lab = f\"Average Nonzero {directionality.capitalize()} PSD (\\u03bcm$^3$)\"\n",
    "            folder = f\"{directionality}_average_nonzero_psd\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid data_type for inbound/outbound connection.\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid connection_type. Must be 'connection' or 'cell'.\")\n",
    "\n",
    "    if chain_test:\n",
    "        title += f\" ({chain_description} Chains)\"\n",
    "        y_lab = y_lab.replace(\"PSD\", \"Chain PSD Products\")\n",
    "        \n",
    "    shared_values = np.array(list(comparison_dict[\"shared\"].values()))\n",
    "    nonassembly_values = np.array(list(comparison_dict[\"nonassembly\"].values()))\n",
    "    # Filter out zeros if non_zero is specified for summed_psd\n",
    "    if non_zero and data_type == \"summed_psd\":\n",
    "        shared_values = shared_values[shared_values != 0]\n",
    "        nonassembly_values = nonassembly_values[nonassembly_values != 0]\n",
    "\n",
    "    # Perform the Wilcoxon rank-sum test (one-sided, shared > nonassembly)\n",
    "    if chain_description=='Inhibitory':\n",
    "        rank_sum_stat, rank_sum_p = stats.ranksums(shared_values, nonassembly_values, alternative='less')\n",
    "        print(f\"Wilcoxon Rank-Sum Test (unpaired, shared < nonassembly):\\nStatistic: {rank_sum_stat:.4g}, P-value: {rank_sum_p:.4g}\")\n",
    "    else:\n",
    "        rank_sum_stat, rank_sum_p = stats.ranksums(shared_values, nonassembly_values, alternative='greater')\n",
    "        print(f\"Wilcoxon Rank-Sum Test (unpaired, shared > nonassembly):\\nStatistic: {rank_sum_stat:.4g}, P-value: {rank_sum_p:.4g}\")\n",
    "\n",
    "    title = f'{title}\\nRank-Sum P-value: {rank_sum_p:.4g}'\n",
    "\n",
    "    # If paired=True, also perform a Wilcoxon signed-rank test on paired observations\n",
    "    if paired:\n",
    "        shared_keys = set(comparison_dict.get('shared', {}).keys())\n",
    "        nonassembly_keys = set(comparison_dict.get('nonassembly', {}).keys())\n",
    "        common_keys = shared_keys & nonassembly_keys\n",
    "\n",
    "        if common_keys:\n",
    "            # Extract paired data for common keys\n",
    "            shared_paired = np.array([comparison_dict[\"shared\"][key] for key in common_keys])\n",
    "            nonassembly_paired = np.array([comparison_dict[\"nonassembly\"][key] for key in common_keys])\n",
    "\n",
    "            # Perform Wilcoxon signed-rank test on paired data\n",
    "            if chain_description=='Inhibitory':\n",
    "                signed_rank_stat, signed_rank_p = stats.wilcoxon(shared_paired, nonassembly_paired, alternative='less')\n",
    "                print(f\"Wilcoxon Signed-Rank Test (paired, shared < nonassembly):\\nStatistic: {signed_rank_stat:.4g}, P-value: {signed_rank_p:.4g}\")\n",
    "            else:\n",
    "                signed_rank_stat, signed_rank_p = stats.wilcoxon(shared_paired, nonassembly_paired, alternative='greater')\n",
    "                print(f\"Wilcoxon Signed-Rank Test (paired, shared > nonassembly):\\nStatistic: {signed_rank_stat:.4g}, P-value: {signed_rank_p:.4g}\")\n",
    "\n",
    "            title = f'{title}, Signed-Rank P-value: {signed_rank_p:.4g}'\n",
    "        else:\n",
    "            print(\"No common observations found for paired analysis.\")\n",
    "\n",
    "    plot_shared_vs_nonassembly(shared_values, nonassembly_values, title, y_lab, p_val = rank_sum_p, save=True, figure_name=figure_name)\n",
    "    plot_shared_vs_nonassembly_with_side_plot(shared_values, nonassembly_values, title, y_lab, p_val = rank_sum_p, save=True, figure_name=figure_name + \"_with_side_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pyr_cells_rectangular_connectome.json') as f:\n",
    "    lsmm_json_input = json.load(f)\n",
    "v1dd_data = LSMMData.LSMMData(lsmm_json_input)\n",
    "\n",
    "data_a = v1dd_data.data\n",
    "params_a = v1dd_data.params\n",
    "dirs_a = v1dd_data.dirs\n",
    "mappings_a = v1dd_data.mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pull necessary data from V1DD using LSMMData Manager\n",
    "cell_table = data_a['structural']['pre_cell'].copy()\n",
    "cell_table['connectome_index'] = cell_table.index\n",
    "post_cell_table = data_a['structural']['post_cell'].copy()\n",
    "post_cell_table['connectome_index'] = post_cell_table.index\n",
    "synapse_table = data_a['structural']['synapse']\n",
    "\n",
    "# Establish seperate sets for the pre and post synaptic partnes\n",
    "# This is necessary as the set of connectome index of pre-synaptic cells do not \n",
    "# match the post-synaptic cells due to allowing unproofread post-synaptic targets.\n",
    "individual_assembly_indexes = [mappings_a['connectome_indexes_by_assembly'][f'A {i}'] for i in range(1,16)]\n",
    "individual_post_assembly_indexes = [mappings_a['post_connectome_indexes_by_assembly'][f'A {i}'] for i in range(1,16)]\n",
    "\n",
    "coregistered_post_cell_indexes = mappings_a['assemblies_by_post_connectome_index'].keys()\n",
    "coregistered_cell_indexes = mappings_a['assemblies_by_connectome_index'].keys()\n",
    "\n",
    "no_a_cell_indexes = mappings_a['connectome_indexes_by_assembly']['No A']\n",
    "no_a_post_cell_indexes = mappings_a['post_connectome_indexes_by_assembly']['No A']\n",
    "\n",
    "pooled_assembly_indexes = list(set(coregistered_cell_indexes) - set(no_a_cell_indexes))\n",
    "pooled_assembly_post_indexes = list(set(coregistered_post_cell_indexes) - set(no_a_post_cell_indexes))\n",
    "# Each cell has a distinct root id, so it is unnecessary to establish different sets\n",
    "assembly_to_root_ids = mappings_a['pt_root_ids_by_assembly']\n",
    "assembly_root_ids_set = set(mappings_a['assemblies_by_pt_root_id'].keys())\n",
    "\n",
    "# Filter synapses_table to only synapses between two assembly cells (including No A)\n",
    "synapses_df = synapse_table[synapse_table['pre_pt_root_id'].isin(assembly_root_ids_set)]\n",
    "synapses_df = synapses_df[synapses_df['post_pt_root_id'].isin(assembly_root_ids_set)]\n",
    "synapses_df['size'] = synapses_df['size'] * (9 * 9 * 45) / (10**9) # Voxels -> Cubic micrometers\n",
    "\n",
    "# Filter cell tables to only assembly cells\n",
    "cell_table = cell_table[cell_table['pt_root_id'].isin(assembly_root_ids_set)]\n",
    "post_cell_table = post_cell_table[post_cell_table['pt_root_id'].isin(assembly_root_ids_set)]\n",
    "\n",
    "# Finalized set of Root IDs, which are \n",
    "pre_root_ids = set(cell_table['pt_root_id'].values)\n",
    "post_root_ids = set(post_cell_table['pt_root_id'].values)\n",
    "all_root_ids = pre_root_ids | post_root_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prep the sets for Analysis, following our description in the Methods section\n",
    "# Collect our connectomes of pre and post synaptic sets based on the root_ids of the neurons\n",
    "w = {}\n",
    "s = {}\n",
    "b = {}\n",
    "for pre in pre_root_ids:\n",
    "    for post in post_root_ids:\n",
    "        if pre != post:\n",
    "            w[(pre, post)] = 0\n",
    "            s[(pre, post)] = 0\n",
    "            b[(pre, post)] = 0\n",
    "\n",
    "for i, row in synapses_df.iterrows():\n",
    "    pre = row['pre_pt_root_id']\n",
    "    post = row['post_pt_root_id']\n",
    "    w[(pre, post)] += row['size']\n",
    "    s[(pre, post)] += 1\n",
    "    b[(pre, post)] = 1\n",
    "\n",
    "# Split out assemblies and no_a\n",
    "assembly_names = set(assembly_to_root_ids.keys()) - set(['No A'])\n",
    "A_invert = {assembly: set(assembly_to_root_ids[assembly]) for assembly in assembly_names}\n",
    "no_A = set(assembly_to_root_ids['No A'])\n",
    "all_coregistered_root_ids = mappings_a['assemblies_by_pt_root_id'].keys()\n",
    "assembly_root_ids_excluding_no_A = set(all_coregistered_root_ids) - no_A\n",
    "A = {pt_root_id: set(mappings_a['assemblies_by_pt_root_id'][pt_root_id]) for pt_root_id in all_root_ids if 'No A' not in mappings_a['assemblies_by_pt_root_id'][pt_root_id]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_nonzero_pairwise = {}\n",
    "B_pairwise = {}\n",
    "for connection_type in comparison_functions:\n",
    "    W_nonzero_pairwise[connection_type.__name__] = {}\n",
    "    B_pairwise[connection_type.__name__] = {}\n",
    "    for (j, i) in w.keys():\n",
    "        if connection_type(j, i, A):\n",
    "            B_pairwise[connection_type.__name__][(j, i)] = 1 if w[(j, i)] > 0 else 0\n",
    "            if w[(j, i)] > 0:\n",
    "                W_nonzero_pairwise[connection_type.__name__][(j, i)] = w[(j, i)]\n",
    "\n",
    "W_nonzero_out = {}\n",
    "for connection_type in comparison_functions:\n",
    "    W_nonzero_out[connection_type.__name__] = {}\n",
    "    for j in pre_root_ids:\n",
    "        if len([i for i in post_root_ids if i != j and connection_type(j, i, A) and w[(j, i)] > 0]) > 0:\n",
    "            W_nonzero_out[connection_type.__name__][j] = sum([w[(j, i)] for i in post_root_ids if connection_type(j, i, A) and j != i]) / len([i for i in post_root_ids if i != j and connection_type(j, i, A) and w[(j, i)] > 0])\n",
    "\n",
    "W_nonzero_in = {}\n",
    "for connection_type in comparison_functions:\n",
    "    W_nonzero_in[connection_type.__name__] = {}\n",
    "    for i in post_root_ids:\n",
    "        if len([j for j in pre_root_ids if connection_type(j, i, A)]) > 0 and len([j for j in pre_root_ids if j != i and connection_type(j, i, A) and w[(j,i)] > 0]):\n",
    "            W_nonzero_in[connection_type.__name__][i] = sum([w[(j, i)] for j in pre_root_ids if connection_type(j, i, A) and i != j]) / len([j for j in pre_root_ids if j != i and connection_type(j, i, A) and w[(j,i)] > 0])\n",
    "\n",
    "B_out = {}\n",
    "for connection_type in comparison_functions:\n",
    "    B_out[connection_type.__name__] = {}\n",
    "    for j in pre_root_ids:\n",
    "        if len([i for i in post_root_ids if connection_type(j, i, A) and j != i]) > 0:\n",
    "            B_out[connection_type.__name__][j] = sum([b[(j, i)] for i in post_root_ids if connection_type(j, i, A) and j != i]) / len([i for i in post_root_ids if connection_type(j, i, A) and j != i])\n",
    "\n",
    "B_in = {}\n",
    "for connection_type in comparison_functions:\n",
    "    B_in[connection_type.__name__] = {}\n",
    "    for i in post_root_ids:\n",
    "        if len([j for j in pre_root_ids if connection_type(j, i, A) and i != j]) > 0:\n",
    "            B_in[connection_type.__name__][i] = sum([b[(j, i)] for j in pre_root_ids if connection_type(j, i, A) and i != j]) / len([j for j in pre_root_ids if connection_type(j, i, A) and j != i])\n",
    "# create contingency table for monosynaptic connections count by connection type\n",
    "monosynaptic_pairwise_contingency_table = construct_contingency_table(B_pairwise, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all produced sets\n",
    "save_folder = 'master_freeze_produced_sets/monosynaptic_rectangular/rectangular_'\n",
    "with open(f\"{save_folder}W_nonzero_pairwise.pkl\", \"wb\") as f:\n",
    "    pickle.dump(W_nonzero_pairwise, f)\n",
    "with open(f\"{save_folder}W_nonzero_out.pkl\", \"wb\") as f:\n",
    "    pickle.dump(W_nonzero_out, f)\n",
    "with open(f\"{save_folder}W_nonzero_in.pkl\", \"wb\") as f:\n",
    "    pickle.dump(W_nonzero_in, f)\n",
    "with open(f\"{save_folder}B_out.pkl\", \"wb\") as f:\n",
    "    pickle.dump(W_nonzero_out, f)\n",
    "with open(f\"{save_folder}B_in.pkl\", \"wb\") as f:\n",
    "    pickle.dump(W_nonzero_in, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Monosynaptic Pairwise Connections by Connection Type Contingency Table:\")\n",
    "chi_squared_analysis(monosynaptic_pairwise_contingency_table, save=True, figure_name='Prob_Conn_by_Conn_Type')\n",
    "chi_squared_analysis_v2(monosynaptic_pairwise_contingency_table, save=True, figure_name='Prob_Conn_by_Conn_Type_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksum_signedrank_two_group_comparison(W_nonzero_pairwise,\n",
    "                                        aggregation_method='connection',\n",
    "                                        data_type='summed_psd',\n",
    "                                        non_zero=True,\n",
    "                                        save=True,\n",
    "                                        figure_name='Nonzero_PSD_by_Conn'\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksum_signedrank_two_group_comparison(B_out,\n",
    "                                        aggregation_method='cell',\n",
    "                                        directionality='outbound',\n",
    "                                        data_type='binary',\n",
    "                                        paired=True,\n",
    "                                        save=True,\n",
    "                                        figure_name='Prob_Outbound_Conn'\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksum_signedrank_two_group_comparison(W_nonzero_out,\n",
    "                                        aggregation_method='cell',\n",
    "                                        directionality='outbound',\n",
    "                                        data_type='summed_psd',\n",
    "                                        paired=True,\n",
    "                                        non_zero=True,\n",
    "                                        save=True,\n",
    "                                        figure_name = 'Avg_Nonzero_Outbound_PSD'\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksum_signedrank_two_group_comparison(B_in,\n",
    "                                        aggregation_method='cell',\n",
    "                                        directionality='inbound',\n",
    "                                        data_type='binary',\n",
    "                                        paired=True,\n",
    "                                        save=True,\n",
    "                                        figure_name='Prob_Inbound_Conn'\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksum_signedrank_two_group_comparison(W_nonzero_in,\n",
    "                                        aggregation_method='cell',\n",
    "                                        directionality='inbound',\n",
    "                                        data_type='summed_psd',\n",
    "                                        paired=True,\n",
    "                                        non_zero=True,\n",
    "                                        save=True,\n",
    "                                        figure_name='Avg_Nonzero_Inbound_PSD'\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-Order Connectivity Analysis: Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_centrality_plot(input_centrality_dict: dict,\n",
    "                                     just_pyramidal=False,\n",
    "                                     outdegree=False,\n",
    "                                     indegree=False, \n",
    "                                     closeness=False, \n",
    "                                     betweenness=False,\n",
    "                                     save=False,\n",
    "                                     figure_name=None):\n",
    "    \"\"\"\n",
    "    Produces a raincloud plot for centrality metrics.\n",
    "\n",
    "    Parameters:\n",
    "        input_centrality_dict (dict): Dictionary containing centrality values.\n",
    "        just_pyramidal (bool): Whether to filter to pyramidal cells only.\n",
    "        outdegree (bool): Whether to use outdegree centrality.\n",
    "        indegree (bool): Whether to use indegree centrality.\n",
    "        closeness (bool): Whether to use closeness centrality.\n",
    "        betweenness (bool): Whether to use betweenness centrality.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if outdegree and indegree:\n",
    "        raise ValueError(\"Must either be working with outdegree or indegree.\")\n",
    "    if closeness and betweenness:\n",
    "        raise ValueError(\"Must either be working with closeness or betweenness.\")\n",
    "    if (outdegree or indegree) and (closeness or betweenness):\n",
    "        raise ValueError(\"Must either be working with directionality (indegree/outdegree) or higher-order (betweenness/closeness).\")\n",
    "\n",
    "    suffix = \"of Co-Registered Cells\"\n",
    "\n",
    "    # Based on the connectome flags, set the correct y_label and plot title\n",
    "    if outdegree:\n",
    "        centrality_desc = \"Outdegree_Centrality\"\n",
    "        suffix = \"Outdegree Centrality \" + suffix\n",
    "        y_lab = \"Outdegree Centrality\"\n",
    "    elif indegree:\n",
    "        centrality_desc = \"Indegree_Centrality\"\n",
    "        suffix = \"Indegree Centrality \" + suffix\n",
    "        y_lab = \"Indegree Centrality\"\n",
    "    elif closeness: \n",
    "        centrality_desc = \"Closeness_Centrality\"\n",
    "        suffix = \"Closeness Centrality \" + suffix\n",
    "        y_lab = \"Closeness Centrality\"\n",
    "    elif betweenness:\n",
    "        centrality_desc = \"Betweenness_Centrality\"\n",
    "        suffix = \"Betweenness Centrality \" + suffix\n",
    "        y_lab = \"Betweenness Centrality\"\n",
    "    else:\n",
    "        raise ValueError(\"Must Specify Degree\")\n",
    "\n",
    "    centrality_dict = {}\n",
    "    for key in input_centrality_dict.keys():\n",
    "        centrality_dict[key] = np.array(input_centrality_dict[key])\n",
    "\n",
    "    all_arr = [centrality_dict['All A'], centrality_dict['No A']]\n",
    "    result = stats.ranksums(centrality_dict['All A'], centrality_dict['No A'], 'greater')\n",
    "\n",
    "    # Calculate sample sizes\n",
    "    n_all_a = len(centrality_dict['All A'])\n",
    "    n_no_a = len(centrality_dict['No A'])\n",
    "\n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Prepare data for raincloud plot\n",
    "    data = pd.DataFrame({\n",
    "        \"Values\": np.concatenate(all_arr),\n",
    "        \"Group\": [f\"Assembly\\n(n={n_all_a})\"] * len(centrality_dict['All A']) + \\\n",
    "                 [f\"Non-Assembly\\n(n={n_no_a})\"] * len(centrality_dict['No A'])\n",
    "    })\n",
    "\n",
    "    # Create the raincloud plot\n",
    "    ax = pt.RainCloud(\n",
    "        y=\"Values\",\n",
    "        x=\"Group\",\n",
    "        data=data,\n",
    "        palette=[(.4, .6, .8, .5), 'grey'],\n",
    "        width_viol=0.3,  # Adjust violin width\n",
    "        alpha=0.8,  # Transparency of the cloud\n",
    "        move=0.25,  # Adjust position of violins\n",
    "        point_size = 6,\n",
    "        orient=\"v\"  # Horizontal orientation\n",
    "    )\n",
    "\n",
    "    # Set markings for significance\n",
    "    y_labels = [f\"Assembly\\n(n={n_all_a})\", f\"Non-Assembly\\n(n={n_no_a})\"]\n",
    "    pairs = [(y_labels[0], y_labels[1])]\n",
    "    annot = Annotator(ax, \n",
    "                    pairs,\n",
    "                    data=data,\n",
    "                    x=\"Group\",\n",
    "                    y=\"Values\",\n",
    "                    order=y_labels # Force the order\n",
    "                    )\n",
    "    annot.set_pvalues([result.pvalue])\n",
    "    annot.configure(text_format=\"star\", loc=\"inside\", fontsize=30)\n",
    "    annot.annotate()\n",
    "\n",
    "    # Add a multiline title to include the p-value, add y_label\n",
    "    title = f'{suffix}\\nRank-Sum P-value: {result.pvalue:.4g}'\n",
    "    plt.title(title, size=32)\n",
    "    plt.ylabel(y_lab, size=28)\n",
    "    plt.xticks(fontsize=28)  # Adjust size of xticks\n",
    "    plt.yticks(fontsize=28)  # Adjust size of yticks\n",
    "    plt.xlabel(\"Assigned Assembly Status\", size=28)\n",
    "\n",
    "    if save == True:\n",
    "        save_figure(figure_name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Cells Proofread Connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull Data from LSMM Data\n",
    "with open('all_cells_proofread_connectome.json') as f:\n",
    "    lsmm_json_input = json.load(f)\n",
    "v1dd_data = LSMMData.LSMMData(lsmm_json_input)\n",
    "\n",
    "data_a = v1dd_data.data\n",
    "params_a = v1dd_data.params\n",
    "dirs_a = v1dd_data.dirs\n",
    "mappings_a = v1dd_data.mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Centrality Measurements\n",
    "binary_connectome = data_a['structural']['binary_connectome']\n",
    "all_to_all_graph = nx.from_numpy_array(binary_connectome, create_using=nx.DiGraph)\n",
    "\n",
    "indegree_centrality = nx.in_degree_centrality(all_to_all_graph)\n",
    "outdegree_centrality = nx.out_degree_centrality(all_to_all_graph)\n",
    "closeness_centrality = nx.closeness_centrality(all_to_all_graph, wf_improved = True)\n",
    "betweenness_centrality = nx.betweenness_centrality(all_to_all_graph, normalized= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_index_by_assemblies = mappings_a['connectome_indexes_by_assembly']\n",
    "assembly_connectome_indexes = np.unique(np.concatenate([val for key, val in connectome_index_by_assemblies.items() if key != 'No A']))\n",
    "no_assembly_connectome_indexes = np.array(list(connectome_index_by_assemblies['No A']))\n",
    "\n",
    "# Produce Grouped Counts for Inbound and Outbound Connections\n",
    "indegree_centrality_by_grouped_membership = {'No A': [], 'All A': []}\n",
    "outdegree_centrality_by_grouped_membership = {'No A': [], 'All A': []}\n",
    "closeness_centrality_by_grouped_membership = {'No A': [], 'All A': []}\n",
    "betweenness_centrality_by_grouped_membership = {'No A': [], 'All A': []}\n",
    "\n",
    "# Add to dictionaries to plot\n",
    "for assembly_cell_idx in assembly_connectome_indexes:\n",
    "    indegree_centrality_by_grouped_membership['All A'].append(indegree_centrality[assembly_cell_idx])\n",
    "    outdegree_centrality_by_grouped_membership['All A'].append(outdegree_centrality[assembly_cell_idx])\n",
    "    closeness_centrality_by_grouped_membership['All A'].append(closeness_centrality[assembly_cell_idx])\n",
    "    betweenness_centrality_by_grouped_membership['All A'].append(betweenness_centrality[assembly_cell_idx])\n",
    "for no_assembly_cell_idx in no_assembly_connectome_indexes:\n",
    "    indegree_centrality_by_grouped_membership['No A'].append(indegree_centrality[no_assembly_cell_idx])\n",
    "    outdegree_centrality_by_grouped_membership['No A'].append(outdegree_centrality[no_assembly_cell_idx])\n",
    "    closeness_centrality_by_grouped_membership['No A'].append(closeness_centrality[no_assembly_cell_idx])\n",
    "    betweenness_centrality_by_grouped_membership['No A'].append(betweenness_centrality[no_assembly_cell_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all produced sets\n",
    "save_folder = 'master_freeze_produced_sets/centrality/all_cell_connectome_'\n",
    "with open(f\"{save_folder}indegree_centrality.pkl\", \"wb\") as f:\n",
    "    pickle.dump(indegree_centrality_by_grouped_membership, f)\n",
    "with open(f\"{save_folder}outdegree_centrality.pkl\", \"wb\") as f:\n",
    "    pickle.dump(outdegree_centrality_by_grouped_membership, f)\n",
    "with open(f\"{save_folder}closeness_centrality.pkl\", \"wb\") as f:\n",
    "    pickle.dump(closeness_centrality_by_grouped_membership, f)\n",
    "with open(f\"{save_folder}betweenness_centrality.pkl\", \"wb\") as f:\n",
    "    pickle.dump(betweenness_centrality_by_grouped_membership, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_centrality_plot(outdegree_centrality_by_grouped_membership,\n",
    "                        outdegree = True, save=True, figure_name='Outdegree_Centrality_All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_centrality_plot(indegree_centrality_by_grouped_membership,\n",
    "                        indegree = True, save=True, figure_name='Indegree_Centrality_All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_centrality_plot(betweenness_centrality_by_grouped_membership,\n",
    "                        betweenness = True, save=True, figure_name='Betweenness_Centrality_All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_centrality_plot(closeness_centrality_by_grouped_membership,\n",
    "                        closeness = True, save=True, figure_name='Closeness_Centrality_All')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyramidal Cells Proofread Connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull Data from LSMM Data\n",
    "with open('pyr_cells_proofread_connectome.json') as f:\n",
    "    lsmm_json_input = json.load(f)\n",
    "v1dd_data = LSMMData.LSMMData(lsmm_json_input)\n",
    "\n",
    "data_a = v1dd_data.data\n",
    "params_a = v1dd_data.params\n",
    "dirs_a = v1dd_data.dirs\n",
    "mappings_a = v1dd_data.mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Centrality Measurements\n",
    "binary_connectome = data_a['structural']['binary_connectome']\n",
    "all_to_all_graph = nx.from_numpy_array(binary_connectome, create_using=nx.DiGraph)\n",
    "\n",
    "indegree_centrality = nx.in_degree_centrality(all_to_all_graph)\n",
    "outdegree_centrality = nx.out_degree_centrality(all_to_all_graph)\n",
    "closeness_centrality = nx.closeness_centrality(all_to_all_graph, wf_improved = True)\n",
    "betweenness_centrality = nx.betweenness_centrality(all_to_all_graph, normalized= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_index_by_assemblies = mappings_a['connectome_indexes_by_assembly']\n",
    "assembly_connectome_indexes = np.unique(np.concatenate([val for key, val in connectome_index_by_assemblies.items() if key != 'No A']))\n",
    "no_assembly_connectome_indexes = np.array(list(connectome_index_by_assemblies['No A']))\n",
    "\n",
    "# Produce Grouped Counts for Inbound and Outbound Connections\n",
    "indegree_centrality_by_grouped_membership = {'No A': [], 'All A': []}\n",
    "outdegree_centrality_by_grouped_membership = {'No A': [], 'All A': []}\n",
    "closeness_centrality_by_grouped_membership = {'No A': [], 'All A': []}\n",
    "betweenness_centrality_by_grouped_membership = {'No A': [], 'All A': []}\n",
    "\n",
    "# Add to dictionaries to plot\n",
    "for assembly_cell_idx in assembly_connectome_indexes:\n",
    "    indegree_centrality_by_grouped_membership['All A'].append(indegree_centrality[assembly_cell_idx])\n",
    "    outdegree_centrality_by_grouped_membership['All A'].append(outdegree_centrality[assembly_cell_idx])\n",
    "    closeness_centrality_by_grouped_membership['All A'].append(closeness_centrality[assembly_cell_idx])\n",
    "    betweenness_centrality_by_grouped_membership['All A'].append(betweenness_centrality[assembly_cell_idx])\n",
    "for no_assembly_cell_idx in no_assembly_connectome_indexes:\n",
    "    indegree_centrality_by_grouped_membership['No A'].append(indegree_centrality[no_assembly_cell_idx])\n",
    "    outdegree_centrality_by_grouped_membership['No A'].append(outdegree_centrality[no_assembly_cell_idx])\n",
    "    closeness_centrality_by_grouped_membership['No A'].append(closeness_centrality[no_assembly_cell_idx])\n",
    "    betweenness_centrality_by_grouped_membership['No A'].append(betweenness_centrality[no_assembly_cell_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all produced sets\n",
    "save_folder = 'master_freeze_produced_sets/centrality/pyr_only_connectome_'\n",
    "with open(f\"{save_folder}indegree_centrality.pkl\", \"wb\") as f:\n",
    "    pickle.dump(indegree_centrality_by_grouped_membership, f)\n",
    "with open(f\"{save_folder}outdegree_centrality.pkl\", \"wb\") as f:\n",
    "    pickle.dump(outdegree_centrality_by_grouped_membership, f)\n",
    "with open(f\"{save_folder}closeness_centrality.pkl\", \"wb\") as f:\n",
    "    pickle.dump(closeness_centrality_by_grouped_membership, f)\n",
    "with open(f\"{save_folder}betweenness_centrality.pkl\", \"wb\") as f:\n",
    "    pickle.dump(betweenness_centrality_by_grouped_membership, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_centrality_plot(outdegree_centrality_by_grouped_membership,\n",
    "                        outdegree = True,\n",
    "                        just_pyramidal = True, \n",
    "                        save=True,\n",
    "                        figure_name='Outdegree_Centrality_Pyr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_centrality_plot(indegree_centrality_by_grouped_membership,\n",
    "                        indegree = True,\n",
    "                        just_pyramidal = True, \n",
    "                        save=True,\n",
    "                        figure_name='Indegree_Centrality_Pyr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_centrality_plot(betweenness_centrality_by_grouped_membership,\n",
    "                        betweenness = True,\n",
    "                        just_pyramidal = True, \n",
    "                        save=True,\n",
    "                        figure_name='Betweenness_Centrality_Pyr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_centrality_plot(closeness_centrality_by_grouped_membership,\n",
    "                        closeness = True,\n",
    "                        just_pyramidal = True, \n",
    "                        save=True,\n",
    "                        figure_name='Closeness_Centrality_Pyr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-Order Conectivity Analysis: Chain Motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_cells_proofread_connectome.json') as f:\n",
    "    loaded_json = json.load(f)\n",
    "my_data = LSMMData.LSMMData(loaded_json)\n",
    "# with open('pyr_rect.data', 'wb') as f:\n",
    "#     pickle.dump(my_data, f)\n",
    "\n",
    "data_a = my_data.data\n",
    "params_a = my_data.params\n",
    "dirs_a = my_data.dirs\n",
    "mappings_a = my_data.mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a graph\n",
    "cell_table = data_a['structural']['pre_cell'].copy()\n",
    "cell_table['connectome_index'] = cell_table.index\n",
    "post_cell_table = data_a['structural']['post_cell'].copy()\n",
    "post_cell_table['connectome_index'] = post_cell_table.index\n",
    "\n",
    "synapse_table = data_a['structural']['synapse']\n",
    "# adjacency_matrix = data_a['structural']['binary_connectome']\n",
    "binary_connectome = data_a['structural']['binary_connectome']\n",
    "\n",
    "index_to_root_id = [mappings_a['connectome_index_to_root_id'][i] for i in range(binary_connectome.shape[0])]\n",
    "\n",
    "summed_size_connectome = data_a['structural']['summed_size_connectome']\n",
    "summed_size_connectome_df = pd.DataFrame(\n",
    "    summed_size_connectome,\n",
    "    index=index_to_root_id,\n",
    "    columns=index_to_root_id\n",
    ")\n",
    "\n",
    "pyr_graph = nx.from_numpy_array(binary_connectome, create_using=nx.DiGraph)\n",
    "pyr_graph = nx.relabel_nodes(pyr_graph, {i: index_to_root_id[i] for i in range(len(index_to_root_id))})\n",
    "\n",
    "# Or uncomment below to generate new motif analysis results (Can take quite a while on larger graphs)\n",
    "# Motif Analysis with DotMotif: 2 Chain, All Pyr\n",
    "executor = GrandIsoExecutor(graph= pyr_graph)\n",
    "chain_defs = Motif(\"\"\"\n",
    "                A -> B\n",
    "                B -> C\n",
    "              \"\"\")\n",
    "\n",
    "chain_results = executor.find(chain_defs)\n",
    "\n",
    "two_chain_results_array = np.array([list(c.values()) for c in tqdm(chain_results)])\n",
    "\n",
    "### Pool necessary Data\n",
    "chain_count_string_array = ['pyr_cell_2chain']\n",
    "individual_assembly_indexes = [mappings_a['connectome_indexes_by_assembly'][f'A {i}'] for i in range(1,16)]\n",
    "individual_post_assembly_indexes = [mappings_a['post_connectome_indexes_by_assembly'][f'A {i}'] for i in range(1,16)]\n",
    "\n",
    "coregistered_post_cell_indexes = mappings_a['assemblies_by_post_connectome_index'].keys()\n",
    "coregistered_cell_indexes = mappings_a['assemblies_by_connectome_index'].keys()\n",
    "no_a_cell_indexes = mappings_a['connectome_indexes_by_assembly']['No A']\n",
    "no_a_post_cell_indexes = mappings_a['post_connectome_indexes_by_assembly']['No A']\n",
    "pooled_assembly_indexes = list(set(coregistered_cell_indexes) - set(no_a_cell_indexes))\n",
    "pooled_assembly_post_indexes = list(set(coregistered_post_cell_indexes) - set(no_a_post_cell_indexes))\n",
    "\n",
    "assembly_to_root_ids = mappings_a['pt_root_ids_by_assembly']\n",
    "assembly_root_ids_set = set(mappings_a['assemblies_by_pt_root_id'].keys())\n",
    "\n",
    "# Filter synapses_table to only synapses between two assembly cells (including No A)\n",
    "synapses_df = synapse_table[synapse_table['pre_pt_root_id'].isin(assembly_root_ids_set)]\n",
    "synapses_df = synapses_df[synapses_df['post_pt_root_id'].isin(assembly_root_ids_set)]\n",
    "\n",
    "# Filter cell tables to only assembly cells (including 'No A')\n",
    "assembly_cell_table = cell_table[cell_table['pt_root_id'].isin(assembly_root_ids_set)]\n",
    "assembly_post_cell_table = post_cell_table[post_cell_table['pt_root_id'].isin(assembly_root_ids_set)]\n",
    "\n",
    "assembly_pre_root_ids = set(assembly_cell_table['pt_root_id'].values)\n",
    "assembly_post_root_ids = set(assembly_post_cell_table['pt_root_id'].values)\n",
    "all_root_ids = assembly_pre_root_ids | assembly_post_root_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store weights and binary connectivity\n",
    "W_chain_excitatory = {}\n",
    "W_chain_inhibitory = {}\n",
    "B_chain_excitatory = {}\n",
    "B_chain_inhibitory = {}\n",
    "\n",
    "# Define all potential (pre-cell, post-cell) pairs with excitatory and inhibitory chain types\n",
    "for j in all_root_ids:\n",
    "    for i in all_root_ids:\n",
    "        if j != i:  # Exclude autapses\n",
    "            W_chain_excitatory[(j, i)] = 0\n",
    "            W_chain_inhibitory[(j, i)] = 0\n",
    "            B_chain_excitatory[(j, i)] = 0\n",
    "            B_chain_inhibitory[(j, i)] = 0\n",
    "\n",
    "pt_root_id_to_classification = cell_table.set_index('pt_root_id')['classification_system'].to_dict()\n",
    "\n",
    "# Process each row in `two_chain_results_array` to populate weights and binary connectivity\n",
    "for _, row in enumerate(two_chain_results_array):\n",
    "    pre_cell, mid_cell, post_cell = row  # j: pre-cell, k: middle cell, i: post-cell\n",
    "    if pre_cell in all_root_ids and post_cell in all_root_ids:\n",
    "    # Determine chain type (excitatory if middle cell is in excitatory set, else inhibitory)\n",
    "        if pt_root_id_to_classification[mid_cell] == 'inhibitory':\n",
    "            W_chain = W_chain_inhibitory\n",
    "            B_chain = B_chain_inhibitory\n",
    "        elif pt_root_id_to_classification[mid_cell] == 'excitatory':\n",
    "            W_chain = W_chain_excitatory\n",
    "            B_chain = B_chain_excitatory\n",
    "    # Get synapse weights for connections j -> k and k -> i\n",
    "        w_jk = summed_size_connectome_df.loc[pre_cell, mid_cell] * (9 * 9 * 45) / (10**9) # cubic micrometers\n",
    "        w_ki = summed_size_connectome_df.loc[mid_cell, post_cell] * (9 * 9 * 45) / (10**9) # cubic micrometers\n",
    "    \n",
    "    # Updates weights and binary connectivity\n",
    "        W_chain[(pre_cell, post_cell)] += (w_jk * w_ki)\n",
    "        B_chain[(pre_cell, post_cell)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PARIWISE NONZERO PSD and CONNECTION PROBABILITY BY CONNECTION TYPE\n",
    "# Aggregate nonzero pairs and calculate connectivity probabilities by connection type\n",
    "W_chain_nonzero_pairwise_excitatory = {}\n",
    "W_chain_nonzero_pairwise_inhibitory = {}\n",
    "B_chain_pairwise_excitatory = {}\n",
    "B_chain_pairwise_inhibitory = {}\n",
    "\n",
    "for cond_function in comparison_functions:\n",
    "    # Initialize dictionaries per connection type\n",
    "    W_chain_nonzero_pairwise_excitatory[cond_function.__name__] = {}\n",
    "    W_chain_nonzero_pairwise_inhibitory[cond_function.__name__] = {}\n",
    "    B_chain_pairwise_excitatory[cond_function.__name__] = {}\n",
    "    B_chain_pairwise_inhibitory[cond_function.__name__] = {}\n",
    "\n",
    "    # Process all (j, i) pairs from excitatory/inhibitory dictionaries\n",
    "    for (j, i) in W_chain_excitatory.keys():\n",
    "        if cond_function(j, i, A):\n",
    "            # Set binary connectivity for each connection type\n",
    "            B_chain_pairwise_excitatory[cond_function.__name__][(j, i)] = 1 if W_chain_excitatory[(j, i)] > 0 else 0\n",
    "            B_chain_pairwise_inhibitory[cond_function.__name__][(j, i)] = 1 if W_chain_inhibitory[(j, i)] > 0 else 0\n",
    "            \n",
    "            # Store only nonzero weights\n",
    "            if W_chain_excitatory[(j, i)] > 0:\n",
    "                W_chain_nonzero_pairwise_excitatory[cond_function.__name__][(j, i)] = W_chain_excitatory[(j, i)]\n",
    "            if W_chain_inhibitory[(j, i)] > 0:\n",
    "                W_chain_nonzero_pairwise_inhibitory[cond_function.__name__][(j, i)] = W_chain_inhibitory[(j, i)]\n",
    "\n",
    "\n",
    "## ADD IN NORMALIZATION BY # POTENTIAL CONNECTIONS\n",
    "## Inbound, Outbound Collections\n",
    "from collections import Counter\n",
    "\n",
    "classification_map = cell_table.set_index('connectome_index')['classification_system'].to_dict() # Map to tell us if its Excitatory or Inhibitory \n",
    "backup_assembly_pre_root_ids = assembly_pre_root_ids\n",
    "backup_assembly_post_root_ids = assembly_post_root_ids\n",
    "classification_counts = Counter(classification_map.values())\n",
    "\n",
    "num_excitatory = classification_counts.get('excitatory', 0)\n",
    "num_inhibitory = classification_counts.get('inhibitory', 0)\n",
    "# Aggregate chain weights and connectivity for outbound and inbound paths\n",
    "W_nonzero_chain_out_excitatory = {}\n",
    "W_nonzero_chain_out_inhibitory = {}\n",
    "W_nonzero_chain_in_excitatory = {}\n",
    "W_nonzero_chain_in_inhibitory = {}\n",
    "B_chain_out_excitatory = {}\n",
    "B_chain_out_inhibitory = {}\n",
    "B_chain_in_excitatory = {}\n",
    "B_chain_in_inhibitory = {}\n",
    "\n",
    "# Iterate through connection types in C (e.g., shared, nonassembly)\n",
    "for connection_type in comparison_functions:\n",
    "    # Initialize per connection type dictionaries\n",
    "    W_nonzero_chain_out_excitatory[connection_type.__name__] = {}\n",
    "    W_nonzero_chain_out_inhibitory[connection_type.__name__] = {}\n",
    "    W_nonzero_chain_in_excitatory[connection_type.__name__] = {}\n",
    "    W_nonzero_chain_in_inhibitory[connection_type.__name__] = {}\n",
    "    B_chain_out_excitatory[connection_type.__name__] = {}\n",
    "    B_chain_out_inhibitory[connection_type.__name__] = {}\n",
    "    B_chain_in_excitatory[connection_type.__name__] = {}\n",
    "    B_chain_in_inhibitory[connection_type.__name__] = {}\n",
    "    # Outbound analysis (from pre-cell j to post-cell i through middle cell k)\n",
    "    for j in all_root_ids:\n",
    "        potential_partners_out = {i for i in all_root_ids if i != j and connection_type(j, i, A)}\n",
    "        if potential_partners_out:  # Only proceed if valid post-cell partners exist\n",
    "            realized_chains_count_excitatory = 0\n",
    "            realized_chains_count_inhibitory = 0\n",
    "            sum_weights_excitatory = 0\n",
    "            sum_weights_inhibitory = 0\n",
    "            potential_excitatory_chains = (num_excitatory - 2) * (len(potential_partners_out)) # maybe middle cell is in set of coregistered that satisfies condition\n",
    "            potential_inhibitory_chains = num_inhibitory * len(potential_partners_out)\n",
    "            for i in potential_partners_out:\n",
    "                temporary_chain_results = two_chain_results_array[(two_chain_results_array[:, 0] == j) & (two_chain_results_array[:, 2] == i)]\n",
    "\n",
    "                for chain in temporary_chain_results:\n",
    "                    j, k, i = chain\n",
    "                    w_jk = summed_size_connectome_df.loc[j, k]\n",
    "                    w_ki = summed_size_connectome_df.loc[k, i]\n",
    "                    # Check chain type and accumulate only when both segments have valid weights\n",
    "                    if pt_root_id_to_classification.get(k) == 'excitatory':\n",
    "                        sum_weights_excitatory += w_jk * w_ki\n",
    "                        realized_chains_count_excitatory += 1  # Increase excitatory chain count\n",
    "                    elif pt_root_id_to_classification.get(k) == 'inhibitory':\n",
    "                        sum_weights_inhibitory += w_jk * w_ki\n",
    "                        realized_chains_count_inhibitory += 1  # Increase inhibitory chain count\n",
    "\n",
    "            # Normalize weights by realized chain count only if nonzero\n",
    "            if realized_chains_count_excitatory > 0:\n",
    "                W_nonzero_chain_out_excitatory[connection_type.__name__][j] = sum_weights_excitatory / realized_chains_count_excitatory\n",
    "            if realized_chains_count_inhibitory > 0:\n",
    "                W_nonzero_chain_out_inhibitory[connection_type.__name__][j] = sum_weights_inhibitory / realized_chains_count_inhibitory\n",
    "\n",
    "            # Calculate binary connectivity only if there are potential partners\n",
    "            B_chain_out_excitatory[connection_type.__name__][j] = realized_chains_count_excitatory / potential_excitatory_chains if len(potential_partners_out) > 0 else 0\n",
    "            B_chain_out_inhibitory[connection_type.__name__][j] = realized_chains_count_inhibitory / potential_inhibitory_chains if len(potential_partners_out) > 0 else 0\n",
    "    \n",
    "    for i in all_root_ids:\n",
    "        potential_partners_in = {j for j in all_root_ids if i != j and connection_type(j, i, A)}\n",
    "        if potential_partners_in:  # Only proceed if valid post-cell partners exist\n",
    "            realized_chains_count_excitatory = 0\n",
    "            realized_chains_count_inhibitory = 0\n",
    "            sum_weights_excitatory = 0\n",
    "            sum_weights_inhibitory = 0\n",
    "            potential_excitatory_chains = (num_excitatory - 2) * (len(potential_partners_in)) # maybe middle cell is in set of coregistered that satisfies condition\n",
    "            potential_inhibitory_chains = num_inhibitory * len(potential_partners_in)\n",
    "            for j in potential_partners_in:\n",
    "                temporary_chain_results = two_chain_results_array[(two_chain_results_array[:, 0] == j) & (two_chain_results_array[:, 2] == i)]\n",
    "\n",
    "                for chain in temporary_chain_results:\n",
    "                    j, k, i = chain\n",
    "                    w_jk = summed_size_connectome_df.loc[j, k]\n",
    "                    w_ki = summed_size_connectome_df.loc[k, i]\n",
    "                    # Check chain type and accumulate only when both segments have valid weights\n",
    "                    if pt_root_id_to_classification.get(k) == 'excitatory':\n",
    "                        sum_weights_excitatory += w_jk * w_ki\n",
    "                        realized_chains_count_excitatory += 1  # Increase excitatory chain count\n",
    "                    elif pt_root_id_to_classification.get(k) == 'inhibitory':\n",
    "                        sum_weights_inhibitory += w_jk * w_ki\n",
    "                        realized_chains_count_inhibitory += 1  # Increase inhibitory chain count\n",
    "\n",
    "            # Normalize weights by realized chain count only if nonzero\n",
    "            if realized_chains_count_excitatory > 0:\n",
    "                W_nonzero_chain_in_excitatory[connection_type.__name__][i] = sum_weights_excitatory / realized_chains_count_excitatory\n",
    "            if realized_chains_count_inhibitory > 0:\n",
    "                W_nonzero_chain_in_inhibitory[connection_type.__name__][i] = sum_weights_inhibitory / realized_chains_count_inhibitory\n",
    "\n",
    "            # Calculate binary connectivity only if there are potential partners\n",
    "            B_chain_in_excitatory[connection_type.__name__][i] = realized_chains_count_excitatory / potential_excitatory_chains if len(potential_partners_in) > 0 else 0\n",
    "            B_chain_in_inhibitory[connection_type.__name__][i] = realized_chains_count_inhibitory / potential_inhibitory_chains if len(potential_partners_in) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all produced sets\n",
    "save_folder = 'master_freeze_produced_sets/chain_connections/'\n",
    "with open(f\"{save_folder}W_chain_nonzero_pairwise_excitatory.pkl\", \"wb\") as f:\n",
    "    pickle.dump(W_chain_nonzero_pairwise_excitatory, f)\n",
    "with open(f\"{save_folder}W_chain_nonzero_pairwise_inhibitory.pkl\", \"wb\") as f:\n",
    "    pickle.dump(W_chain_nonzero_pairwise_inhibitory, f)\n",
    "with open(f\"{save_folder}B_chain_pairwise_excitatory.pkl\", \"wb\") as f:\n",
    "    pickle.dump(B_chain_pairwise_excitatory, f)\n",
    "with open(f\"{save_folder}B_chain_pairwise_inhibitory.pkl\", \"wb\") as f:\n",
    "    pickle.dump(B_chain_pairwise_inhibitory, f)\n",
    "with open(f\"{save_folder}W_nonzero_chain_out_excitatory.pkl\", \"wb\") as f:\n",
    "    pickle.dump(W_nonzero_chain_out_excitatory, f)\n",
    "with open(f\"{save_folder}W_nonzero_chain_out_inhibitory.pkl\", \"wb\") as f:\n",
    "    pickle.dump(W_nonzero_chain_out_inhibitory, f)\n",
    "with open(f\"{save_folder}W_nonzero_chain_in_excitatory.pkl\", \"wb\") as f:\n",
    "    pickle.dump(W_nonzero_chain_in_excitatory, f)\n",
    "with open(f\"{save_folder}W_nonzero_chain_in_inhibitory.pkl\", \"wb\") as f:\n",
    "    pickle.dump(W_nonzero_chain_in_inhibitory, f)\n",
    "with open(f\"{save_folder}B_chain_out_excitatory.pkl\", \"wb\") as f:\n",
    "    pickle.dump(B_chain_out_excitatory, f)\n",
    "with open(f\"{save_folder}B_chain_out_inhibitory.pkl\", \"wb\") as f:\n",
    "    pickle.dump(B_chain_out_inhibitory, f)\n",
    "with open(f\"{save_folder}B_chain_in_excitatory.pkl\", \"wb\") as f:\n",
    "    pickle.dump(B_chain_in_excitatory, f)\n",
    "with open(f\"{save_folder}B_chain_in_inhibitory.pkl\", \"wb\") as f:\n",
    "    pickle.dump(B_chain_in_inhibitory, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excitatory_contingency_table = construct_contingency_table(B_chain_pairwise_excitatory, groups)\n",
    "inhibitory_contingency_table = construct_contingency_table(B_chain_pairwise_inhibitory, groups)\n",
    "\n",
    "print(\"Excitatory Chain Contingency Table:\")\n",
    "chi_squared_analysis(excitatory_contingency_table, save=True, figure_name='Prob_Conn_by_Conn_Type_E_Chains')\n",
    "chi_squared_analysis_v2(excitatory_contingency_table, save=True, figure_name='Prob_Conn_by_Conn_Type_E_Chains_v2')\n",
    "\n",
    "print(\"\\nInhibitory Chain Contingency Table:\")\n",
    "chi_squared_analysis(inhibitory_contingency_table, save=True, figure_name='Prob_Conn_by_Conn_Type_I_Chains')\n",
    "chi_squared_analysis_v2(inhibitory_contingency_table, save=True, figure_name='Prob_Conn_by_Conn_Type_I_Chains_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksum_signedrank_two_group_comparison(W_chain_nonzero_pairwise_excitatory,\n",
    "                                        aggregation_method='connection',\n",
    "                                        data_type='summed_psd',\n",
    "                                        non_zero=True,\n",
    "                                        chain_test=True,\n",
    "                                        chain_description= \"Excitatory\",\n",
    "                                        save=True,\n",
    "                                        figure_name='Nonzero_PSD_by_Conn_E_Chain'\n",
    "                                        )\n",
    "\n",
    "ranksum_signedrank_two_group_comparison(W_chain_nonzero_pairwise_inhibitory,\n",
    "                                        aggregation_method='connection',\n",
    "                                        data_type='summed_psd',\n",
    "                                        non_zero=True,\n",
    "                                        chain_test=True,\n",
    "                                        chain_description= \"Inhibitory\",\n",
    "                                        save=True,\n",
    "                                        figure_name='Nonzero_PSD_by_Conn_I_Chain'\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksum_signedrank_two_group_comparison(B_chain_out_excitatory,\n",
    "                                        aggregation_method='cell',\n",
    "                                        directionality='outbound',\n",
    "                                        data_type='binary',\n",
    "                                        paired=True,\n",
    "                                        chain_test=True,\n",
    "                                        chain_description= \"Excitatory\",\n",
    "                                        save=True,\n",
    "                                        figure_name='Prob_Outbound_Conn_E_Chain'\n",
    "                                        )\n",
    "\n",
    "ranksum_signedrank_two_group_comparison(B_chain_out_inhibitory,\n",
    "                                        aggregation_method='cell',\n",
    "                                        directionality='outbound',\n",
    "                                        data_type='binary',\n",
    "                                        paired=True,\n",
    "                                        chain_test=True,\n",
    "                                        chain_description= \"Inhibitory\",\n",
    "                                        save=True,\n",
    "                                        figure_name='Prob_Outbound_Conn_I_Chain'\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksum_signedrank_two_group_comparison(W_nonzero_chain_out_excitatory,\n",
    "                                        aggregation_method='cell',\n",
    "                                        directionality='outbound',\n",
    "                                        data_type='summed_psd',\n",
    "                                        non_zero=True,\n",
    "                                        paired=True,\n",
    "                                        chain_test=True,\n",
    "                                        chain_description= \"Excitatory\",\n",
    "                                        save=True,\n",
    "                                        figure_name='Avg_Nonzero_Outbound_PSD_E_Chain'\n",
    "                                        )\n",
    "\n",
    "ranksum_signedrank_two_group_comparison(W_nonzero_chain_out_inhibitory,\n",
    "                                        aggregation_method='cell',\n",
    "                                        directionality='outbound',\n",
    "                                        data_type='summed_psd',\n",
    "                                        non_zero=True,\n",
    "                                        paired=True,\n",
    "                                        chain_test=True,\n",
    "                                        chain_description= \"Inhibitory\",\n",
    "                                        save=True,\n",
    "                                        figure_name='Avg_Nonzero_Outbound_PSD_I_Chain'\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksum_signedrank_two_group_comparison(B_chain_in_excitatory,\n",
    "                                        aggregation_method='cell',\n",
    "                                        directionality='inbound',\n",
    "                                        data_type='binary',\n",
    "                                        paired=True,\n",
    "                                        chain_test=True,\n",
    "                                        chain_description= \"Excitatory\",\n",
    "                                        save=True,\n",
    "                                        figure_name='Prob_Inbound_Conn_E_Chain'\n",
    "                                        )\n",
    "\n",
    "ranksum_signedrank_two_group_comparison(B_chain_in_inhibitory,\n",
    "                                        aggregation_method='cell',\n",
    "                                        directionality='inbound',\n",
    "                                        data_type='binary',\n",
    "                                        paired=True,\n",
    "                                        chain_test=True,\n",
    "                                        chain_description= \"Inhibitory\",\n",
    "                                        save=True,\n",
    "                                        figure_name='Prob_Inbound_Conn_I_Chain'\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksum_signedrank_two_group_comparison(W_nonzero_chain_in_excitatory,\n",
    "                                        aggregation_method='cell',\n",
    "                                        directionality='inbound',\n",
    "                                        data_type='summed_psd',\n",
    "                                        non_zero=True,\n",
    "                                        paired=True,\n",
    "                                        chain_test=True,\n",
    "                                        chain_description= \"Excitatory\",\n",
    "                                        save=True,\n",
    "                                        figure_name='Avg_Nonzero_Inbound_PSD_E_Chain'\n",
    "                                        )\n",
    "\n",
    "ranksum_signedrank_two_group_comparison(W_nonzero_chain_in_inhibitory,\n",
    "                                        aggregation_method='cell',\n",
    "                                        directionality='inbound',\n",
    "                                        data_type='summed_psd',\n",
    "                                        non_zero=True,\n",
    "                                        paired=True,\n",
    "                                        chain_test=True,\n",
    "                                        chain_description= \"Inhibitory\",\n",
    "                                        save=True,\n",
    "                                        figure_name='Avg_Nonzero_Inbound_PSD_I_Chain'\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tail Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_standard_em(x, K, seed=747, weights_init=None, means_init=None, precisions_init=None):\n",
    "    \"\"\"\n",
    "    Estimate GMM's parameters by using the standard EM algorithm, with k-means clustering initialization.\n",
    "\n",
    "    Args:\n",
    "        x (1D numpy array): The observed data.\n",
    "        K (int): The number of mixture components.\n",
    "        seed (int): The random seed.\n",
    "        weights_init (array): Optional initial weights for GMM components.\n",
    "        means_init (array): Optional initial means for GMM components.\n",
    "        precisions_init (array): Optional initial precisions for GMM components.\n",
    "\n",
    "    Returns:\n",
    "        results (dict): A dictionary containing estimated parameters (weights, means, std deviations).\n",
    "    \"\"\"\n",
    "    # Convert input to DataFrame for compatibility with GaussianMixture\n",
    "    x = pd.DataFrame(x)\n",
    "    \n",
    "    # Fit Gaussian Mixture Model\n",
    "    model = mixture.GaussianMixture(\n",
    "        n_components=K,\n",
    "        random_state=seed,\n",
    "        covariance_type='diag',\n",
    "        weights_init=weights_init,\n",
    "        means_init=means_init,\n",
    "        precisions_init=precisions_init\n",
    "    )\n",
    "    model.fit(x)\n",
    "\n",
    "    # Extract parameters and flatten\n",
    "    weights = model.weights_.tolist()  # Flatten weights\n",
    "    means = model.means_.flatten().tolist()  # Flatten means\n",
    "    std = np.sqrt(model.covariances_.flatten()).tolist()  # Flatten std deviations\n",
    "\n",
    "    results = {\n",
    "        'pp': weights,\n",
    "        'mu': means,\n",
    "        'std': std\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "def gmm_pdf_cdf(x, weights, means, std_devs):\n",
    "    \"\"\"\n",
    "    Compute the PDF and CDF for a Gaussian Mixture Model (GMM).\n",
    "\n",
    "    Parameters:\n",
    "        x (np.ndarray): Points at which to evaluate the PDF and CDF.\n",
    "        means (list): Means of the Gaussian components.\n",
    "        stds (list): Standard deviations of the Gaussian components.\n",
    "        weights (list): Weights of the Gaussian components.\n",
    "\n",
    "    Returns:\n",
    "        pdf (np.ndarray): PDF values for the GMM.\n",
    "        cdf (np.ndarray): CDF values for the GMM.\n",
    "    \"\"\"\n",
    "    pdf = np.zeros_like(x)\n",
    "    cdf = np.zeros_like(x)\n",
    "    for weight, mean, std_dev in zip(weights, means, std_devs):\n",
    "        pdf += weight * norm.pdf(x, mean, std_dev)\n",
    "        cdf += weight * norm.cdf(x, mean, std_dev)\n",
    "    return pdf, cdf\n",
    "\n",
    "def gmm_ppf(q, x, weights, means, std_devs):\n",
    "    \"\"\"\n",
    "    Compute the PPF (percent-point function) for a Gaussian Mixture Model (GMM).\n",
    "\n",
    "    Parameters:\n",
    "        q (np.ndarray): Quantiles at which to compute the PPF.\n",
    "        x (np.ndarray): Points used to evaluate the CDF.\n",
    "        means (list): Means of the Gaussian components.\n",
    "        stds (list): Standard deviations of the Gaussian components.\n",
    "        weights (list): Weights of the Gaussian components.\n",
    "\n",
    "    Returns:\n",
    "        ppf (np.ndarray): PPF values for the GMM.\n",
    "    \"\"\"\n",
    "    cdf_vals = gmm_pdf_cdf(x, weights, means, std_devs)[1]\n",
    "    cdf_func = interp1d(cdf_vals, x, bounds_error=False, fill_value=(x[0], x[-1]))\n",
    "    return cdf_func(q)\n",
    "\n",
    "def hist_with_GMM_fit_plus_qqplot_with_decision_boundary(PSD, means, std_devs, weights):\n",
    "    \"\"\"\n",
    "    Plots a histogram of the provided data along with a Gaussian Mixture Model (GMM) fit,\n",
    "    includes the decision boundary, and generates a QQ plot comparing empirical and theoretical quantiles.\n",
    "\n",
    "    Parameters:\n",
    "    PSD (np.ndarray): An array of data values (e.g., log10(PSD) values) to be plotted and fitted with the GMM.\n",
    "    means (np.ndarray): The means of the Gaussian components in the GMM.\n",
    "    std_devs(np.ndarray): The standard deviations of the Gaussian components in the GMM.\n",
    "    weights(np.ndarray): The weights of the Gaussian components in the GMM.\n",
    "\n",
    "    \"\"\"\n",
    "    # Calculate the decision boundary (quadratic formula for intersection)\n",
    "    a = (1 / (2 * std_devs[0]**2)) - (1 / (2 * std_devs[1]**2))\n",
    "    b = -(means[0] / (std_devs[0]**2)) + (means[1] / (std_devs[1]**2))\n",
    "    c = ((means[0]**2) / (2 * std_devs[0]**2)) - ((means[1]**2) / (2 * std_devs[1]**2)) - \\\n",
    "        np.log(std_devs[1] / std_devs[0]) - np.log(weights[1] / weights[0])\n",
    "    roots = np.roots([a, b, c])\n",
    "    decision_boundary = min(roots)  # Take the smaller root as the decision boundary\n",
    "    print('Decision Boundary: ', decision_boundary)\n",
    "    # Initialize plots\n",
    "    fig, (hist, qq) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Get x range\n",
    "    xmin, xmax = np.min(PSD), np.max(PSD)\n",
    "    x_range = np.linspace(xmin, xmax, 500)\n",
    "\n",
    "    # Fit GMM\n",
    "    pdf, cdf = gmm_pdf_cdf(x_range, weights, means, std_devs)\n",
    "\n",
    "    # Histogram with GMM fit overlaid\n",
    "    hist.hist(PSD, bins='rice', density=True, alpha=0.5, color='purple', label=\"Empirical Data\")\n",
    "    hist.plot(x_range, pdf, color=\"gold\", alpha=0.7, label=\"GMM Fit\")\n",
    "    hist.axvline(decision_boundary, color=\"red\", linestyle=\"--\", label=f\"Decision Boundary ({decision_boundary:.4g})\")\n",
    "    hist.set_title(\"Histogram of log10(PSD) with GMM Fit and Decision Boundary\")\n",
    "    hist.set_xlabel(\"Log10(PSD)\")\n",
    "    hist.set_ylabel(\"Density\")\n",
    "    hist.legend()\n",
    "\n",
    "    # Generate Quantiles of Empirical, Theoretical Distributions\n",
    "    empirical_quantiles = np.percentile(PSD, np.linspace(0, 100, len(PSD)))\n",
    "    theoretical_quantiles = gmm_ppf(np.linspace(0, 1, len(PSD)), x_range, weights, means, std_devs)\n",
    "\n",
    "    # QQ Plot\n",
    "    qq.plot(theoretical_quantiles, empirical_quantiles, 'o', color=\"purple\", markersize=3, label='Quantiles')\n",
    "    qq.plot([min(theoretical_quantiles), max(theoretical_quantiles)], \n",
    "             [min(theoretical_quantiles), max(theoretical_quantiles)], 'g--', label='Y=X')\n",
    "    qq.set_xlabel('Theoretical Quantiles')\n",
    "    qq.set_ylabel('Empirical Quantiles')\n",
    "    qq.set_title('QQ Plot')\n",
    "    qq.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return decision_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connections into df\n",
    "connection_data = []\n",
    "\n",
    "for connection_type, connections in W_nonzero_pairwise.items():\n",
    "    for (pre, post), size in connections.items():\n",
    "        connection_data.append({\n",
    "            \"pre\": pre,\n",
    "            \"post\": post,\n",
    "            \"size\": size,\n",
    "            \"connection_type\": connection_type\n",
    "        })\n",
    "\n",
    "connections_df = pd.DataFrame(connection_data)\n",
    "\n",
    "connections_df['size'] = connections_df['size']\n",
    "connections_df['log_size'] = np.log10(connections_df['size'])\n",
    "\n",
    "# expectation-maximization for GMM fit \n",
    "component_params = perform_standard_em(connections_df['log_size'], K=2, seed=747)\n",
    "\n",
    "# GMM plot and fit evaluation\n",
    "decision_boundary = hist_with_GMM_fit_plus_qqplot_with_decision_boundary(\n",
    "    connections_df['log_size'],\n",
    "    component_params['mu'], \n",
    "    component_params['std'],\n",
    "    component_params['pp']\n",
    ")\n",
    "\n",
    "# print component parameters\n",
    "print(\"GMM Parameters:\")\n",
    "print(component_params)\n",
    "\n",
    "# get tail boundary and tail df\n",
    "tail_minimum = decision_boundary\n",
    "\n",
    "connections_df_tail = connections_df[connections_df['log_size'] >= tail_minimum]\n",
    "\n",
    "# expected and observed proportions of connection types\n",
    "categories = ['Shared', 'Nonassembly']\n",
    "shared_count = len(connections_df[connections_df['connection_type'] == 'shared'])\n",
    "nonassembly_count = len(connections_df[connections_df['connection_type'] == 'nonassembly'])\n",
    "total_count = shared_count + nonassembly_count\n",
    "\n",
    "prop_shared = shared_count / total_count\n",
    "prop_nonassembly = nonassembly_count / total_count\n",
    "\n",
    "tail_shared_count = len(connections_df_tail[connections_df_tail['connection_type'] == 'shared'])\n",
    "tail_nonassembly_count = len(connections_df_tail[connections_df_tail['connection_type'] == 'nonassembly'])\n",
    "\n",
    "total_tail_count = tail_shared_count + tail_nonassembly_count\n",
    "expected_shared = total_tail_count * prop_shared\n",
    "expected_nonassembly = total_tail_count * prop_nonassembly\n",
    "\n",
    "observed = [tail_shared_count, tail_nonassembly_count]\n",
    "expected = [expected_shared, expected_nonassembly]\n",
    "chi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)\n",
    "\n",
    "observed_props = [tail_shared_count / total_tail_count, tail_nonassembly_count / total_tail_count]\n",
    "expected_props = [prop_shared, prop_nonassembly]\n",
    "\n",
    "# frequency table\n",
    "frequency_table = tabulate(\n",
    "    [[cat, obs, f\"{exp:.2f}\"] for cat, obs, exp in zip(categories, observed, expected)],\n",
    "    headers=[\"Connection Type\", \"Observed Frequency\", \"Expected Frequency\"],\n",
    "    tablefmt=\"pretty\"\n",
    ")\n",
    "\n",
    "# Construct the proportion table\n",
    "proportion_table = tabulate(\n",
    "    [[cat, f\"{obs:.4g}\", f\"{exp:.4g}\"] for cat, obs, exp in zip(categories, observed_props, expected_props)],\n",
    "    headers=[\"Connection Type\", \"Observed Proportion\", \"Expected Proportion\"],\n",
    "    tablefmt=\"pretty\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Observed vs Expected Frequencies:')\n",
    "print(frequency_table, \"\\n\")\n",
    "\n",
    "print(\"Observed vs Expected Proportions:\")\n",
    "print(proportion_table, \"\\n\")\n",
    "\n",
    "print(f\"Chi-squared statistic: {chi2_stat:.4g}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assembly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
